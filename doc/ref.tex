%% $Id: ref.tex 2694 2014-01-24 09:22:30Z schwicht $
%% was: ea.tex,v 1.68 2001/08/13 12:55:02 schwicht Exp
\documentclass[11pt,a4paper]{amsart}
\RequirePackage[latin1]{inputenc}
\RequirePackage{bussproofs}
\RequirePackage{amssymb}
\RequirePackage{verbatim}
\RequirePackage{alltt}
\RequirePackage{enumerate} %allows \begin{comment} text \end{comment}
\RequirePackage{url}
%% for pdftex
\RequirePackage[backref]{hyperref}

%% see http://www.ctan.org/tex-archive/help/Catalogue/entries/natbib.html
%% \RequirePackage[round]{natbib}
%% natbib does not seem to work well with hyperref

%% Font commands
\newcommand{\B}{\boldsymbol}
   %Bold math symbol, use as \B{a}
\newcommand{\C}[1]{\mathcal{#1}}
   %Euler Script - only caps, use as \C{A}
\newcommand{\D}[1]{\mathbb{#1}}
   %Doubles - only caps, use as \D{A}
\newcommand{\F}[1]{\mathfrak{#1}}
   %Fraktur, use as \F{a}

%% Logic: formulas
\newcommand{\allc}{\forall^{\mathrm{c}}} %with computational content
\newcommand{\allnc}{\forall^{\mathrm{nc}}} %no computational content
\newcommand{\bigland}{\mathop{\hbox{$\bigwedge$\kern-.6em$\bigwedge$}}}
\newcommand{\biglor}{\mathop{\hbox{$\bigvee$\kern-.6em$\bigvee$}}}
\newcommand{\ex}{\exists} %constructive
\newcommand{\exca}{\tilde{\exists}} %classical, arithmetical falsum
\newcommand{\excl}{\tilde{\exists}} %classical, with logical falsum
\newcommand{\exd}{\exists^{\mathrm{d}}} %double
\newcommand{\exL}{\exists^{\mathrm{l}}} %left
\newcommand{\exnc}{\exists^{\mathrm{u}}} %constructive, no comp. content
\newcommand{\exR}{\exists^{\mathrm{r}}} %right
\newcommand{\falsum}{\bot}
\newcommand{\inTo}{\hookrightarrow}
\newcommand{\landc}{\land^{\mathrm{c}}}
\newcommand{\landcl}{\mathop{\tilde{\land}}} %in combination with excl
\newcommand{\landd}{\land^{\mathrm{d}}}
\newcommand{\landL}{\land^{\mathrm{l}}}
\newcommand{\landnc}{\land^{\mathrm{u}}}
\newcommand{\landR}{\land^{\mathrm{r}}}
\newcommand{\lorc}{\lor^{\mathrm{c}}}
\newcommand{\lord}{\lor^{\mathrm{d}}}
\newcommand{\lorL}{\lor^{\mathrm{l}}}
\newcommand{\loru}{\lor^{\mathrm{u}}}
\newcommand{\lornc}{\lor^{\mathrm{nc}}}
\newcommand{\lorR}{\lor^{\mathrm{r}}}
\newcommand{\lorcl}{\mathrel{\tilde{\lor}}}
\newcommand{\outTo}{\to}
\newcommand{\toc}{\to^{\mathrm{c}}}
\newcommand{\tonc}{\to^{\mathrm{nc}}}
\newcommand{\truth}{\top}

%% Logic: rules
\newcommand{\AVar}{\mathrm{AssVar}}
\newcommand{\allcI}{({\allc})^{+}}
\newcommand{\allcE}{({\allc})^{-}}
\newcommand{\allI}{{\forall}^{+}}
\newcommand{\allncI}{({\allnc})^{+}}
\newcommand{\allncE}{({\allnc})^{-}}
\newcommand{\allE}{{\forall}^{-}}
\newcommand{\exI}{{\ex}^{+}}
\newcommand{\exE}{{\ex}^{-}}
\newcommand{\impE}{{\to}^{-}}
\newcommand{\impI}{{\to}^{+}}
\newcommand{\landE}{\land^{-}}
\newcommand{\landI}{{\land}^{+}}
\newcommand{\lorI}{\lor^{+}}
\newcommand{\lorIzero}{\lor^{+}_0}
\newcommand{\lorIone}{\lor^{+}_1}
\newcommand{\lorE}{\lor^{-}}
\newcommand{\lorIi}{\lor^{+}_i}
\newcommand{\tocE}{({\toc})^{-}}
\newcommand{\tocI}{({\toc})^{+}}
\newcommand{\toE}{{\to}^{-}}
\newcommand{\toI}{{\to}^{+}}
\newcommand{\toncE}{({\tonc})^{-}}
\newcommand{\toncI}{({\tonc})^{+}}

%% Logic: proof terms
\newcommand{\exelim}[2]{(#1.#2)}
\newcommand{\lorelim}[4]{(#1.#2,#3.#4)}

%% Algebras
\newcommand{\as}{\mathit{a}\!\mathit{s}}
\newcommand{\branch}{\mathrm{Branch}}
\newcommand{\bs}{\mathit{b}\!\mathit{s}}
\newcommand{\caseBooleConst}{\mathrm{if}}
\newcommand{\caseListConst}{\C{C}}
\newcommand{\caseNatConst}{\C{C}}
\newcommand{\CasesAxiom}{\mathrm{Cases}}
\newcommand{\coind}{{}^{\mathrm{co}}\mathrm{Ind}}
\newcommand{\cons}{\mathrm{Cons}}
%% \newcommand{\cons}{\mathrm{cons}}
\newcommand{\constr}{\mathrm{C}}
\newcommand{\emp}{\mathrm{Empty}}
\newcommand{\even}{\mathrm{even}}
\newcommand{\evenodd}{\mathrm{evenodd}}
\newcommand{\false}{\mathsf{f\kern-0.14em f}}
\newcommand{\grec}{\C{F}}
\newcommand{\ind}{\mathrm{Ind}}
\newcommand{\leaf}{\mathrm{Leaf}}
\newcommand{\nil}{\mathrm{Nil}}
%% \newcommand{\nil}{\mathrm{nil}}
\newcommand{\odd}{\mathrm{odd}}
\newcommand{\one}{\mathrm{1}}
\newcommand{\p}{P}
\newcommand{\rec}{\C{R}}
\newcommand{\s}{S}
\newcommand{\suc}{\mathrm{S}}
\newcommand{\Sup}{\mathrm{Sup}}
\newcommand{\ST}{\delta}
\newcommand{\tcons}{\mathrm{Tcons}}
\newcommand{\termCont}{\mathrm{Cont}}
\newcommand{\termGet}{\mathrm{Get}}
\newcommand{\termProdIntro}{\typeProd^+}
\newcommand{\termPut}{\mathrm{Put}}
\newcommand{\termStop}{\mathrm{Stop}}
\newcommand{\termSumIntroLeft}{\mathrm{Inl}}
\newcommand{\termSumIntroRight}{\mathrm{Inr}}
%% \newcommand{\termSumIntroLeft}{\mathrm{inl}}
%% \newcommand{\termSumIntroRight}{\mathrm{inr}}
\newcommand{\termSumElim}{\typeSum^-}
\newcommand{\termUnit}{\mathbf{u}}
\newcommand{\tlist}{\mathrm{tlist}}
\newcommand{\tree}{\mathrm{tree}}
\newcommand{\true}{\mathsf{t\kern-0.14em t}}

%% Types
\newcommand{\Ty}{\mathrm{Ty}}
\newcommand{\constrtypes}[1]{\mathrm{KT}_{#1}}
\newcommand{\typeB}{\mathbf{B}}
\newcommand{\typeBin}{\mathbf{D}}
\newcommand{\typeEv}{\mathbf{Ev}}
\newcommand{\typeIntv}{\mathbf{I}} %standard rational intervals
\newcommand{\typeInTo}{\hookrightarrow}
\newcommand{\typeL}[1]{\mathbf{L}(#1)}
\newcommand{\typeN}{\mathbf{N}}
\newcommand{\typeOd}{\mathbf{Od}}
\newcommand{\typeOrd}{\mathbf{O}}
\newcommand{\typeOutTo}{\to}
\newcommand{\typeP}{\mathbf{P}}
\newcommand{\typePrimProd}{\mathrm{@@}}
\newcommand{\typeProd}{\times}
\newcommand{\typeRead}{\mathbf{R}}
\newcommand{\typeSD}{\mathbf{SD}}
\newcommand{\typeSum}{+}
%% \newcommand{\typeTensor}{\times}
\newcommand{\typeTlist}{\mathbf{Ts}}
\newcommand{\typeTo}{\to} %Stans proposal
\newcommand{\typeTree}{\mathbf{T}}
\newcommand{\typeUnit}{\mathbf{U}}
\newcommand{\typeW}{\mathbf{W}}
\newcommand{\typeWrite}{\mathbf{W}}

%% Systems
\newcommand{\EAIO}{\mathrm{EA({;})}} %Elementary arithmetic
\newcommand{\ETCF}{\mathrm{E}\textrm{-}\mathrm{TCF}}
\newcommand{\HA}{\mathrm{HA}} %Heyting Arithmetic
\newcommand{\ID}{\mathrm{ID}} %Arithmetic with inductively defined predicates
\newcommand{\IDelta}{\mathrm{I}\Delta}
\newcommand{\Ilev}{\mathbf{Z}}
\newcommand{\ISigma}{\mathrm{I}\Sigma}
\newcommand{\LRA}{\mathrm{LA({;})}} %Linear two-sorted arithmetic
\newcommand{\LRT}{\mathrm{LT({;})}} %Linear two-sorted Goedel T
\newcommand{\PA}{\mathrm{PA}} %Peano Arithmetic
\newcommand{\PCF}{\mathrm{PCF}}
\newcommand{\PV}{\mathrm{PV}}
\newcommand{\RA}{\mathrm{A({;})}} %Two-sorted Arithmetic
\newcommand{\RAOne}{\Sigma_1\textrm{-}\mathrm{A({;})}}
\newcommand{\RT}{\mathrm{T({;})}} %Two-sorted Goedel T
\newcommand{\RTOne}{\mathrm{T}_{1}\mathrm{({;})}}
\newcommand{\T}{\mathrm{T}} %Goedels T
\newcommand{\TCF}{\mathrm{TCF}} %Theory of Computable Functionals

%% Axioms
\newcommand{\AC}{\mathrm{AC}} %axiom of choice
\newcommand{\IP}{\mathrm{IP}} %independence of premise
\newcommand{\MP}{\mathrm{MP}} %Markov's principle
\newcommand{\PH}{\mathrm{PH}}

%% Extraction
\newcommand{\extrTy}[1]{\tau(#1)}
\newcommand{\extrTer}[1]{\mathrm{et}(#1)}
\newcommand{\extrTerP}[1]{\mathrm{et}^{+}(#1)}
\newcommand{\extrTerN}[2]{\mathrm{et}^{-}_{#2}(#1)}
\newcommand{\nulltype}{\circ}
\newcommand{\nullterm}{\varepsilon}

%% Inductively generated predicates
\newcommand{\Acc}{\mathrm{Acc}}
\newcommand{\LAnd}{\mathrm{AndD}}
\newcommand{\LOr}{\mathrm{OrD}}
%% \newcommand{\LAnd}{\mathrm{And}}
%% \newcommand{\LOr}{\mathrm{Or}}
\newcommand{\coclauses}[1]{{}^{\mathrm{co}}\mathrm{Cl}_{#1}}
\newcommand{\clauses}[1]{\mathrm{Cl}_{#1}}
\newcommand{\Ex}{\mathrm{ExD}}
%% \newcommand{\Ex}{\mathrm{Ex}}
%% \newcommand{\Exd}{\mathrm{Ex}^{\mathrm{d}}}
\newcommand{\Even}{\mathrm{Even}}
\newcommand{\formulas}{\mathrm{F}}
\newcommand{\ipreds}{\mathrm{IPreds}}
\newcommand{\preds}{\mathrm{Preds}}
\newcommand{\Tree}{\mathrm{Tree}}
\newcommand{\TrCl}{\mathrm{TC}}

%% Computability
\newcommand{\dotminus}{\mathbin{{{-\mkern -9.5mu%
\mathchoice{\raise 2pt\hbox{$\cdot$}\mkern6mu}%
{\raise 2pt\hbox{$\cdot$}\mkern6mu}%
{\raise 1.5pt\hbox{$\scriptstyle\cdot$}\mkern4mu}%
{\raise 1pt\hbox{$\scriptscriptstyle\cdot$}\mkern4mu}}}}}
\newcommand{\Elementary}{\mathrm{Elem}}
\newcommand{\Prim}{\mathrm{Prim}}
\newcommand{\Prime}{\mathrm{Prime}}
\newcommand{\comp}{\mathrm{comp}}
\newcommand{\state}{\mathrm{state}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\dc}{\mathrm{dc}}
\newcommand{\Comp}{\mathrm{Comp}}
\newcommand{\compose}[2]{#1 \textbf{\ ;\ } #2}
\newcommand{\composethree}[3]{#1 \textbf{\ ;\ } #2 \textbf{\ ;\ } #3}
\newcommand{\composefour}[4]
{#1 \textbf{\ ;\ } #2 \textbf{\ ;\ } #3 \textbf{\ ;\ } #4}
\newcommand{\ifthenelsefi}[3]
{\textbf{if\ } #1 \textbf{\ then\ } #2 \textbf{\ else\ } #3 \textbf{\ fi}}
\newcommand{\forloop}[2]
{\textbf{for\ } #1 \textbf{\ do\ } #2 \textbf{\ od}}
\newcommand{\whileloop}[2]
{\textbf{while\ } #1 \textbf{\ do\ } #2 \textbf{\ od}}
\newcommand{\Subt}{\mathrm{Subt}}
\newcommand{\Mult}{\mathrm{Mult}}
\newcommand{\hd}{\mathrm{hd}}
\newcommand{\tl}{\mathrm{tl}}

%% Goedel theorems
\newcommand{\Ax}{\mathrm{Ax}}
\newcommand{\Cons}{\mathrm{Cons}}
\newcommand{\ctx}{\mathrm{ctx}}
\newcommand{\Deriv}{\mathrm{Deriv}}
\newcommand{\dotand}{\mathbin{\dot\land}}
\newcommand{\dotneg}{\mathord{\dot\neg}}
\newcommand{\fla}{\mathrm{fmla}}
\newcommand{\For}{\mathrm{For}}
\newcommand{\PR}{\mathrm{PR}}
\newcommand{\Prf}{\mathrm{Prf}}
\newcommand{\ran}{\mathrm{ran}}
\newcommand{\Refut}{\mathrm{Refut}}
\newcommand{\remove}{\mathrm{remove}}
\newcommand{\sn}{\mathrm{sn}}
\newcommand{\Sub}{\mathrm{Sub}}
\newcommand{\sub}{\mathrm{sub}}
\newcommand{\substval}{\mathrm{substval}}
\newcommand{\Symb}{\mathrm{Symb}}
\newcommand{\symb}{\mathrm{sb}}
%% \newcommand{\symb}{\mathrm{symb}}
\newcommand{\Th}{\mathrm{Th}}
\newcommand{\Thm}{\mathrm{Thm}}
\newcommand{\union}{\mathrm{union}}
\newcommand{\update}{\mathrm{update}}

%% Miscellaneous
\newcommand{\acc}{\mathrm{acc}}
\newcommand{\Add}{\mathrm{Add}}
\newcommand{\AllFor}{\mathrm{AllFor}}
\newcommand{\AlphaEq}{\alpha\hbox{-Eq}}
%% \newcommand{\AlphaEq}{=_{\alpha}}
%% \newcommand{\AlphaEq}{\mathrm{AlphaEq}}
\newcommand{\andb}{\mathrel{\mathrm{andb}}}
\newcommand{\atom}{\mathrm{atom}}
\newcommand{\ar}{\mathrm{ar}}
\newcommand{\BNFdef}{\mathtt{\; ::= \;}}
\newcommand{\BNFor}{\mid}
\newcommand{\bottom}{\bot}
\newcommand{\CA}{\mathrm{CA}}
\newcommand{\Card}{\mathrm{Card}}
\newcommand{\caseof}[2]{[\textbf{case}\; #1\; \textbf{of}\; #2]}
\newcommand{\Cases}{\C{C}}
\newcommand{\CK}{\mathrm{CK}}
\newcommand{\cnv}{\mapsto}
\newcommand{\CoGTotal}{{}^{\mathrm{co}}\GTotal}
\newcommand{\coI}{{}^{\mathrm{co}}I}
\newcommand{\Con}{\mathrm{Con}}
\newcommand{\con}[3]{#2 ::_{#1} #3}
\newcommand{\concat}[2]{\hbox{$#1$ {\bf ;} $#2$}}
\newcommand{\cond}{\mathrm{cond}}
\newcommand{\consistent}{\uparrow}
%% \newcommand{\consistent}{\smallsmile}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\CoSTotal}{{}^{\mathrm{co}}\STotal}
\newcommand{\corec}{{}^{\mathrm{co}}\C{R}}
\newcommand{\CV}{\mathrm{CV}}
\newcommand{\cvind}{\mathrm{cvind}}
\newcommand{\defeq}{:=}
\newcommand{\defequiv}{:=}
\newcommand{\dep}[1]{\mathrm{dp}(#1)} %depth of a formula or proof tree
\newcommand{\destr}{\mathrm{D}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\Efq}{\mathrm{Efq}}
\newcommand{\Eq}{\mathrm{EqD}}
%% \newcommand{\Eq}{\mathrm{Eq}}
\newcommand{\emptyseq}{\langle\rangle}
\newcommand{\en}{\mathrm{en}}
\newcommand{\entails}{\vdash}
\newcommand{\eps}{\varepsilon}
\newcommand{\eqd}[2]{#1 \equiv #2}
%% \newcommand{\eqd}[2]{\mathrm{Eq}(#1,#2)}
\newcommand{\eqdef}{=:}
\newcommand{\ext}{\mathrm{ext}}
\newcommand{\FA}{\mathrm{FA}}
\newcommand{\falsityF}{\mathbf{F}}
\newcommand{\fix}{Y}
\newcommand{\forces}{\Vdash}
\newcommand{\fordo}[2]{\hbox{{\bf for} $#1$ {\bf do} $#2$ {\bf od}}}
\newcommand{\FPV}{\mathrm{FPV}}
\newcommand{\FV}{\mathrm{FV}}
\newcommand{\gfp}[1]{\C{C}_{#1}}
\newcommand{\GInd}{\mathrm{GInd}}
\newcommand{\gn}[1]{\ulcorner #1 \urcorner}
\newcommand{\goe}[3]{|#1|^{#2}_{#3}}
\newcommand{\goemr}[3]{\norm{#1}^{#2}_{#3}}
\newcommand{\GTotal}{T} %obsolete, to be replaced by \Total
\newcommand{\head}{\mathrm{hd}}
\newcommand{\height}[1]{|#1|} %height of a formula or proof tree
\newcommand{\Hyp}{\mathrm{Hyp}}
\newcommand{\ic}{\mathrm{[}\mathrm{ic}\mathrm{]}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\If}{\mathrm{If}}
\newcommand{
  \ifthenelse}[3]{[\textbf{if}\; #1\; \textbf{then}\; #2\; \textbf{else}\; #3]}
\newcommand{\impb}{\mathrel{\mathrm{impb}}} %boolean function implication
\newcommand{\incons}{\mathrm{incons}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\infseq}[3]{#1 \colon N \, \vdash^{#2} \, #3} %agreed on 03-01-15
\newcommand{\infseqr}[4]{#1 \colon N \, \vdash^{#2}_{\Sigma_{#3}}\, #4}
\newcommand{\Inhab}{\texttt{Inhab}}
\newcommand{\Init}{\mathrm{Init}}
\newcommand{\inquotes}[1]{``#1''}
\newcommand{\KB}{\mathrm{KB}}
\newcommand{\lev}[1]{\mathrm{lev}(#1)}
\newcommand{\lfp}[1]{\C{I}_{#1}}
\newcommand{\lh}[1]{\mathrm{lh}(#1)}
\newcommand{\Lim}{\mathrm{Lim}}
\newcommand{\LinOrd}{\mathrm{LinOrd}}
\newcommand{\listappend}{\mathbin{\ast}}
\newcommand{\listrev}{\mathrm{Rev}}
\newcommand{\lnf}[1]{\mathrm{nf}(#1)}
\newcommand{\Map}{\C{M}}
\newcommand{\mr}{\mathrel{\mathbf{r}}}
\newcommand{\mrind}{\textbf{r}}
\newcommand{\nci}{\mathrm{nci}}
\newcommand{\nf}[1]{\mathrm{nf}(#1)}
\newcommand{\Nf}{\mathrm{Nf}}
\newcommand{\norm}[1]{| \! |#1| \! |}
\newcommand{\notconsistent}{\mathrel{\not \kern-0.15em {\consistent}}}
\newcommand{\On}{\mathrm{On}}
\newcommand{\opr}{\mathrm{pr}} %ordinal predecessor enumeration
\newcommand{\opred}{\mathrm{pred}} %ordinal predecessor
\newcommand{\orb}{\mathrel{\mathrm{orb}}} %boolean function or
\newcommand{\overwrite}[3]{#1[#2\mapsto #3]}
\newcommand{\pcond}{\mathrm{pcond}}
\newcommand{\pred}{\mathrm{P}}
\newcommand{\primpair}[2]{#1 @ #2}
\newcommand{\Prog}{\mathrm{Prog}}
\newcommand{\realin}{\mathord{\mathrm{in}}}
\newcommand{\realout}{\mathord{\mathrm{out}}}
\newcommand{\red}{\rightarrow}
\newcommand{\redl}{\leftarrow}
\newcommand{\redlstar}{\leftarrow^*}
\newcommand{\redpar}{\rightarrow_p}
\newcommand{\redplus}{\rightarrow^+}
\newcommand{\redstar}{\rightarrow^*}
\newcommand{\reflect}{{\uparrow}}
\newcommand{\reify}{{\downarrow}}
\newcommand{\Rev}{R}
\newcommand{\rf}[3]{\mathrm{rf}(#1;#2;#3)}
\newcommand{\RTotal}{\mathrm{RT}}
\newcommand{\SC}{\mathrm{SC}}
\newcommand{\set}[2]{\{\,#1\mid#2\,\}}
\newcommand{\size}[1]{\norm{#1}} %size of a proof tree or formula
\newcommand{\SE}{\mathrm{SE}} %structural existence
\newcommand{\Sep}{\mathrm{Sep}}
\newcommand{\Seq}{\mathrm{Seq}}
\newcommand{\SN}{\mathrm{SN}}
\newcommand{\SP}{\mathrm{SP}}
\newcommand{\Spec}{\mathrm{Spec}}
\newcommand{\SProg}{\mathrm{SProg}}
\newcommand{\Stab}{\mathrm{Stab}}
\newcommand{\STotal}{\mathrm{ST}}
\newcommand{\subst}[3]{#1[#2:= #3]}
\newcommand{\substnarrow}[3]{#1[#2{:=}#3]}
\newcommand{\Succ}{\mathrm{Succ}}
\newcommand{\tail}{\mathrm{tl}}
\newcommand{\Ter}{\mathrm{Ter}}
\newcommand{\termfam}[1]{#1^{\infty}}
\newcommand{\TExt}{\mathrm{TExt}}
\newcommand{\Tot}{\mathrm{Tot}}
\newcommand{\Total}{T}
\newcommand{\UP}{\mathrm{P}} %proof pattern
\newcommand{\val}[1]{\lbrack\!\lbrack#1\rbrack\!\rbrack}
\newcommand{\valmax}{\mathrm{valmax}}
\newcommand{\valop}[1]{\lbrack#1\rbrack}
\newcommand{\valstrict}[1]{\lbrace\!\!\lbrace#1\rbrace\!\!\rbrace}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\vars}{\mathrm{vars}}
\newcommand{\Wf}{\mathrm{Wf}}
\newcommand{\WOrd}{\mathrm{WOrd}}
\newcommand{\Zero}{\mathrm{Zero}}

%% Temporarily, from the old Miscellaneous part:
\newcommand*{\eqrel}[2]{{=}(#1,#2)}
\newcommand{\unif}[3]{\mathrm{unif}(#1, #2 = #3)}
\newcommand{\unifalg}[1]{\Longrightarrow_{#1}}
\newcommand{\unifprefixalg}[1]{\longrightarrow_{#1}}

\makeindex

\newenvironment{enumeratei}{\begin{enumerate}[\upshape (i)]}
                           {\end{enumerate}}
   %% produces (i), (ii), etc,  Cross-reference with \eqref
\newenvironment{enumeratea}{\begin{enumerate}[\upshape (a)]}
                           {\end{enumerate}}
   %% produces (a), (b), etc,  Cross-reference with \eqref

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem*{axiom*}{Axiom}
\newtheorem*{namedtheorem}{\theoremname}
\newcommand{\theoremname}{testing}
\newenvironment{named}[1]{\renewcommand{\theoremname}{#1}
  \begin{namedtheorem}}
  {\end{namedtheorem}}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem*{notation*}{Notation}
\newtheorem{convention}[theorem]{Convention}
\newtheorem*{convention*}{Convention}
\newtheorem{example}[theorem]{Example}
\newtheorem*{example*}{Example}
\newtheorem{examples}[theorem]{Examples}
\newtheorem*{examples*}{Examples}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem*{exercise*}{Exercise}
\newtheorem{exercises}[theorem]{Exercises}
\newtheorem*{exercises*}{Exercises}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem*{remarks*}{Remarks}
\newtheorem{note}[theorem]{Note}
\newtheorem*{note*}{Note}
\newtheorem*{acknowledgement*}{Acknowledgement}
\newtheorem{problem}{Problem}
\newtheorem*{problem*}{Problem}
\newtheorem{solution}{Solution}
\newtheorem*{solution*}{Solution}

\allowdisplaybreaks[4]
\setlength{\textheight}{19 true cm}

\makeindex

\title{Minlog reference manual}
\author{Helmut Schwichtenberg}
\date{\today}

\hyphenation{ana-lyzed}
\hyphenation{appro-xi-ma-ted}
\hyphenation{cha-rac-te-rize}
\hyphenation{com-pu-ta-tio-nal-ly}
\hyphenation{con-ti-nu-ous}
\hyphenation{ea-si-ly}
\hyphenation{eli-mi-na-tion}
\hyphenation{ge-ne-ral}
\hyphenation{ge-ne-ra-lized}
\hyphenation{ge-ne-ra-ted}
\hyphenation{ma-the-ma-ti-cal-ly}
\hyphenation{na-tu-ral}
\hyphenation{nor-ma-li-za-tion}
\hyphenation{op-tio-nal}
\hyphenation{pa-ra-me-ter}
\hyphenation{pa-ra-me-ters}
\hyphenation{pre-di-cate}
\hyphenation{pre-sent-ly}
\hyphenation{pro-va-ble}
\hyphenation{re-le-vant}
\hyphenation{sa-tis-fac-to-ry}
\hyphenation{Si-mi-lar-ly}
\hyphenation{sprea-ding}
\hyphenation{whe-ther}

\begin{document}

\maketitle

\tableofcontents

\begin{acknowledgement*}
  The Minlog system has been under development since around 1990; its
  first appearance in print is in \cite{Schwichtenberg92a}.  My
  sincere thanks go to the many contributors:
  \begin{itemize}
    %% Klaus Aehlig (many contributions, in particular efficiency
    %% related issues),
  \item Freiric Barral\index{Barral} (reflection),
  \item Holger Benl\index{Benl} (Dijkstra algorithm, inductive data
    types),
  \item Ulrich Berger\index{Berger} (very many contributions),
  \item Michael Bopp\index{Bopp} (program development by proof
    transformation),
  \item Wilfried Buchholz\index{Buchholz} (translation of classical
    proofs into intuitionistic ones),
  \item Luca Chiarabini\index{Chiarabini} (program development by
    proof transformation),
  \item Laura Crosilla\index{Crosilla} (tutorial),
  \item Matthias Eberl\index{Eberl} (normalization by evaluation),
  \item Fredrik Nordvall Forsberg\index{Forsberg} (Haskell translation)
  \item Simon Huber\index{Huber} (many contributions, in particular
    guarded recursion, general induction),
  \item Dan Hernest\index{Hernest} (functional interpretation),
  \item Felix Joachimski\index{Joachimski} (many contributions, in
    particular translation of classical proofs into intuitionistic
    ones, producing Tex output, documentation),
  \item Ralph Matthes\index{Matthes} (documentation),
  \item Kenji Miyamoto\index{Miyamoto} (corecursion, coinduction),
  \item Karl-Heinz Niggl\index{Niggl} (program development by proof
    transformation),
  \item Jaco van de Pol%
    \index{Pol, van de} (experiments concerning monotone functionals),
  \item Florian Ranzi\index{Ranzi} (matching),
  \item Diana Ratiu\index{Ratiu} (decoration),
  \item Martin Ruckert\index{Ruckert} (many contributions, in
    particular grammar and the MPC tool),
  \item Stefan Schimanski\index{Schimanski} (pretty printing),
  \item Robert Stärk\index{Stärk} (alpha equivalence),
  \item Monika Seisenberger\index{Seisenberger} (many contributions,
    including inductive definitions and translation of classical
    proofs into intuitionistic ones),
  \item Trifon Trifonov\index{Trifonov} (functional interpretation),
  \item Klaus Weich\index{Weich} (proof search, the Fibonacci
    numbers example),
  \item Wolfgang Zuber\index{Zuber} (documentation).
  \end{itemize}
\end{acknowledgement*}


\section{Introduction}
\label{Intro}
Proofs in mathematics generally deal with abstract, \inquotes{higher
  type} objects.  Therefore an analysis of computational aspects of
such proofs must be based on a theory of computation in higher types.
A mathematically satisfactory such theory has been provided by Scott
\cite{Scott70} and Ershov \cite{Ershov77}.  The basic concept is that
of a \emph{partial continuous functional}.  Since each such can be
seen as a limit of its finite approximations, we get for free the
notion of a computable functional: it is given by a recursive
enumeration of finite approximations.  The price to pay for this
simplicity is that functionals are now \emph{partial}, in stark
contrast to the view of Gödel \cite{Goedel58}.  However, the total
functionals can be defined as a subset of partial ones.  In fact, as
observed by Kreisel, they form a dense subset w.r.t.\ the Scott
topology.  The next step is to build a theory, with the partial
continuous functionals as the intended range of its (typed) variables.
The constants of this \inquotes{theory of computable functionals}
$\TCF$ denote computable functionals.  It suffices to restrict the
prime formulas to those built with inductively defined predicates.
For instance, falsity\index{$\falsityF$}%
\index{falsity $\falsityF$} can be defined by $\falsityF \defeq \eqd
{\false} {\true}$, where $\Eq$ is the inductively defined Leibniz
equality\index{equality!Leibniz}%
\index{Leibniz equality}.  The only logical connectives are
implication and universal quantification: existence, conjunction and
disjunction can be seen as inductively defined (with parameters).
$\TCF$ is well suited to reflect on the computational content of
proofs, along the lines of the Brouwer-Heyting-Kolmogorov
interpretation, or more technically a realizability interpretation in
the sense of Kleene and Kreisel.  Moreover the computational content
of classical (or \inquotes{weak}) existence proofs can be analyzed in
$\TCF$, in the sense of Gödel's \cite{Goedel58} Dialectica
interpretation and the so-called $A$-translation of Friedman
\cite{Friedman78}\index{Friedman} and Dragalin
\cite{Dragalin79}\index{Dragalin}.  The difference of $\TCF$ to
well-established theories like Martin-Löf's \cite{MartinLoef84}
intuitionistic type theory or the theory of constructions underlying
the Coq proof assistant \nocite{Coq09} is that $\TCF$ treats partial
continuous functionals as first class citizens.  Since they are the
mathematically correct domain of computable functionals, it seems that
this is a reasonable step to take.

Minlog is intended to reason about computable functionals,
using minimal logic.  It is an interactive prover with the following
features.
\begin{enumeratei}
\item Proofs are treated as first class objects: they can be normalized
  and then used for reading off an instance if the proven formula is
  existential, or changed for program development by proof
  transformation.
\item To keep control over the complexity of extracted programs, we
  follow Kreisel's proposal and aim at a theory with a strong language
  and weak existence axioms.  It should be conservative over (a fragment
  of) arithmetic.
\item Minlog is based on minimal rather than classical or
  intuitionistic logic.  This more general setting makes it possible to
  implement program extraction from classical proofs, via a refined
  $A$-translation (cf.\ \cite{BergerBuchholzSchwichtenberg02}).
\item Constants are intended to denote computable functionals.  Since
  their (mathematically correct) domains are the Scott-Ershov partial
  continuous functionals, this is the intended range of the
  quantifiers.
\item Variables carry (simple) types, with free algebras as base
  types.  The latter need not be finitary (we allow, e.g., countably
  branching trees), and can be simultaneously generated.  Type and
  predicate parameters are allowed; they are thought of as being
  implicitly universally quantified (\inquotes{ML polymorphism}).
\item To simplify equational reasoning, the system identifies
  terms with the same normal form.  A rich collection of rewrite rules
  is provided, which can be extended by the user.  Decidable predicates
  are implemented via boolean valued functions, hence the rewrite
  mechanism applies to them as well.
\end{enumeratei}
We now describe in more details some of these features.

\subsection{Simultaneous free algebras}
A free algebra is given by \emph{constructors}, for instance zero and
successor for the natural numbers.  We want to treat other data types
as well, like lists and binary trees.  When dealing with inductively
defined sets, it will also be useful to explicitly refer to the
generation tree.  Such trees are quite often countably branching, and
hence we allow infinitary free algebras from the outset.

The freeness of the constructors is expressed by requiring that their
ranges are disjoint and that they are injective.  Moreover, we view
the free algebra as a domain and require that its bottom element is
not in the range of the constructors.  Hence the constructors are
total and non-strict.  For the notion of totality cf.\ \cite[Chapter
8.3]{Stoltenberg94}.

In our intended semantics we do not require that every semantic object
is the denotation of a closed term, not even for finitary algebras.
One reason is that for normalization by evaluation (cf.\
\cite{BergerEberlSchwichtenberg03}) we want to allow term families in
our semantics.

To make a free algebra into a domain and still have the constructors
injective and with disjoint ranges, we model, e.g., the natural numbers
as shown in Figure~\ref{F:nat}.
\begin{figure}
  %% \begin{picture}(168,108)
  \begin{picture}(170,120)
    \put(48,0){\makebox(0,0){$\bullet$}}
    \put(36,0){\makebox(0,0){$\bot$}}
    \put(48,0){\line(-1,1){24}}
    \put(24,24){\makebox(0,0){$\bullet$}}
    \put(12,24){\makebox(0,0){$0$}}
    \put(48,0){\line(1,1){24}}
    \put(72,24){\makebox(0,0){$\bullet$}}
    \put(90,24){\makebox(0,0){$S \bot$}}

    \put(72,24){\line(-1,1){24}}
    \put(48,48){\makebox(0,0){$\bullet$}}
    \put(30,48){\makebox(0,0){$S 0$}}
    \put(72,24){\line(1,1){24}}
    \put(96,48){\makebox(0,0){$\bullet$}}
    \put(120,48){\makebox(0,0){$S(S \bot)$}}

    \put(96,48){\line(-1,1){24}}
    \put(72,72){\makebox(0,0){$\bullet$}}
    \put(48,72){\makebox(0,0){$S(S 0)$}}
    \put(96,48){\line(1,1){24}}
    \put(120,72){\makebox(0,0){$\bullet$}}
    \put(150,72){\makebox(0,0){$S(S(S \bot))$}}

    \put(120,72){\line(-1,1){24}}
    \put(96,96){\makebox(0,0){$\bullet$}}
    \put(66,96){\makebox(0,0){$S(S(S 0))$}}
    \put(120,72){\line(1,1){24}}
    \put(147,99){\makebox(0,0){.}}
    \put(150,102){\makebox(0,0){.}}
    \put(153,105){\makebox(0,0){.}}
    \put(159,111){\makebox(0,0){$\bullet$}}
    \put(181,111){\makebox(0,0){$\infty$}}
  \end{picture}
  \caption{The domain of natural numbers}
  \label{F:nat}
\end{figure}
Notice that for more complex algebras we usually need many more
\inquotes{infinite} elements; this is a consequence of the closure of
domains under suprema.  To make dealing with such complex structures
less annoying, we will normally restrict attention to the \emph{total}
elements of a domain, in this case -- as expected -- the elements
labelled $0$, $S 0$, $S(S 0)$ etc.


\subsection{Partial continuous functionals}
As already mentioned, the (mathematically correct) domains of
computable functionals have been identified by Scott and Ershov as the
partial continuous functionals; cf.\ \cite{Stoltenberg94}.  Since we
want to deal with computable functionals in our theory, we consider it
as mandatory to accommodate their domains.  This is also true if one
is interested in total functionals only; they have to be treated as
particular partial continuous functionals.  We will make use of
inductively defined predicates $\GTotal_{\rho}$ with the total
functionals of type $\rho$ as their intended meaning.  To make formal
arguments with quantifiers relativized to total objects more
managable, we use a special sort of variables intended to range over
such objects only.  For example, $\texttt{n}, \texttt{n0},
\texttt{n1}, \texttt{n2}, \dots$ range over total natural numbers, and
$\verb#n^#, \verb#n^0#, \verb#n^1#, \verb#n^2#, \dots$ are general
variables.  This amounts to an abbreviation of
\begin{alignat*}{2}
  &\forall_{\hat{x}}(\GTotal_{\rho} \hat{x} \to A) &\quad\hbox{by}\quad&
  \forall_x A,
  \\
  &\exists_{\hat{x}}(\GTotal_{\rho} \hat{x} \land A) &\quad\hbox{by}\quad&
  \exists_x A.
\end{alignat*}

\subsection{Primitive recursion, computable functionals}
The elimination constants corresponding to the constructors are called
primitive recursion operators $\rec$.  They are described in detail in
section~\ref{S:Pconst}.  In this setup, every closed term reduces to a
numeral.

However, we shall also use constants for rather arbitrary computable
functionals, and axiomatize them according to their intended meaning
by means of rewrite rules.  An example is the general fixed point
operator $\fix$, which is axiomatized by $\fix F = F(\fix F)$.
Clearly then it cannot be true any more that every closed term reduces
to a numeral.  We may have non-terminating terms, but this just means
that not always it is a good idea to try to normalize a term.

An important consequence of admitting non-terminating terms is that
our notion of proof is not decidable: when checking, e.g., whether two
terms are equal we may run into a non-terminating computation.  But we
still have semi-decidability of proofs, i.e., an algorithm to check
the correctness of a proof that can only give correct results, but may
not terminate.  In practice this is sufficient.

To avoid this somewhat unpleasant undecidability phenomenon, we may
also view our proofs as abbreviated forms of full proofs, with certain
equality arguments left implicit.  If some information sufficient to
recover the full proof (e.g., for each node a bound on the number of
rewrite steps needed to verify it) is stored as part of the proof,
then we retain decidability of proofs.


\subsection{Decidable predicates, axioms for predicates}
As already mentioned, decidable predicates are represented by means of
boolean valued functions, hence the rewrite mechanism applies to them
as well.  Equality is decidable for finitary algebras only; for
infinitary algebras one uses the inductively defined Leibniz equality%
\index{Leibniz equality} instead.

\subsection{Minimal logic, proof transformation}
For generalities about minimal logic cf.\ \cite{TroelstravanDalen88}
or \cite{TroelstraSchwichtenberg00}.  A description of the theory
behind the present implementation can be found in
\cite{SchwichtenbergWainer12}.

\subsection{Comparison with Coq and Isabelle}
\label{SS:Coq}
Coq \cite{Coq09} has evolved from a calculus of constructions defined
by Huet\index{Huet} and Coquand\index{Coquand}.  It is a constructive,
but impredicative system based on type theory.  More recently it has
been extended by Paulin-Mohring%
\index{Paulin-Mohring} to also include inductively defined predicates.
Program extraction from proofs has been implemented by Paulin-Mohring,
Filliatre\index{Filliatre} and Letouzey\index{Letouzey}, in the sense
that Ocaml programs are extracted from proofs.

The Isabelle/HOL system of Paulson\index{Paulson} and
Nipkow\index{Nipkow} has its roots in Church's theory of simple types
and Hilbert's Epsilon calculus.  It is an inherently classical system;
however, since many proofs in fact use constructive arguments, in is
conceivable that program extraction can be done there as well.  This
has been explored by Berghofer in his thesis \cite{Berghofer03}.

Compared with the Minlog system, the following points are of interest.
\begin{enumeratei}
\item The fact that in Coq a formula is just a map into the type
  \texttt{Prop} (and in Isabelle into the type \texttt{bool}) can be
  used to define such a function by what is called \emph{strong
    elimination}, say by $f(\true) \defeq A$ and $f(\false) \defeq B$ with
  fixed formulas $A$ and $B$.  The problem is that then it is
  impossible to assign an ordinary type (say in the sense of ML) to a
  proof.  It is not clear how this problem for program extraction can
  be avoided (in a clean way) for both Coq and Isabelle.  In Minlog it
  does not exist due to the separation of terms and formulas.
\item The impredicativity (in the sense of quantification over
  predicate variables) built into Coq and Isabelle has as a
  consequence that extracted programs need to abstract over type
  variables, which is not allowed in program languages of the ML
  family.  Therefore one can only allow outer universal quantification
  over type and predicate variables in proofs to be used for program
  extraction; this is done in the Minlog system from the outset.
  However, many uses of quantification over predicate variables (like
  defining the logical connectives apart from $\to$ and $\forall$) can
  be achieved by means of inductively defined predicates.  This
  feature is available in all three systems.
\item The distinction between properties with and without
  computational content seems to be crucial for a reasonable program
  extraction environment; this feature is available in all three
  systems.  However, it also seems to be necessary to distinguish
  between universal quantifiers with and without computational
  content, as in \cite{Berger93a}.  At present this feature is
  available in the Minlog system only.
\item Coq has records, whose fields may contain proofs and
  may depend on earlier fields.  This can be useful, but does not seem
  to be really essential.  If desired, in Minlog one can use
  products for this purpose; however, proof objects have to be
  introduced explicitly via assumptions.
\item Minlog's automated proof search \texttt{search} tool is
  based on \cite{Miller91b}\index{Miller}; it produces
  proofs in minimal logic.  In addition, Coq has many strong
  tactics, for instance \texttt{Omega} for quantifier free
  Presburger\index{Presburger} arithmetic, \texttt{Arith} for
  proving simple arithmetic properties and \texttt{Ring} for proving
  consequences of the ring axioms.  Similar tactics exist in
  Isabelle.  These tactics tend to produce rather long proofs,
  which is due to the fact that equality arguments are carried out
  explicitly.  This is avoided in Minlog by relativizing every
  proof to a set of rewrite rules, and identifyling terms and formulas
  with the same normal form w.r.t.\ these rules.
\item In Isabelle as well as in Minlog the extracted programs are
  provided as terms within the language, and a soundness proof can be
  generated automatically.  For Coq (and similarly for Nuprl) such a
  feature could at present only be achieved by means of some form of
  reflection.
\end{enumeratei}

\section{Types, with simultaneous free algebras as base types}
\label{S:Types}
Generally we consider typed theories only.  Types are built from type
variables and type constants by algebra type formation \texttt{(alg
  $\rho_1 \dots \rho_n$)} and arrow type formation $\rho \typeTo
\sigma$.  Product types $\rho \typeProd \sigma$ and sum types $\rho
\typeSum \sigma$ can be seen as algebras with parameters.  However,
for efficiency reasons\footnote{Usage of a primitive product will
  increase efficiency when normalizing proofs via
  normalization-by-evaluation.  This is done by first translating a
  proof into a term, then normalizing the term and finally translating
  it back into a proof.  When translating a proof into a term, for
  instance at existence introduction a term of product type is formed,
  whose components need to be accessed.  Such computations will be
  done at the object level (since we use normalization-by-evaluation)
  and therefore be faster when primitive pairing is used(pp.}  Minlog
also has a primitive product type formation.

We have type constants \texttt{atomic}, \texttt{existential},
\texttt{prop} and \texttt{nulltype}.  They will be used to assign
types to formulas.  E.g., $\forall_n (n=0)$ receives the type
$\texttt{nat} \to \texttt{atomic}$, and $\forall_{n,m} \ex_k(n+m=k)$
receives the type $\texttt{nat} \to \texttt{nat} \to
\texttt{existential}$.  The type \texttt{prop} is used for predicate
variables, e.g., $R$ of arity \texttt{nat,nat -> prop}.  Types of
formulas will be necessary for normalization by evaluation of proof
terms.  The type \texttt{nulltype}\index{nulltype} (written
$\nulltype$ in text and displayed $\verb#eps#$ in Minlog) will be
useful when assigning to a formula the type of a program to be
extracted from a proof of this formula.  Types not involving the types
\texttt{atomic}, \texttt{existential}, \texttt{prop} and
\texttt{nulltype} are called object types.

%% \subsection{Type variables and constants}
Type variable\index{type variable} names are $\texttt{alpha},
\texttt{beta} \dots$; $\texttt{alpha}$ is provided by default.  To
have infinitely many type variables available, we allow appended
indices: $\texttt{alpha}1, \texttt{alpha}2, \texttt{alpha}3 \dots$
will be type variables.  The only type constants%
\index{type constant} are $\texttt{atomic}, \texttt{existential},
\texttt{prop}$ and $\texttt{nulltype}$.

\subsection{Generalities for substitutions, type substitutions}
\label{SS:GenSubst}
Generally, a substitution is a list $((x_1\ t_1) \dots (x_n\ t_n))$ of
lists of length two, with distinct variables $x_i$ and such that for
each $i$, $x_i$ is different from $t_i$.  It is understood as
simultaneous substitution.  The default equality is \texttt{equal?};
however, in the versions ending with \texttt{-wrt} (for \inquotes{with
respect to}) one can provide special notions of equality.  To construct
substitutions we have
\begin{alignat*}{2}
&\texttt{(make-substitution \textsl{args} \textsl{vals})},%
\index{make-substitution@\texttt{make-substitution}}
\\
&\texttt{(make-substitution-wrt \textsl{arg-val-equal?}\ \textsl{args}
\textsl{vals})},%
\index{make-substitution-wrt@\texttt{make-substitution-wrt}}
\\
&\texttt{(make-subst \textsl{arg} \textsl{val})},%
\index{make-subst@\texttt{make-subst}}
\\
&\texttt{(make-subst-wrt \textsl{arg-val-equal?}\ \textsl{arg} \textsl{val})},%
\index{make-subst-wrt@\texttt{make-subst-wrt}}
\\
&\texttt{empty-subst}.\index{empty-subst@\texttt{empty-subst}}
\end{alignat*}
Accessing a substitution is done via the usual access operations
for association list: \texttt{assoc} and \texttt{assoc-wrt}.
We also provide
\begin{alignat*}{2}
  &\texttt{(restrict-substitution-wrt \textsl{subst} \textsl{test?})},%
  \index{restrict-substitution-wrt@\texttt{restrict-substitution-wrt}}
  \\
  &\texttt{(restrict-substitution-to-args \textsl{subst} \textsl{args})},%
  \index{restrict-substitution-to-args@\texttt{restrict-substitution-to-args}}
  \\
  &\texttt{(substitution-equal?\ \textsl{subst1} \textsl{subst2})},%
  \index{substitution-equal?@\texttt{substitution-equal?}}
  \\
  &\texttt{(substitution-equal-wrt?\ \textsl{arg-equal?}\ \textsl{val-equal?}\
    \textsl{subst1} \textsl{subst2})},%
  \index{substitution-equal-wrt?@\texttt{substitution-equal-wrt?}}
  \\
  &\texttt{(subst-item-equal-wrt?\ \textsl{arg-equal?}\ \textsl{val-equal?}\
    \textsl{item1} \textsl{item2})},%
  \index{subst-item-equal-wrt?@\texttt{subst-item-equal-wrt?}}
  \\
  &\texttt{(consistent-substitutions-wrt?}
  \\
  &\qquad\texttt{\textsl{arg-equal?}\
    \textsl{val-equal?}\ \textsl{subst1} \textsl{subst2})}.%
  \index{consistent-substitutions-wrt?@\texttt{consistent-substitutions-wrt?}}
\end{alignat*}

\emph{Composition}\index{composition} $\vartheta \eta$ of two
substitutions
\begin{align*}
  \vartheta &= ((x_1\ s_1) \dots (x_m\ s_m)),
  \\
  \eta &= ((y_1\ t_1) \dots (y_n\ t_n))
\end{align*}
is defined as follows.  In the list $((x_1\ s_1\eta) \dots (x_m\
s_m\eta)\ (y_1\ t_1) \dots (y_n\ t_n))$ remove all bindings $(x_i\
s_i\eta)$ with $s_i\eta = x_i$, and also all bindings $(y_j\ t_j)$
with $y_j \in \{x_1, \dots, x_n\}$.  It is easy to see that
composition is associative, with the empty substitution as unit.
We provide
\begin{alignat*}{2}
  \texttt{(compose-substitutions-wrt}\
  &\texttt{\textsl{substitution-proc} \textsl{arg-equal?}}
  \\
  &\texttt{\textsl{arg-val-equal?}\ \textsl{subst1} \textsl{subst2}}).%
  \index{compose-substitutions-wrt@\texttt{compose-substitutions-wrt}}
\end{alignat*}

We shall have occasion to use these general substitution procedures
for the following kinds of substitutions
\begin{equation*}
  \begin{tabular}{|l|l|l|l|}
    \hline
    for
    &called
    &domain equality
    &arg-val-equality
    \\
    \hline
    type variables
    &\texttt{tsubst}\index{tsubst@\texttt{tsubst}}
    &\texttt{equal?}
    &\texttt{equal?}
    \\
    object variables
    &\texttt{osubst}\index{osubst@\texttt{osubst}}
    &\texttt{equal?}
    &\texttt{var-term-equal?}\index{var-term-equal?@\texttt{var-term-equal?}}
    \\
    predicate variables
    &\texttt{psubst}\index{psubst@\texttt{psubst}}
    &\texttt{equal?}
    &\texttt{pvar-cterm-equal?}%
    \index{pvar-cterm-equal?@\texttt{pvar-cterm-equal?}}
    \\
    assumption variables
    &\texttt{asubst}\index{asubst@\texttt{asubst}}
    &\texttt{avar=?}\index{avar=?@\texttt{avar=?}}
    &\texttt{avar-proof-equal?}%
    \index{avar-proof-equal?@\texttt{avar-proof-equal?}}
    \\
    \hline
  \end{tabular}
\end{equation*}
The following substitutions will make sense for a
\begin{equation*}
  \begin{tabular}{|l|l|}
    \hline
    type
    &\texttt{tsubst}
    \\
    term
    &\texttt{tsubst} and \texttt{osubst}
    \\
    formula
    &\texttt{tsubst} and \texttt{osubst} and \texttt{psubst}
    \\
    proof
    &\texttt{tsubst} and \texttt{osubst} and \texttt{psubst}
    and \texttt{asubst}
    \\
    \hline
  \end{tabular}
\end{equation*}

In particular, for \emph{type substitutions} \texttt{tsubst}
we have
\begin{alignat*}{2}
  &\texttt{(type-substitute \textsl{type} \textsl{tsubst})},%
  \index{type-substitute@\texttt{type-substitute}}
  \\
  &\texttt{(type-subst \textsl{type} \textsl{tvar} \textsl{type1})},%
  \index{type-subst@\texttt{type-subst}}
  \\
  &\texttt{(compose-t-substitutions \textsl{tsubst1} \textsl{tsubst2})}.%
  \index{compose-t-substitutions@\texttt{compose-t-substitutions}}
\end{alignat*}
As display function for type substitutions one can use the general
\texttt{pp-subst}\index{pp-subst@\texttt{pp-subst}} or the special
\begin{align*}
  &\texttt{(display-t-substitution \textsl{tsubst})},%
  \index{display-t-substitution@\texttt{display-t-substitution}}
\end{align*}

We add here some notions and observations on substitutions%
\index{substitution} $\vartheta$ for type, object, predicate and
assumption variables (or topa-substitutions).  Our treatment is based
on (unpublished) work of Buchholz\index{Buchholz}, who introduced the
concept we call \inquotes{admissibility} for substitutions.

Let
\begin{equation*}
  \overline{r^{\rho}} \defeq \rho,
  \quad
  \overline{P^{(\vec{\sigma})}}
  \defeq \overline{\set {\vec{x}^{\vec{\sigma}}} {A}} \defeq
  (\vec{\sigma}),
  \quad
  \overline{M^A} \defeq A.
\end{equation*}
Consider a substitution $\vartheta$ whose domain consists of type
variables $\alpha$, object variables $x$ and predicate variables $P$.
Let
\begin{align*}
  &\alpha \vartheta \defeq
  \begin{cases}
    \vartheta(\alpha) &\text{if $\alpha \in \dom(\vartheta)$},
    \\
    \alpha &\text{otherwise},
  \end{cases}
  \qquad
  x \vartheta \defeq
  \begin{cases}
    \vartheta(x) &\text{if $x \in \dom(\vartheta)$},
    \\
    x &\text{otherwise},
  \end{cases}
  \\
  &P \vartheta \defeq
  \begin{cases}
    \vartheta(P) &\text{if $P \in \dom(\vartheta)$},
    \\
    \set {\vec{x}} {P \vec{x}} &\text{otherwise}.
  \end{cases}
\end{align*}
Call $\vartheta$ \emph{admissible}%
\index{substitution!admissible}\index{admissible} for $x$ if
$\overline{x \vartheta} = \overline{x} \vartheta$, and for $P$ if
$\overline{P \vartheta} = \overline{P} \vartheta$.  We define the
result $r \vartheta$ of carrying out a substitution $\vartheta$ in a
term $r$, provided $\vartheta$ is admissible for all $x \in \FV(r)$
(in short: $\vartheta$ is admissible for $r$).  The definition is by
induction on $r$.  $x \vartheta$ has been defined above, and
\begin{align*}
  c \vartheta &\defeq c,
  \\
  (\lambda_x r)\vartheta &\defeq \lambda_y(r \vartheta_x^y)
  \quad \hbox{with $y$ new, $\overline{y} = \overline{x} \vartheta$},
  \\
  (r s)\vartheta &\defeq (r \vartheta)(s \vartheta).
\end{align*}
To see that this definition makes sense we have to prove

\begin{lemma*}
  If $\vartheta$ is admissible for $\lambda_x r$, then $\vartheta_x^y$
  is admissible for $r$.
\end{lemma*}

\begin{proof}
  Let $z \in \FV(r)$.  We show $\overline{z \vartheta_x^y} =
  \overline{z} \vartheta_x^y$.  \emph{Case} $z \ne x$.
  \begin{equation*}
    \overline{z \vartheta_x^y} = \overline{z \vartheta} =
    \overline{z} \vartheta = \overline{z} \vartheta_x^y
    \quad \hbox{since $\vartheta$ is admissible for $r$}.
  \end{equation*}
  \emph{Case} $z=x$.
  \begin{equation*}
    \overline{x \vartheta_x^y} = \overline{y} =
    \overline{x} \vartheta = \overline{x} \vartheta_x^y
    \quad \hbox{by assumption on $y$}.
    \qedhere
  \end{equation*}
\end{proof}

\begin{lemma*}
  Let $\vartheta$ be admissible for the term $r$.  Then
  $\overline{r \vartheta} = \overline{r} \vartheta$.
\end{lemma*}

\begin{proof}
  \emph{Case} $x$.  $\overline{x \vartheta} = \overline{\vartheta(x)}$
  holds since $\vartheta$ is assumed to be admissible for $x$.

  \emph{Case} $\lambda_x r$.
  \begin{equation*}
    \overline{(\lambda_x r)\vartheta}  =
    \overline{\lambda_y(r \vartheta_x^y)} =
    \overline{y} \typeTo \overline{r \vartheta_x^y} =
    \overline{x} \vartheta \typeTo \overline{r} \vartheta_x^y =
    \overline{x} \vartheta \typeTo \overline{r} \vartheta =
    \overline{(\lambda_x r)}\vartheta.
    \qedhere
  \end{equation*}
\end{proof}

\begin{lemma*}
  Assume that $\vartheta$ is admissible for $r$ and $\eta$ is
  admissible for $r \vartheta$.  Then
  \begin{enumeratea}
  \item $\eta \circ \vartheta$ is admissible for $r$, and
  \item $r \vartheta \eta = r(\eta \circ \vartheta)$.
  \end{enumeratea}
\end{lemma*}

\begin{proof}
  (a).  Let $x \in \FV(r)$.  We show $\overline{x(\eta \circ
    \vartheta)} = \overline{x}(\eta \circ \vartheta)$, i.e.,
  $\overline{x \vartheta \eta} = \overline{x\vartheta} \eta$.
  Consider $x \vartheta$.  Since $\eta$ is admissible for $r
  \vartheta$, it is also admissible for the subterm $x \vartheta$.
  Hence by the previous lemma $\overline{x \vartheta \eta} =
  \overline{x\vartheta} \eta$.

  (b).  We only consider the abstraction case.  By definition
  \begin{align*}
    &(\lambda_x r)\vartheta = \lambda_y(r \vartheta_x^y)
    \quad \hbox{with $y$ new, $\overline{y} = \overline{x} \vartheta$}.
    \\
    &(\lambda_x r)\vartheta \eta = \lambda_y(r \vartheta_x^y) \eta =
    \lambda_z(r \vartheta_x^y \eta_y^z)
    \quad \hbox{with $z$ new, $\overline{z} = \overline{y} \eta$}.
    \\
    &(\lambda_x r)(\eta \circ \vartheta) =
    \lambda_u(r (\eta \circ \vartheta)_x^u)
    \quad \hbox{with $u$ new, $\overline{u} =
      \overline{x}(\eta \circ \vartheta) =
      \overline{x} \vartheta \eta =
      \overline{y} \eta =
      \overline{z}$}.
  \end{align*}
  Hence we may assume $u=z$.  But $\lambda_u(r (\eta \circ
  \vartheta)_x^u) = \lambda_z(r( \eta_y^z \circ \vartheta_x^y))$,
  since $y \notin \FV(r)$ and
  \begin{align*}
    &(\eta \circ \vartheta)_x^u v = v = (\eta_y^z \circ \vartheta_x^y) v
    \quad \hbox{for $v \ne x,y$},
    \\
    &(\eta \circ \vartheta)_x^u x = u = z = (\eta_y^z \circ \vartheta_x^y) x.
  \end{align*}
  By induction hypothesis $\lambda_z(r( \eta_y^z \circ \vartheta_x^y)) =
  \lambda_z(r \vartheta_x^y \eta_y^z)$.
  Hence the claim.
\end{proof}

The result $A \vartheta$ and $\set {\vec{x}} {A} \vartheta$ of
carrying out a substitution $\vartheta$ in a formula $A$ or a
comprehension term $\set {\vec{x}} {A}$ is defined similarly, provided
$\vartheta$ is admissible for the respective expression, and similar
lemmata can be proven.

Now consider a type-object-predicate-assumption substitution
$\vartheta$ with type variables $\alpha$, object variables $x$,
predicate variables $P$ and assumption variables $u$ in its domain.
Again we allow that the type $\sigma$ of $x$ and the arity
$(\vec{\sigma})$ of $P$ depend on type variables $\alpha \in
\dom(\vartheta)$, but we require $\overline{\vartheta(x)} =
\overline{x} \vartheta$ and $\overline{\vartheta(P)} = \overline{P}
\vartheta$.  Moreover we allow that the formula $A$ of $u$ depends on
$\alpha, x, P \in \dom(\vartheta)$, but we require
$\overline{\vartheta(u)} = \overline{u} \vartheta$.  Let
\begin{equation*}
  u \vartheta \defeq
  \begin{cases}
    \vartheta(u) &\text{if $u \in \dom(\vartheta)$},
    \\
    u &\text{otherwise}.
  \end{cases}
\end{equation*}
Call a type-object-predicate-assumption substitution
\emph{admissible}\index{substitution!admissible}\index{admissible} for
a derivation $M$ if for all $x, P, u \in \FV(M)$ we have $\overline{x
  \vartheta} = \overline{x} \vartheta$, $\overline{P \vartheta} =
\overline{P} \vartheta$ and $\overline{u \vartheta} = \overline{u}
\vartheta$.  The result $M \vartheta$ of carrying out a substitution
$\vartheta$ in a derivation $M$ is defined as follows, provided
$\vartheta$ is admissible for $M$.  We define $M \vartheta$ by
induction on $M$.
\begin{align*}
  c \vartheta &\defeq c,
  \\
  (\lambda_x M)\vartheta &\defeq \lambda_y(M \vartheta_x^y)
  \quad \hbox{with $y$ new, $\overline{y} = \overline{x} \vartheta$},
  \\
  (M r)\vartheta &\defeq (M \vartheta)(r \vartheta),
  \\
  (\lambda_u M)\vartheta &\defeq \lambda_v(M \vartheta_u^v)
  \quad \hbox{with $v$ new, $\overline{v} = \overline{u} \vartheta$},
  \\
  (M N)\vartheta &\defeq (M \vartheta)(N \vartheta).
\end{align*}
Again lemmata similar to those above can be proven.

As test for the admissibility of a substitution we provide
\begin{align*}
  &\texttt{(admissible-substitution?\ \textsl{topasubst} \textsl{expr})}.%
  \index{admissible-substitution?@\texttt{admissible-substitution?}}
\end{align*}

\subsection{Type unification and matching}
\label{SS:TypeUnifMatch}
We need type unification for object types only, that is, types built
from type variables and algebra types by arrow and star.  However, the
type constants \texttt{atomic}, \texttt{existential}, \texttt{prop}
and \texttt{nulltype} do not do any harm and can be included.

\texttt{type-unify} checks whether two terms can be unified.  It
returns \texttt{\#f}, if this is impossible, and a most general unifier
otherwise.  \texttt{type-unify-list} does the same for lists of terms.
We provide
\begin{alignat*}{2}
  &\texttt{(type-unify \textsl{type1} \textsl{type2})},%
  \index{type-unify@\texttt{type-unify}}
  \\
  &\texttt{(type-unify-list \textsl{types1} \textsl{types2})}.%
  \index{type-unify-list@\texttt{type-unify-list}}
\end{alignat*}

Notice that the algorithm we use (via disagreement pairs) does not
yield idempotent unifiers (as opposed to the Martelli-Montanari
algorithm \cite{Martelli82} in \texttt{modules/type-inf.scm}):

\begin{verbatim}
(pp-subst (type-unify (py "alpha1=>alpha2=>boole")
                      (py "alpha2=>alpha1=>alpha1")))
;;   alpha2 -> boole
;;   alpha1 -> alpha2
\end{verbatim}

\texttt{type-match} checks whether a given pattern can be transformed
by a substitution into a given instance.  It returns \texttt{\#f}, if
this is impossible, and the substitution otherwise.
\texttt{type-match-list} does the same for lists of terms.
We provide
\begin{alignat*}{2}
  &\texttt{(type-match \textsl{pattern} \textsl{instance})},%
  \index{type-match@\texttt{type-match}}
  \\
  &\texttt{(type-match-list \textsl{patterns} \textsl{instances})}.%
  \index{type-match-list@\texttt{type-match-list}}
\end{alignat*}

\subsection{Algebras and types}
\label{SS:AlgsTypes}
We now consider concrete information systems, our basis for continuous
functionals.

Types will be built from base types by the formation of function
types, $\rho \typeTo \sigma$.  As domains for the base types we choose
non-flat and possibly infinitary free algebras, given by their
constructors.  The main reason for taking non-flat base domains is
that we want the constructors to be injective and with disjoint
ranges.  This generally is not the case for flat domains.

In our constructors of an algebra we allow a certain
\inquotes{nesting}\index{algebra!nested} w.r.t.\ already generated
algebras $\bar{\iota}$.  Then the cototal ideals of this algebra will
\inquotes{incorporate} total ideals of type $\bar{\iota}$.  An example
are finitely branching non-wellfounded trees, where $\bar{\iota}$ is
the list type.  Such cototal ideals will be useful as witnesses of
\inquotes{nested coinductive/inductive definitions}.

We inductively define \emph{type forms}%
\index{type form}
\begin{equation*}
  \rho, \sigma \BNFdef \alpha \BNFor \rho \typeTo \sigma \BNFor
  \mu_{\xi}((\rho_{i \nu})_{\nu < n_i} \typeTo \xi)_{i<k}
\end{equation*}
with $\alpha, \xi$ type variables and $k \ge 1$ (since we want our
algebras to be inhabited).  Note that $(\rho_{\nu})_{\nu < n} \typeTo
\sigma$ means $\rho_0 \typeTo \dots \typeTo \rho_{n-1} \typeTo
\sigma$, associated to the right.

Let $\FV(\rho)$ denote the set of type variables free in $\rho$.  We
define $\SP(\alpha, \rho)$ \inquotes{$\alpha$ occurs at most
  \emph{strictly positive}%
\index{strictly positive} in $\rho$} by induction on $\rho$.
\begin{equation*}
  \SP(\alpha, \beta)
  \qquad
  \frac{\alpha \notin \FV(\rho)\quad \SP(\alpha, \sigma)}
       {\SP(\alpha, \rho \typeTo \sigma)}
       \qquad
       \frac{\hbox{$\SP(\alpha, \rho_{i\nu})$ for all $i<k$, $\nu<n_i$}}
            {\SP(\alpha, \mu_{\xi}((\rho_{i\nu})_{\nu<n_i} \typeTo \xi)_{i<k})}
\end{equation*}
Now we can define $\Ty(\rho)$ \inquotes{$\rho$ is a
  \emph{type}\index{type}}, again by induction on $\rho$.
\begin{align*}
  &\Ty(\alpha)
  \qquad
  \frac{\Ty(\rho) \quad \Ty(\sigma)}
       {\Ty(\rho \typeTo \sigma)}
  \\
  &\frac{\hbox{$\Ty(\rho_{i\nu})$ and $\SP(\xi, \rho_{i\nu})$
      for all $i<k$, $\nu<n_i$
      \quad
      $\xi \notin \FV(\rho_{0\nu})$ for all $\nu<n_0$}}
       {\Ty(\mu_{\xi}((\rho_{i\nu})_{\nu<n_i} \typeTo \xi)_{i<k})}
\end{align*}
We call
\begin{equation*}
  \iota \defeq \mu_{\xi}((\rho_{i\nu})_{\nu<n_i} \typeTo \xi)_{i<k}
\end{equation*}
an \emph{algebra}\index{algebra}.  Sometimes it is helpful to display
the type parameters and write $\iota(\vec{\alpha}, \vec{\beta}\,)$,
where $\vec{\alpha}, \vec{\beta}$ are all type variables except $\xi$
free in some $\rho_{i\nu}$, and $\vec{\alpha}$ are the ones occuring
only strictly positive.  If we write the $i$-th component of $\iota$
in the form $(\rho_{\nu}(\xi))_{\nu<n} \typeTo \xi$, then we call
\begin{equation*}
  (\rho_{\nu}(\iota))_{\nu<n} \typeTo \iota
\end{equation*}
the $i$-th \emph{constructor type}%
\index{constructor type} of $\iota$.

In $(\rho_{\nu}(\xi))_{\nu < n} \to \xi$ we call
$\rho_{\nu}(\xi)$ a \emph{parameter} argument type%
\index{parameter argument type} if $\xi$ does not occur in it, and a
\emph{recursive} argument type%
\index{recursive argument type} otherwise.
A recursive argument type $\rho_{\nu}(\xi)$ is \emph{nested} if it has
an occurrence of $\xi$ in a strictly positive parameter position of
another (previously defined) algebra, and unnested otherwise.  An
algebra $\iota$ is called \emph{nested}\index{algebra!nested} if it
has a constructor with at least one nested recursive argument type,
and \emph{unnested}\index{algebra!unnested} otherwise.

Every type $\rho$ should have a \emph{total inhabitant}%
\index{inhabitant!total}, i.e., a closed term of this type built
solely from constructors, variables and assumed total inhabitants of
some of its (type) variables.  To ensure this we have required
that for every algebra $\mu_{\xi} ((\rho_{i\nu})_{\nu<n_i} \typeTo
\xi)_{i<k}$ the initial $(\rho_{0\nu})_{\nu<n_0} \typeTo \xi$ has
no recursive argument types.  Note that it might not be necessary to
actually use assumed total inhabitants for all variables of a type.
An example is the list type $\typeL{\alpha}$, which has the $\nil$
constructor as a total inhabitant.  However, for the type
$\typeL{\alpha}^{+}$ ($\defeq \mu_{\xi}(\alpha \typeTo \xi, \alpha
\typeTo \xi \typeTo \xi)$) we need to assume a total inhabitant of
$\alpha$.

Here are some examples of algebras.
\begin{alignat*}{2}
  &\typeUnit &&\defeq \mu_{\xi} \xi \qquad \hbox{(unit)},
  \\
  &\typeB &&\defeq \mu_{\xi} (\xi, \xi) \qquad \hbox{(booleans)},
  \\
  &\typeN &&\defeq \mu_{\xi} (\xi, \xi \typeTo \xi) \qquad
  \hbox{(natural numbers, unary)},
  \\
  &\typeP &&\defeq \mu_{\xi} (\xi, \xi \typeTo \xi, \xi \typeTo \xi)
  \qquad \hbox{(positive numbers, binary)},
  \\
  &\typeBin &&\defeq \mu_{\xi} (\xi, \xi \typeTo \xi \typeTo \xi)
  \qquad \hbox{(binary trees, or derivations)},
  \\
  &\typeOrd &&\defeq \mu_{\xi} (\xi, \xi \typeTo \xi, (\typeN \typeTo
  \xi) \typeTo \xi) \qquad \hbox{(ordinals)},
  \\
  &\typeTree_0 &&\defeq \typeN, \quad \typeTree_{n+1} \defeq \mu_{\xi}
  (\xi, (\typeTree_n \typeTo \xi) \typeTo \xi) \qquad
  \hbox{(trees)}.
\end{alignat*}
Examples of algebras strictly positive in their type parameters are
\begin{alignat*}{2}
  &\typeL{\alpha} &&\defeq \mu_{\xi} (\xi, \alpha \typeTo \xi \typeTo
  \xi) \qquad \hbox{(lists)},
  \\
  &\alpha \typeProd \beta &&\defeq \mu_{\xi} (\alpha \typeTo \beta
  \typeTo \xi) \qquad \hbox{(product)},
  \\
  &\alpha \typeSum \beta &&\defeq \mu_{\xi} (\alpha \typeTo \xi, \beta
  \typeTo \xi) \qquad \hbox{(sum)}.
\end{alignat*}
An example of a nested algebra is
\begin{equation*}
  \typeTree \defeq \mu_{\xi} (\typeL{\xi} \typeTo \xi)
  \quad \hbox{(finitely branching trees)}.
\end{equation*}
Note that $\typeTree$ has a total inhabitant since $\typeL{\alpha}$ has
one (given by the $\nil$ constructor).

\begin{remark*}[Substitution for type parameters]
  Let $\rho \in \Ty(\vec{\alpha}\,)$; we write $\rho(\vec{\alpha}\,)$
  for $\rho$ to indicate its dependence on the type parametes
  $\vec{\alpha}$.  We can substitute types $\vec{\sigma}$ for
  $\vec{\alpha}$, to obtain $\rho(\vec{\sigma})$.  Examples are
  $\typeL{\typeB}$, the type of lists of booleans, and $\typeN
  \typeProd \typeN$, the type of pairs of natural numbers.

  Note that often there are many equivalent ways to define a
  particular type.  For instance, we could take $\typeUnit \typeSum
  \typeUnit$ to be the type of booleans, $\typeL{\typeUnit}$ to be the
  type of natural numbers, and $\typeL{\typeB}$ to be the type of
  positive binary numbers.
\end{remark*}

For every constructor type $\kappa_i(\xi)$ of an algebra $\iota =
\mu_{\xi}(\vec{\kappa}\,)$ we provide a (typed) \emph{constructor
  symbol}%
\index{constructor symbol} $\constr_i$ of type $\kappa_i(\iota)$.  In
some cases they have standard names, for instance
\begin{alignat*}{2}
  &\true^{\typeB}, \false^{\typeB} \quad \hbox{for the two
    constructors of the type $\typeB$ of booleans},
  \\
  &0^{\typeN},\suc^{\typeN \typeTo \typeN} \quad \hbox{for the type
    $\typeN$ of (unary) natural numbers},
  \\
  &\one^{\typeP}, \s_0^{\typeP \typeTo \typeP}, \s_1^{\typeP \typeTo
    \typeP}
  \quad \hbox{for the type $\typeP$ of (binary) positive numbers},
  \\
  &\nil^{\typeL{\rho}}, \cons^{\rho \typeTo \typeL{\rho} \typeTo
    \typeL{\rho}} \quad \hbox{for the type $\typeL{\rho}$ of lists},
  \\
  &(\termSumIntroLeft_{{\rho}{\sigma}})^{\rho \typeTo \rho \typeSum
    \sigma}, (\termSumIntroRight_{{\sigma}{\rho}})^{\sigma \typeTo
    \rho \typeSum \sigma} \quad \hbox{for the sum type $\rho \typeSum
    \sigma$},
  \\
  &\branch \colon \typeL{\typeTree} \typeTo \typeTree \quad \hbox{for
    the type $\typeTree$ of finitely branching trees}.
\end{alignat*}
We denote the constructors of the type $\typeBin$ of derivations by
$0^{\typeBin}$ (axiom) and $\constr^{\typeBin \typeTo \typeBin \typeTo
  \typeBin}$ (rule).  Another example uses the parametrized algebra
\begin{equation*}
  \typeRead(\alpha) \defeq
  \mu_{\xi}(\alpha \typeTo \xi, \alpha \typeTo \xi, \alpha \typeTo \xi,
  \xi \typeTo \xi \typeTo \xi \typeTo \xi)
\end{equation*}
(labelled read-and-finally-write-one-digit trees), whose constructors
we name
\begin{alignat*}{2}
  &\termPut_d \colon \alpha \typeTo \typeRead(\alpha)
  \quad \hbox{($d \in \{-1,0,1\}$)} &\quad&
  \hbox{finally write $d$ and continue},
  \\
  &\termGet \colon
  \typeRead(\alpha) \typeTo \typeRead(\alpha) \typeTo \typeRead(\alpha)
  \typeTo \typeRead(\alpha) &\quad&
  \hbox{read}.
\end{alignat*}
Using $\typeRead(\alpha)$ we then define
\begin{equation*}
  \typeWrite \defeq
  \mu_{\xi}(\xi, \typeRead(\xi) \typeTo \xi)
  \quad \hbox{(nested alternating read-write trees)}
\end{equation*}
with constructors
\begin{alignat*}{2}
  &\termStop \colon \typeWrite &\qquad& \hbox{stop},
  \\
  &\termCont \colon \typeRead(\typeWrite) \typeTo \typeWrite &\qquad&
  \hbox{branch by applying a read-write instruction,}
  \\
  & &\qquad&
  \hbox{and continue}.
\end{alignat*}
Later we will consider ideals built with finite read-write
instructions, but infinitely many alternations, via a \inquotes{nested
  inductive/coinductive} definition.

One can extend the definition of algebras and types to
\emph{simultaneously defined algebras}%
\index{algebra!simultaneously defined}.  Instead of $\xi$ we consider a
list $\vec{\xi} = \xi_0, \dots, \xi_{N-1}$ of typevariables and
generate $\vec{\xi},j$-constructor types ($j<N$) by
\begin{equation*}
  \frac{(\rho_{i \nu}(\vec{\beta}\,) \in
    \Ty(\vec{\alpha}, \vec{\beta}\,))_{i<k; \nu < n_i}}
       {( (\rho_{i \nu}(\vec{\xi}\,))_{\nu < n_i} \to \xi_j) \in
         \constrtypes{\vec{\xi},j}\,(\vec{\alpha}\,)}.
\end{equation*}
Let $k = \sum_{j<N} k_j$ with $k_j \ge 1$ and $m_j \defeq \sum_{l<j}
k_j$, hence m$_j + k_j = m_{j+1}$.  For $m_j \le i < m_{j+1}$ let
$\kappa_i \in \constrtypes{\vec{\xi},j}\,(\vec{\alpha}\,)$.  Then all
$\iota_j$ in $\vec{\iota} \defeq \mu_{\vec{\xi}}(\kappa_0, \dots,
\kappa_{k-1})$ are in $\mathrm{Alg}(\vec{\alpha}\,)$.  To ensure total
inhabitants of the algebra we require that the initial constructor
type for $\iota_j$ has argument types involving $\iota_i$ for $i<j$
only. --- Examples of simultaneously defined algebras are
\begin{alignat*}{2}
  &(\typeEv, \typeOd) &&\defeq \mu_{\xi, \zeta}(\xi, \zeta \typeTo \xi,
  \xi \typeTo \zeta)
  \quad \hbox{(even and odd numbers),}
  \\
  &(\typeTlist(\rho), \typeTree(\rho)) &&\defeq
  \mu_{\xi, \zeta} (\xi, \zeta \typeTo \xi \typeTo \xi,
  \rho \typeTo \zeta, \xi \typeTo \zeta)
  \quad \hbox{(tree lists and trees).}
\end{alignat*}
$\typeTree(\rho)$ defines finitely branching trees, and
$\typeTlist(\rho)$ finite lists of such trees; the trees carry objects
of a type $\rho$ at their leaves.  The constructor symbols and their
types are
\begin{equation*}
  \begin{split}
    &\emp^{\typeTlist(\rho)}, \quad
    \tcons^{\typeTree(\rho) \typeTo \typeTlist(\rho) \typeTo
      \typeTlist(\rho)},
    \\
    &\leaf^{\rho \typeTo \typeTree(\rho)}, \quad
    \branch^{\typeTlist(\rho) \typeTo \typeTree(\rho)}.
  \end{split}
\end{equation*}
However, for simplicity we often consider non-simultaneous algebras only.

An algebra form $\iota$ is \emph{structure-finitary}%
\index{algebra!structure-finitary} if in its generation the rule
leading to $\sigma \typeTo \rho$ has not been used.  It is
\emph{finitary}\index{algebra!finitary} if in addition it has no type
variables.  In the examples above $\typeUnit$, $\typeB$, $\typeN$,
$\typeP$ and $\typeBin$ are all finitary, but $\typeOrd$ and
$\typeTree_{n+1}$ are not.  $\typeL{\rho}$, $\rho \typeProd \sigma$
and $\rho \typeSum \sigma$ are structure-finitary, and finitary if
their parameter types are.  The nested algebra $\typeTree$ above is
finitary.  An algebra is \emph{explicit}\index{algebra!explicit} if
all its constructor types have parameter argument types only (i.e., no
recursive argument types).  In the examples above $\typeUnit$,
$\typeB$, $\rho \typeProd \sigma$ and $\rho + \sigma$ are explicit,
but $\typeN$, $\typeP$, $\typeL{\rho}$, $\typeBin$, $\typeOrd$,
$\typeTree_{n+1}$ and $\typeTree$ are not.

We will also need the notion of the \emph{level}%
\index{type!level of} of a type, which is defined by
\begin{align*}
  &\lev{\iota} \defeq 0,
  \\
  &\lev{\rho \typeTo \sigma} \defeq \max(\lev{\sigma}, 1+\lev{\rho}),
  \\
  &\lev{\rho \typeProd \sigma} \defeq \max(\lev{\sigma}, \lev{\rho})
  \qquad \hbox{for the primitive product}.
\end{align*}
\emph{Base} types%
\index{type!base} are types of level $0$, and a \emph{higher}
type\index{type!higher} has level at least $1$.

For a type $\rho(\vec{\alpha}\,) \in \Ty(\vec{\alpha}\,)$ (hence also
for an algebra $\iota(\vec{\alpha}\,) \in
\mathrm{Alg}(\vec{\alpha}\,)$) we define the \emph{map} operator%
\index{map operator}
\begin{equation*}
  \Map_{\lambda_{\vec{\alpha}}\rho(\vec{\alpha}\,)}^{\vec{\sigma} \to  \vec{\tau}} \colon
  \rho(\vec{\sigma}\,) \typeTo (\vec{\sigma} \typeTo \vec{\tau}\,) \typeTo
  \rho(\vec{\tau}\,)
\end{equation*}
(where $(\vec{\sigma} \typeTo \vec{\tau}) \typeTo \rho(\vec{\tau}\,)$
means $(\sigma_1 \typeTo \tau_1) \typeTo \dots \typeTo (\sigma_n
\typeTo \tau_n) \typeTo \rho(\vec{\tau}\,)$).  If none of
$\vec{\alpha}$ appears free in $\rho(\vec{\alpha}\,)$ let
\begin{equation*}
  \Map_{\lambda_{\vec{\alpha}}\rho(\vec{\alpha}\,)}^{\vec{\sigma} \to  \vec{\tau}} x \vec{f}
  \defeq x.
\end{equation*}
Otherwise we use an outer recursion on $\rho(\vec{\alpha}\,)$
and if $\rho(\vec{\alpha}\,)$ is $\iota(\vec{\alpha}\,)$ an inner
one on $x$.  In case $\rho(\vec{\alpha}\,)$ is $\iota(\vec{\alpha}\,)$
we abbreviate
$\Map_{\lambda_{\vec{\alpha}}\iota(\vec{\alpha}\,)}^{\vec{\sigma} \to
  \vec{\tau}}$ by $\Map_{\iota}^{\vec{\sigma} \to \vec{\tau}}$ or
$\Map_{\iota(\vec{\sigma}\,)}^{\vec{\tau}}$.

The immediate cases for the outer recursion are
\begin{equation*}
  \Map_{\lambda_{\vec{\alpha}}\alpha_i}^{\vec{\sigma} \to  \vec{\tau}} x \vec{f} \defeq
  f_i x,
  \qquad
  \Map_{\lambda_{\vec{\alpha}}(\sigma \typeTo \rho)}^{\vec{\sigma} \to  \vec{\tau}}
  h \vec{f} x \defeq
  \Map_{\lambda_{\vec{\alpha}}\rho}^{\vec{\sigma} \to  \vec{\tau}} (h x) \vec{f}.
\end{equation*}
It remains to consider $\iota(\vec{\pi}(\vec{\alpha}\,))$.  In case
$\vec{\pi}(\vec{\alpha}\,)$ is not $\vec{\alpha}$ let
\begin{equation*}
  \Map_{\lambda_{\vec{\alpha}}\iota(\vec{\pi}(\vec{\alpha}\,))}
  ^{\vec{\sigma} \to  \vec{\tau}} x \vec{f} \defeq
  \Map_{\iota}^{\vec{\pi}(\vec{\sigma}\,) \to \vec{\pi}(\vec{\tau}\,)}
  x (\Map_{\lambda_{\vec{\alpha}}\pi_i(\vec{\alpha}\,)}^{\vec{\sigma} \to \vec{\tau}}
  \cdot \vec{f}\;)_{i<|\vec{\pi}\,|}
\end{equation*}
with $\Map_{\lambda_{\vec{\alpha}}\pi_i(\vec{\alpha}\,)}
^{\vec{\sigma} \to \vec{\tau}} \cdot \vec{f} \defeq \lambda_x
\Map_{\lambda_{\vec{\alpha}}\pi_i(\vec{\alpha}\,)}^{\vec{\sigma} \to
  \vec{\tau}} x \vec{f}$.  In case $\vec{\pi}(\vec{\alpha}\,)$ is
$\vec{\alpha}$ we use recursion on $x$ and define for a constructor
$\constr_i \colon (\rho_{\nu}(\vec{\sigma},
\iota(\vec{\sigma}\,)))_{\nu < n} \typeTo \iota(\vec{\sigma}\,)$
\begin{equation*}
  \Map_{\iota}^{\vec{\sigma} \to \vec{\tau}}(\constr_i \vec{x}\,) \vec{f}
\end{equation*}
to be the result of applying $\constr'_i$ of type
$(\rho_{\nu}(\vec{\tau}, \iota(\vec{\tau}\,)))_{\nu < n}
\typeTo \iota(\vec{\tau}\,)$ (the same constructor as
$\constr_i$ with only the type changed) to, for each $\nu < n$,
\begin{equation*}
  \Map_{\lambda_{\vec{\alpha}, \beta}\rho_{\nu}(\vec{\alpha}, \beta)}
  ^{\vec{\sigma}, \iota(\vec{\sigma}\,) \to \vec{\tau}, \iota(\vec{\tau}\,)}
  x_{\nu} \vec{f} (\Map_{\iota}
  ^{\vec{\sigma} \to \vec{\tau}} \cdot \vec{f}\,).
\end{equation*}
Note that the final function argument provides the recursive call
w.r.t.\ the recursion on $x$.

Here are some examples.
\begin{align*}
  &\Map_{\typeL{\sigma}}^{\tau} \nil f^{\sigma \typeTo \tau} \defeq
  \nil,
  \\
  &\Map_{\typeL{\sigma}}^{\tau}(x^{\sigma} :: l^{\typeL{\sigma}})
  f^{\sigma \typeTo \tau} \defeq {}
  (f x)::(\Map \,l\, f),
  \\[6pt]
  &\Map_{\typeRead(\sigma)}^{\tau}
  (\termPut_d^{\sigma \typeTo \typeRead(\sigma)} x^{\sigma}) f^{\sigma \typeTo \tau}
   \defeq {}
  \termPut_d^{\tau \typeTo \typeRead(\tau)}(f x)
  \quad \hbox{($d \in \{-1,0,1\}$)},
  \\
  &\Map_{\typeRead(\sigma)}^{\tau}
  (\termGet\, y_1^{\typeRead(\sigma)} y_2^{\typeRead(\sigma)} y_3^{\typeRead(\sigma)})
  f^{\sigma \typeTo \tau}
  \defeq {}
  \termGet(\Map\,y_1\,f)(\Map\,y_2\,f)(\Map\,y_3\,f).
\end{align*}

A slightly more complex example is the nested algebra
$\textbf{T}(\alpha)$ with the single constructor $\constr \colon
\typeL{\alpha \typeSum \textbf{T}(\alpha)} \typeTo
\textbf{T}(\alpha)$:
\begin{equation*}
  \Map_{\textbf{T}(\sigma)}^{\tau}
  (\constr x^{\typeL{\sigma \typeSum \textbf{T}(\sigma)}}) f \defeq
  \constr'( \Map_{\lambda_{\alpha, \beta}\typeL{\alpha \typeSum \beta}}^
          {\sigma,\textbf{T}(\sigma) \to \tau, \textbf{T}(\tau)} x f g)
\end{equation*}
with $g \colon \textbf{T}(\sigma) \typeTo \textbf{T}(\tau)$ defined by
$\Map_{\textbf{T}(\sigma)}^{\tau} \cdot f$.

To add and remove names for type variables, we use
\begin{align*}
  &\texttt{(add-tvar-name \textsl{name1} \dots)},%
  \index{add-tvar-name@\texttt{add-tvar-name}}
  \\
  &\texttt{(remove-tvar-name \textsl{name1} \dots)}.%
  \index{remove-tvar-name@\texttt{remove-tvar-name}}
\end{align*}
We need a constructor, accessors and a test for type variables.
\begin{alignat*}{2}
  &\texttt{(make-tvar \textsl{index} \textsl{name})}%
  \index{make-tvar@\texttt{make-tvar}}
  &\quad& \text{constructor},
  \\
  &\texttt{(tvar-to-index \textsl{tvar})} && \text{accessor},%
  \index{tvar-to-index@\texttt{tvar-to-index}}
  \\
  &\texttt{(tvar-to-name \textsl{tvar})} && \text{accessor},%
  \index{tvar-to-name@\texttt{tvar-to-name}}
  \\
  &\texttt{(tvar?\ \textsl{x})}.%
  \index{tvar?@\texttt{tvar?}}
\end{alignat*}
To generate new type variables we use
\begin{align*}
  &\texttt{(new-tvar)}.%
  \index{new-tvar@\texttt{new-tvar}}
\end{align*}

To introduce (possibly simultaneous) free algebras we use
\begin{equation*}
  \texttt{add-algs}\index{add-algs@\texttt{add-algs}}.
\end{equation*}
Examples are
\begin{verbatim}
(add-algs "nat" '("Zero" "nat") '("Succ" "nat=>nat"))

(add-algs "list" 'prefix-typeop
	  '("list" "Nil")
	  '("alpha=>list=>list" "Cons"))

(add-algs (list "ltlist" "ltree") 'prefix-typeop
	  '("ltlist" "LEmpty")
	  '("ltree=>ltlist=>ltlist" "LTcons")
	  '("alpha=>ltree" "LLeaf")
	  '("ltlist=>ltree" "LBranch"))
\end{verbatim}
The final example simultaneously introduces the two free algebras .
The constructors are introduced as \inquotes{self-evaluating}
constants; they play a special role in our semantics for normalization
by evaluation.

For already introduced algebras we need constructors and accessors
\begin{align*}
  &\texttt{(make-alg \textsl{name} \textsl{type1} \dots)},%
  \index{make-alg@\texttt{make-alg}}
  \\
  &\texttt{(alg-form-to-name \textsl{alg})},%
  \index{alg-form-to-name@\texttt{alg-form-to-name}}
  \\
  &\texttt{(alg-form-to-types \textsl{alg})},%
  \index{alg-form-to-types@\texttt{alg-form-to-types}}
  \\
  &\texttt{(alg-name-to-simalg-names \textsl{alg-name})},%
  \index{alg-name-to-simalg-names@\texttt{alg-name-to-simalg-names}}
  \\
  &\texttt{(alg-name-to-token-types \textsl{alg-name})},%
  \index{alg-name-to-token-types@\texttt{alg-name-to-token-types}}
  \\
  &\texttt{(alg-name-to-typed-constr-names \textsl{alg-name})},%
  \index{alg-name-to-typed-constr-names@\texttt{alg-name-to-typed-constr-names}}
  \\
  &\texttt{(alg-name-to-tvars \textsl{alg-name})},%
  \index{alg-name-to-tvars@\texttt{alg-name-to-tvars}}
  \\
  &\texttt{(alg-name-to-arity \textsl{alg-name})}
  \index{alg-name-to-arity@\texttt{alg-name-to-arity}}.
\end{align*}
We also provide the tests
\begin{alignat*}{2}
  &\texttt{(alg-form?\ \textsl{x})} &\quad& \text{incomplete test},%
  \index{alg-form?@\texttt{alg-form?}}
  \\
  &\texttt{(alg?\ \textsl{x})} && \text{complete test},%
  \index{alg?@\texttt{alg?}}
  \\
  &\texttt{(finalg?\ \textsl{type})} && \text{incomplete test},%
  \index{finalg?@\texttt{finalg?}}
  \\
  &\texttt{(sfinalg?\ \textsl{type})} && \text{incomplete test},%
  \index{sfinalg?@\texttt{sfinalg?}}
  \\
  &\texttt{(nested-alg-name?\ \textsl{name})} && \text{complete test},%
  \index{nested-alg-name?@\texttt{nested-alg-name?}}
  \\
  &\texttt{(ground-type?\ \textsl{x})} && \text{incomplete test}
  \index{ground-type?@\texttt{ground-type?}}.
\end{alignat*}

To remove names for algebras we use
\begin{align*}
  &\texttt{(remove-alg-name \textsl{name1} \dots)}
  \index{remove-alg-name@\texttt{remove-alg-name}}.
\end{align*}

Standard examples for finitary free algebras are the type \texttt{nat}
of unary natural numbers, and the algebra of binary trees.

Minlog initially provides the finitary free algebra
\texttt{unit}\index{unit@\texttt{unit}} consisting of exactly one
element, and \texttt{boole}\index{boole@\texttt{boole}} of booleans;
objects of the latter type are
(cf.\ \cite{BergerEberlSchwichtenberg03})\ \texttt{true},
\texttt{false} and families of terms of this type, and in addition the
bottom object of type \texttt{boole}.  Moreover, Minlog initially has
the structure-finitary algebras
\texttt{yprod}\index{yprod@\texttt{yprod}} for product types $\rho
\typeProd \sigma$ and \texttt{ysum}\index{ysum@\texttt{ysum}} for sum
types $\rho \typeSum \sigma$.  For convenience and readability there
are also structure-finitary algebras for sum types where one component
is the unit type: \texttt{uysum}\index{uysum@\texttt{uysum}} for
$\typeUnit \typeSum \sigma$ and
\texttt{ysumu}\index{ysumu@\texttt{ysumu}} for $\rho \typeSum
\typeUnit$.

Tests:
\begin{align*}
  &\texttt{(arrow-form?\ \textsl{type})},%
  \index{arrow-form?@\texttt{arrow-form?}}
  \\
  &\texttt{(star-form?\ \textsl{type})}%
  \index{star-form?@\texttt{star-form?}}
  \quad \hbox{for the primitive product},
  \\
  &\texttt{(object-type?\ \textsl{type})}.%
  \index{object-type?@\texttt{object-type?}}
\end{align*}

We also need constructors and accessors for arrow types
\begin{alignat*}{2}
  &\texttt{(make-arrow \textsl{arg-type} \textsl{val-type})}
  \index{make-arrow@\texttt{make-arrow}}
  &\quad& \text{constructor},
  \\
  &\texttt{(arrow-form-to-arg-type \textsl{arrow-type})}
  \index{arrow-form-to-arg-type@\texttt{arrow-form-to-arg-type}}
  && \text{accessor},
  \\
  &\texttt{(arrow-form-to-val-type \textsl{arrow-type})}
  \index{arrow-form-to-val-type@\texttt{arrow-form-to-val-type}}
  && \text{accessor}
\end{alignat*}
and star types, i.e., the primitive product,
\begin{alignat*}{2}
  &\texttt{(make-star \textsl{type1} \textsl{type2})}
  \index{make-star@\texttt{make-star}}
  &\quad& \text{constructor},
  \\
  &\texttt{(star-form-to-left-type \textsl{star-type})}
  \index{star-form-to-left-type@\texttt{star-form-to-left-type}}
  && \text{accessor},
  \\
  &\texttt{(star-form-to-right-type star-type)}
  \index{star-form-to-right-type@\texttt{star-form-to-right-type}}
  && \text{accessor.}
\end{alignat*}
For convenience we also have
\begin{alignat*}{2}
  &\texttt{(mk-arrow \textsl{type1} \dots\ \textsl{type})},%
  \index{mk-arrow@\texttt{mk-arrow}}
  \\
  &\texttt{(arrow-form-to-arg-types \textsl{type} <\textsl{n}>)}
  \index{arrow-form-to-arg-types@\texttt{arrow-form-to-arg-types}}
  &\quad& \text{all (first $n$) argument types}
  \\
  &\texttt{(arrow-form-to-final-val-type \textsl{type})}
  \index{arrow-form-to-final-val-type@\texttt{arrow-form-to-final-val-type}}
  && \text{type of final value.}
\end{alignat*}
A test function for types is
\begin{align*}
  &\texttt{(type?\ \textsl{x})}.
  \index{type?@\texttt{type?}}
\end{align*}
For displaying types we have
\begin{equation*}
  \texttt{(type-to-string \textsl{type})},
  \index{type-to-string@\texttt{type-to-string}}
\end{equation*}
which is defined by
\begin{equation*}
  \texttt{(token-tree-to-string (type-to-token-tree \textsl{type}))}.%
  \index{type-to-token-tree@\texttt{type-to-token-tree}}%
  \index{token-tree-to-string@\texttt{token-tree-to-string}}
\end{equation*}
For better line breaks in the display one can use
\begin{equation*}
  \texttt{(pp \textsl{type})}\index{pp@\texttt{pp}},
\end{equation*}
which is defined by
\begin{equation*}
  \texttt{(token-tree-to-pp-tree (type-to-token-tree \textsl{type}))}.
  \index{type-to-token-tree@\texttt{type-to-token-tree}}
  \index{token-tree-to-pp-tree@\texttt{token-tree-to-pp-tree}}
\end{equation*}

\subsection{Coercion}
\label{SS:Coercion}
To develop analysis we use a subtype relation generated from
$\mathtt{pos} < \mathtt{nat} < \mathtt{int} < \mathtt{rat} <
\mathtt{real} < \mathtt{cpx}$.  We view
\texttt{pos}\index{pos@\texttt{pos}},
\texttt{nat}\index{nat@\texttt{nat}},
\texttt{int}\index{int@\texttt{int}},
\texttt{rat}\index{rat@\texttt{rat}},
\texttt{real}\index{real@\texttt{real}},
\texttt{cpx}\index{cpx@\texttt{cpx}} as algebras with the following
constructors and destructors.
\begin{align*}
  &\mathtt{pos} \colon \mathtt{One}\index{One@\texttt{One}},\
  \mathtt{SZero}\index{SZero@\texttt{SZero}},\
  \mathtt{SOne}\index{SOne@\texttt{SOne}}
  \quad \hbox{(positive numbers written in binary)},
  \\
  &\mathtt{nat} \colon \mathtt{Zero}\index{Zero@\texttt{Zero}},\
  \mathtt{Succ}\index{Succ@\texttt{Succ}},
  \\
  &\mathtt{int} \colon \mathtt{IntPos}\index{IntPos@\texttt{IntPos}},\
  \mathtt{IntZero}\index{IntZero@\texttt{IntZero}},\
  \mathtt{IntNeg}\index{IntNeg@\texttt{IntNeg}},
  \\
  &\mathtt{rat} \colon
  \mathtt{RatConstr}\index{RatConstr@\texttt{RatConstr}}\
  \hbox{(written \# infix) and destructors}\
  \mathtt{RatN}\index{RatN@\texttt{RatN}},\
  \mathtt{RatD}\index{RatD@\texttt{RatD}},
  \\
  &\mathtt{real} \colon
  \mathtt{RealConstr}\index{RealConstr@\texttt{RealConstr}}\ \hbox{and
    destructors}\ \mathtt{RealSeq}\index{RealSeq@\texttt{RealSeq}},\
  \mathtt{RealMod}\index{RealMod@\texttt{realMod}},
  \\
  &\mathtt{cpx} \colon
  \mathtt{CpxConstr}\index{CpxConstr@\texttt{CpxConstr}}\
  \hbox{(written \#\# infix) and destructors}\
  \mathtt{RealPart}\index{RealPart@\texttt{RealPart}},\
  \mathtt{ImagPart}\index{ImagPart@\texttt{ImagPart}}.
\end{align*}
We provide
\begin{alignat*}{2}
  &\texttt{(alg-le? \textsl{alg1} \textsl{alg2})},%
  \index{alg-le?@\texttt{alg-le?}}
  \\
  &\texttt{(type-le?\ \textsl{type1} \textsl{type2})},%
  \index{type-le?@\texttt{type-le?}}
  \\
  &\texttt{(algebras-to-embedding \textsl{type1} \textsl{type2})},%
  \index{algebras-to-embedding@\texttt{algebras-to-embedding}}
  \\
  &\texttt{(types-to-embedding \textsl{type1} \textsl{type2})},%
  \index{types-to-embedding@\texttt{types-to-embedding}}
  \\
  &\texttt{(types-lub \textsl{type} .\ \textsl{types})}.%
  \index{types-lub@\texttt{types-lub}}
\end{alignat*}
\texttt{type-match-modulo-coercion} checks whether a given pattern can
be transformed modulo coercion by a substitution into a given
instance.  It returns \texttt{\#f}, if this is impossible, and the
substitution otherwise.
We provide
\begin{alignat*}{2}
  &\texttt{(type-match-modulo-coercion \textsl{pattern} \textsl{instance})}.%
  \index{type-match-modulo-coercion@\texttt{type-match-modulo-coercion}}
\end{alignat*}

\section{Variables}
\label{S:Variables}
A variable of an object type is interpreted by a continuous functional
(object) of that type.  We use the word \inquotes{variable} and not
\inquotes{program variable}, since continuous functionals are not
necessarily computable.  For readable in- and output, and also for
ease in parsing, we may reserve certain strings as names for variables
of a given type, e.g., $\texttt{n}, \texttt{m}$ for variables of type
\texttt{nat}.  Then also $\texttt{n0}, \texttt{n1}, \texttt{n2},
\dots, \texttt{m0}, \dots$ can be used for the same purpose.

In most cases we need to argue about existing (i.e., total) objects
only.  For the notion of totality we have to refer to \cite[Chapter
  8.3]{Stoltenberg94}; particularly relevant here is exercise 8.5.7.
To make formal arguments with quantifiers relativized to total objects
more managable, we use a special sort of variables intended to range
over such objects only.  For example, $\texttt{n}, \texttt{n0},
\texttt{n1}, \texttt{n2}, \dots$ range over total natural numbers, and
$\verb#n^#, \verb#n^0#, \verb#n^1#, \verb#n^2#, \dots$ are general
variables.  We say that the \emph{degree of totality}%
\index{degree of totality} for the former is $1$, and for the latter
$0$.

%% \subsection*{Interface}
To add and remove names for variables of a given type (e.g.,
$\texttt{n}, \texttt{m}$ for variables of type \texttt{nat}), we use
\begin{align*}
  &\texttt{(add-var-name \textsl{name1} \dots\ \textsl{type})},%%
  \index{add-var-name@\texttt{add-var-name}}
  \\
  &\texttt{(remove-var-name \textsl{name1} \dots\ \textsl{type})},%%
  \index{remove-var-name@\texttt{remove-var-name}}
  \\
  &\texttt{(default-var-name \textsl{type}).}
  \index{default-var-name@\texttt{default-var-name}}
\end{align*}
The first variable name added for any given type becomes the default
variable name.  If the system creates new variables of this type, they
will carry that name.  For complex types it sometimes is necessary to
talk about variables of a certain type without using a specific name.
In this case one can use the empty string to create a so called
numerated variable (see below).  The parser is able to produce this
kind of canonical variables from type expressions.

We need a constructor, accessors and tests for variables.
\begin{alignat*}{2}
  &\texttt{(make-var \textsl{type} \textsl{index} \textsl{t-deg}
    \textsl{name})} &\quad& \text{constructor},
  \\
  &\texttt{(var-to-type \textsl{var})} && \text{accessor},%
  \index{var-to-type@\texttt{var-to-type}}
  \\
  &\texttt{(var-to-index \textsl{var})} && \text{accessor},%
  \index{var-to-index@\texttt{var-to-index}}
  \\
  &\texttt{(var-to-t-deg \textsl{var})} && \text{accessor},%
  \index{var-to-t-deg@\texttt{var-to-t-deg}}
  \\
  &\texttt{(var-to-name \textsl{var})} && \text{accessor},%
  \index{var-to-name@\texttt{var-to-name}}
  \\
  &\texttt{(var-form?\ \textsl{x})} && \text{incomplete test},%
  \index{var-form?@\texttt{var-form?}}
  \\
  &\texttt{(var?\ \textsl{x}).} && \text{complete test}.
  \index{var?@\texttt{var?}}
\end{alignat*}
It is guaranteed that \texttt{equal?} is a valid test for equality of
variables.  Moreover, it is guaranteed that parsing a displayed
variable reproduces the variable; the converse need not be the case
(we may want to convert it into some canonical form).

For convenience we have the function
\begin{alignat*}{2}
  &\texttt{(mk-var \textsl{type} <\textsl{index}> <\textsl{t-deg}>
    <\textsl{name}>).}
  \index{mk-var@\texttt{mk-var}}
\end{alignat*}
The type is a required argument; however, the remaining arguments are
optional.  The default for the name string is the value returned by
\begin{alignat*}{2}
  &\texttt{(default-var-name \textsl{type})}.%
  \index{default-var-name@\texttt{default-var-name}}
\end{alignat*}
If there is no default name, a numerated variable is created.
%% The default for the totality is \inquotes{total}.
One can view the already chosen default variable names for some
types by
\begin{align*}
  &\texttt{(display-default-varnames .\ \textsl{types})}.%
  \index{display-default-varnames@\texttt{display-default-varnames}}
\end{align*}
Using the empty string as the name, we can create so called numerated
variables.  We further require that we can test whether a given
variable belongs to those special ones, and that from every numerated
variable we can compute its index:
\begin{align*}
  &\texttt{(numerated-var?\ \textsl{var})},%
  \index{numerated-var?@\texttt{numerated-var}}
  \\
  &\texttt{(numerated-var-to-index \textsl{numerated-var}).}
  \index{numerated-var-to-index@\texttt{numerated-var-to-index}}
\end{align*}
It is guaranteed that \texttt{make-var} used with the empty name
string is a bijection of the product of $\Ty$, $\D{N}$, and the
degrees of totality to the set of numerated variables,
with inverses \texttt{var-to-type}, \texttt{numerated-var-to-index}
and \texttt{var-to-t-deg}.

Although these functions look like an ad hoc extension of the
interface that is convenient for normalization by evaluation, there is
also a deeper background: these functions can be seen as the
\inquotes{computational content} of the well-known phrase \inquotes{we
  assume that there are infinitely many variables of every type}.
Giving a constructive proof for this statement would require to give
infinitely many examples of variables for every type.  This of course
can only be done by specifying a function (for every type) that
enumerates these examples.  To make the specification finite we
require the examples to be given in a uniform way, i.e., by a function
of two arguments.  To make sure that all these examples are in fact
different, we would have to require \texttt{make-var} to be injective.
Instead, we require (classically equivalent) \texttt{make-var} to be a
bijection on its image, as again, this can be turned into a
computational statement by requiring that a witness (i.e., an inverse
function) is given.

Finally, as often the exact knowledge of infinitely many variables of
every type is not needed we require that, either by using the above
functions or by some other form of definition, functions
\begin{align*}
  &\texttt{(type-to-new-var \textsl{type})},%
  \index{type-to-new-var@\texttt{type-to-new-var}}
  \\
  &\texttt{(type-to-new-partial-var \textsl{type})}
  \index{type-to-new-partial-var@\texttt{type-to-new-partial-var}}
\end{align*}
are defined that return a (total or partial) variable of the requested
type, different from all variables that have ever been returned by any
of the specified functions so far.

Occasionally we may want to create a new variable with the same name
(and degree of totality) as a given one.  This is useful, for instance
for bound renaming.  Therefore we supply
\begin{align*}
  &\texttt{(var-to-new-var \textsl{var})},%
  \index{var-to-new-var@\texttt{var-to-new-var}}
  \\
  &\texttt{(var-to-new-partial-var \textsl{var})}.%
  \index{var-to-new-partial-var@\texttt{var-to-new-partial-var}}
\end{align*}

\textbf{Implementation.}
Variables are implemented as lists:
\begin{equation*}
  \texttt{(var \textsl{type} \textsl{index} \textsl{t-deg} \textsl{name})}.
\end{equation*}

\section{Constants}
\label{S:Pconst}
Every constant (or more precisely, object constant) has a type and
denotes a computable (hence continuous) functional of that type.  We
have the following three kinds of constants:
\begin{enumeratei}
\item constructors, kind \texttt{constr},
\item constants with user defined rules (also called program(mable)
  constant, or pconst), kind \texttt{pconst},
\item constants whose rules are fixed, kind \texttt{fixed-rules}.
\end{enumeratei}
The latter are built into the system: for arbitrary algebras we have
recursion, (guarded) general recursion and corecursion operators and
also destructors, and for finitary algebras equality, existence and
structural existence operators. We also need ex-falso-quodlibet and
existence elimination operators.  They are typed in parametrized form,
with the actual type (or formula) given by a type (or type and
formula) substitution that is also part of the constant.  For
instance, equality is typed by $\alpha \typeTo \alpha \typeTo \typeB$
and a type substitution $\alpha \mapsto \rho$.  This is done for
clarity (and brevity, e.g., for large $\rho$ in the example above),
since one should think of the type of a constant in this way.

For constructors and for constants with fixed rules, by efficiency
reasons we want to keep the object denoted by the constant (as needed
for normalization by evaluation) as part of it.  It depends on the
type of the constant, hence must be updated in a given proof whenever
the type changes by a type substitution.

\subsection{Structural recursion operators and Gödel's $\T$}
\label{SS:StrucRec}
Recall the definition of types and constructor types in
section~\ref{S:Types}, and the examples given there.  The (structural)
higher type \emph{recursion operators}%
\index{recursion!operator} $\rec_{\iota}^{\tau}$ (introduced by Gödel
\cite{Goedel58}) are used to construct maps from the algebra $\iota$
to $\tau$, by recursion on the structure of $\iota$.  For instance,
$\rec_{\typeN}^{\tau}$ has type
\begin{equation*}
  \typeN \typeTo \tau \typeTo
  (\typeN \typeTo \tau \typeTo \tau) \typeTo \tau.
\end{equation*}
The first argument is the recursion argument, the second one gives the
base value, and the third one gives the step function, mapping the
recursion argument and the previous value to the next value.  For
example, $\rec_{\typeN}^{\typeN} n m \lambda_{n,p}(\suc p)$ defines
addition $m+n$ by recursion on $n$.

Generally, we define the type of the recursion operator for the
algebra $\iota = \mu_{\xi}\,(\kappa_0, \dots, \kappa_{k-1})$ and
result type $\tau$.  Let the $i$-th $\xi$-constructor type for $\iota$
be
\begin{equation*}
  \kappa_i(\xi) = (\rho_{i \nu}(\xi))_{\nu < n_i} \typeTo \xi.
\end{equation*}
The recursion operator\index{recursion!operator} $\rec_{\iota}^{\tau}$
then has type
\begin{equation*}
  \iota \typeTo (\kappa_i(\iota, \tau))_{i<k} \typeTo \tau
\end{equation*}
with \emph{step types} (w.r.t.\ the result type $\tau$)
\begin{equation*}
  \kappa_i(\iota, \tau) \defeq
  (\rho_{i \nu}(\iota \typeProd \tau))_{\nu < n_i} \typeTo \tau.
\end{equation*}
The recursion argument is of type $\iota$.
\begin{remark*}
  Usage of $\iota \typeProd \tau$ rather than $\tau$ in the step types
  can be seen as a \inquotes{strengthening}, since then one has more
  data available to construct the value of type $\tau$.  Moreover, for
  unnested recursive argument types $\vec{\sigma} \typeTo \tau$ we
  avoid the product type in $\vec{\sigma} \typeTo \iota \typeProd
  \tau$ and take the two argument types $\vec{\sigma} \typeTo \iota$
  and $\vec{\sigma} \typeTo \tau$ instead
  (\inquotes{duplication}\index{duplication}).
\end{remark*}

For some common algebras listed in \ref{SS:AlgsTypes} we spell out the
type of their recursion operators:
\begin{align*}
  &\rec_{\typeB}^{\tau} \colon \typeB \typeTo \tau \typeTo \tau \typeTo
  \tau,
  \\
  &\rec_{\typeN}^{\tau} \colon \typeN \typeTo \tau \typeTo (\typeN
  \typeTo \tau \typeTo \tau) \typeTo \tau,
  \\
  &\rec_{\typeP}^{\tau} \colon \typeP \typeTo \tau \typeTo (\typeP
  \typeTo \tau \typeTo \tau) \typeTo (\typeP \typeTo \tau \typeTo
  \tau) \typeTo \tau,
  \\
  &\rec_{\typeOrd}^{\tau} \colon \typeOrd \typeTo \tau \typeTo
  (\typeOrd \typeTo \tau \typeTo \tau) \typeTo
  ((\typeN \typeTo \typeOrd) \typeTo
  (\typeN \typeTo \tau) \typeTo \tau) \typeTo \tau,
  \\
  &\rec_{\typeL{\rho}}^{\tau} \colon \typeL{\rho} \typeTo \tau \typeTo
  (\rho \typeTo \typeL{\rho} \typeTo \tau \typeTo \tau) \typeTo \tau,
  \\
  &\rec_{\rho \typeSum \sigma}^{\tau} \colon \rho \typeSum \sigma
  \typeTo (\rho \typeTo \tau) \typeTo (\sigma \typeTo \tau) \typeTo
  \tau,
  \\
  &\rec_{\rho {\typeProd} \sigma}^{\tau} \colon \rho \typeProd
  \sigma \typeTo (\rho \typeTo \sigma \typeTo \tau) \typeTo \tau,
  \\
  &\rec_{\typeTree}^{\tau} \colon \typeTree \typeTo (\typeL{\typeTree
    \typeProd \tau} \typeTo \tau) \typeTo \tau,
  \\
  &\rec_{\typeWrite}^{\tau} \colon \typeWrite \typeTo \tau \typeTo
  (\typeRead(\typeWrite \typeProd \tau) \typeTo \tau) \typeTo
  \tau.
\end{align*}

One can extend the definition of the (structural) recursion operators
to simultaneously defined algebras $\vec{\iota} = \mu_{\vec{\xi}} \,
(\kappa_0, \dots, \kappa_{k-1})$ and result types $\vec{\tau}$.  Pick
$k_j, m_j$ as above (for simultaneously defined algebras).  For $m_j
\le i < m_{j+1}$ let $\kappa_i \in \constrtypes{\vec{\xi},j} \,
(\vec{Y})$ be the $\vec{\xi}, j$-constructor type
\begin{equation*}
  (\rho_{i \nu}(\vec{\xi}\,))_{\nu < n_i} \typeTo \xi_j.
\end{equation*}
The $j$-th simultaneous recursion operator%
\index{recursion!operator, simultaneous} $\rec_j^{\vec{\iota},
  \vec{\tau}}$ has type
\begin{equation*}
  \iota_j \typeTo ( \kappa_i(\vec{\iota}, \vec{\tau}\,))_{i<k} \typeTo \tau_j,
\end{equation*}
with step types
\begin{equation*}
  \kappa_i(\vec{\iota},\vec{\tau}\,) \defeq
  (\rho_{i \nu}(\vec{\iota} \typeProd \vec{\tau}\,))_{\nu < n_i} \typeTo
  \tau_l
  \quad \hbox{($m_l \le i < m_{l+1}$)}.
\end{equation*}
Here $\vec{\iota} \typeProd \vec{\tau}$ is the component-wise product.
Again for an unnested recursive argument type $\vec{\sigma} \typeTo
\tau_i$ we use duplication\index{duplication} to avoid the product
type in $\vec{\sigma} \typeTo \iota_i \typeProd \tau_i$ and take the
two argument types $\vec{\sigma} \typeTo \iota_i$ and $\vec{\sigma}
\typeTo \tau_i$ instead.

Note that $k$ is the \emph{total} number of constructors, and that the
recursion argument is of type $\iota_j$.  We will often omit the upper
indices $\vec{\iota}, \vec{\tau}$ when they are clear from the
context.  In case of a non-simultaneous free algebra we write
$\rec_{\iota}^{\tau}$ for $\rec_{1}^{\iota, \tau}$.  o--- An example of
a simultaneous recursion on tree lists and trees will be given below.

\begin{definition*}
  \emph{Terms of Gödel's $\T$}%
  \index{term!of Gödel's $\T$}\index{T@$\T$} for nested algebras are
  inductively defined from typed variables $x^{\rho}$ and constants
  for constructors $\constr_i^{\iota}$, recursion operators
  $\rec_{\iota}^{\tau}$ and map operators
  $\Map_{\lambda_{\vec{\alpha}}\pi}^{\vec{\rho} \to \vec{\tau}}$ by
  abstraction $\lambda_{x^{\rho}} M^{\sigma}$ and application
  $M^{\rho \typeTo \sigma} N^{\rho}$.
\end{definition*}

\subsection{Conversion}
\label{SS:TCFConv}
We define a \emph{conversion relation}%
\index{conversion} $\cnv_{\rho}$ between terms of type $\rho$ by
\begin{align}
  \label{E:BetaConv}
  (\lambda_x M(x))N &\cnv M(N),
  \\
  \label{E:EtaConv}
  \lambda_x( M x) &\cnv M
  \quad \hbox{if $x \notin \FV(M)$ ($M$ not an abstraction)},
  \\
  \label{E:RecConv}
  \rec_{\iota}^{\tau} (\constr_i^{\iota} \vec{N}) \vec{M} &\cnv
  M_i (\Map_{\lambda_{\alpha}\rho_{\nu}(\alpha)}^{\iota \to
    \iota \typeProd \tau} N_{\nu}
  \lambda_x \langle x^{\iota},\rec_{\iota}^{\tau}x\vec{M}\rangle)_{\nu < n}.
\end{align}
where in \eqref{E:RecConv} for simplicity we have spelled out the
non-simultaneous case only; the $i$-th $\xi$-constructor type is
assumed to be $(\rho_{\nu}(\xi))_{\nu < n} \typeTo \xi$.
Note that \eqref{E:RecConv} uses the map operator defined above.  In
the special case $\rho_{\nu}(\alpha) = \alpha$ we can avoid the
product type and instead of the pair
\begin{equation*}
  \Map_{\lambda_{\alpha}\alpha}^{\iota \to \iota \typeProd \tau}
  N_{\nu} \lambda_x \langle
  x^{\iota},\rec_{\iota}^{\tau}x\vec{M}\rangle \quad \hbox{i.e.,}
  \quad \langle N_{\nu}^{\iota}, \rec_{\iota}^{\tau} N_{\nu}
  \vec{M}\rangle
\end{equation*}
take its two components $N_{\nu}^{\iota}$ and $\rec_{\iota}^{\tau}
N_{\nu} \vec{M}$ as separate arguments of $M_i$.

The rule \eqref{E:BetaConv} is called $\beta$-conversion, and
\eqref{E:EtaConv} $\eta$-conversion; their left hand sides are called
\emph{$\beta$-redexes}\index{conversion!$\beta$-} or
\emph{$\eta$-redexes}\index{conversion!$\eta$-}, respectively.  The
left hand side of \eqref{E:RecConv} is called \emph{$\rec$-redex}%
\index{conversion!$\rec$-}; it is a special case of a redex associated
with a constant $D$ defined by \inquotes{computation rules}%
\index{computation rule} (cf.\ \ref{SS:Terms}), and hence also called
a \emph{$D$-redex}\index{conversion!$D$-}.

Let us look at some examples of what can be defined in Gödel's $\T$.
We define the \emph{canonical inhabitant}%
\index{canonical inhabitant} $\nullterm^{\rho}$ of a type $\rho \in
\Ty$:
\begin{equation*}
  \nullterm^{\iota_j} \defeq \constr_{i_j}^{\vec{\iota}} \nullterm^{\vec{\rho}}
  (\lambda_{\vec{x}_1} \nullterm^{\iota_{j_1}})
  \dots
  (\lambda_{\vec{x}_n} \nullterm^{\iota_{j_n}}),
  \quad
  \nullterm^{\rho \typeTo \sigma} \defeq
  \lambda_x \nullterm^{\sigma}.
\end{equation*}

The \emph{projections} of a pair to its components can be defined
easily:
\begin{equation*}
  M 0 \defeq
  \rec_{\rho \typeProd \sigma}^{\rho} M^{\rho \typeProd \sigma}
  (\lambda_{x^{\rho}, y^{\sigma}} x^{\rho}),
  \quad
  M 1 \defeq
  \rec_{\rho \typeProd \sigma}^{\sigma} M^{\rho \typeProd \sigma}
  (\lambda_{x^{\rho}, y^{\sigma}} y^{\sigma}).
\end{equation*}

The \emph{append}-function $\listappend$ for lists is defined
recursively as follows.  We write $\con {} {x} {l}$ as shorthand for
$\cons(x,l)$.
\begin{equation*}
  \nil \listappend l_2 \defeq l_2,
  \qquad
  (x :: l_1) \listappend l_2 \defeq x :: (l_1 \listappend l_2).
\end{equation*}
It can be defined as the term
\begin{equation*}
  l_1 \listappend l_2 \defeq
  \rec_{\typeL{\alpha}}^{\typeL{\alpha} \typeTo \typeL{\alpha}}
  l_1
  (\lambda_{l_2} l_2)
  \lambda_{x, \_, p, l_2}( x :: (p l_2))
  l_2.
\end{equation*}
Here \inquotes{$\_$} is a name for a bound variable which is not used.

Using the append function $\listappend$ we can define \emph{list
  reversal} $\listrev$ by
\begin{equation*}
  \listrev (\nil) \defeq \nil,
  \qquad
  \listrev (\con {} {x} {l}) \defeq \listrev(l) \listappend (x :: \nil).
\end{equation*}
The corresponding term is
\begin{equation*}
  \listrev(l) \defeq \rec_{\typeL{\alpha}}^{\typeL{\alpha}}
  l \; \nil
  \lambda_{x, \_, p}( p \listappend (x :: \nil)).
\end{equation*}

Assume we want to define by simultaneous recursion two functions on
$\typeN$, say $\even, \odd \colon \typeN \typeTo \typeB$.  We want
\begin{align*}
  \even(0) &\defeq \true,
  &
  \odd(0) &\defeq \false,
  \\
  \even(\suc n) &\defeq \odd(n),
  &
  \odd(\suc n) &\defeq \even(n).
\end{align*}
This can be achieved by using pair types: we recursively
define the single function $\evenodd \colon \typeN \typeTo \typeB
\typeProd \typeB$.  The step types are
\begin{equation*}
  \ST_0 = \typeB \typeProd \typeB,
  \quad
  \ST_1 = \typeN \typeTo \typeB \typeProd \typeB \typeTo
  \typeB \typeProd \typeB,
\end{equation*}
and we can define $\evenodd \, m \defeq \rec_{\typeN}^{\typeB
  \typeProd \typeB} m \langle \true, \false \rangle \lambda_{n, p}
\langle p 1, p 0 \rangle$.

Another example concerns the algebras $(\typeTlist(\alpha),
\typeTree(\alpha))$ simultaneously defined in \ref{SS:AlgsTypes} (we
write them without the parameter $\alpha$ here), whose constructors
$\constr_i^{(\typeTlist, \typeTree)}$ for $i \in \{0, \dots, 3\}$ are
\begin{equation*}
  \emp^{\typeTlist}, \quad
  \tcons^{\typeTree \typeTo \typeTlist \typeTo \typeTlist}, \quad
  \leaf^{\typeN \typeTo \typeTree}, \quad
  \branch^{\typeTlist \typeTo \typeTree}.
\end{equation*}
Recall that the elements of the algebra $\typeTree$ (i.e.,
$\typeTree(\alpha)$) are just the finitely branching trees, which
carry objects of type $\alpha$ on their leaves.

Let us compute the types of the recursion operators w.r.t.\ the
result types $\sigma, \tau$, i.e., of
$\rec_{\typeTlist}^{(\typeTlist, \typeTree), (\sigma,\tau)}$ and
$\rec_{\typeTree}^{(\typeTlist, \typeTree), (\sigma,\tau)}$, or
shortly $\rec_{\typeTlist}$ and $\rec_{\typeTree}$.  The step types
are
\begin{equation*}
  \begin{split}
    \ST_0 &\defeq \sigma,
    \\
    \ST_1 &\defeq \typeTree \typeTo \tau \typeTo \typeTlist \typeTo \sigma
    \typeTo \sigma,
  \end{split}
  \quad
  \begin{split}
    \ST_2 &\defeq \alpha \typeTo \tau,
    \\
    \ST_3 &\defeq \typeTlist \typeTo \sigma \typeTo \tau.
  \end{split}
\end{equation*}
Hence the types of the recursion operators are
\begin{align*}
  \rec_{\typeTlist} &\colon \typeTlist \typeTo
  \ST_0 \typeTo \ST_1 \typeTo \ST_2 \typeTo \ST_3 \typeTo \sigma,
  \\
  \rec_{\typeTree} &\colon \typeTree \typeTo
  \ST_0 \typeTo \ST_1 \typeTo \ST_2 \typeTo \ST_3 \typeTo \tau.
\end{align*}

The recursion operator $\rec_{\typeTree}$ or explicitly
$\rec_{\typeTree}^{(\typeTlist, \typeTree), (\sigma,\tau)}$ is
displayed as
\begin{equation*}
  \texttt{(Rec $\typeTree \typeTo \tau$ $\typeTlist \typeTo \sigma$)},
\end{equation*}
where the first arrow type indicates the type of the recursion and its
value type, and the remaining arrow types provide the value types for
the simultaneously defined algebras.

We now introduce some special cases of structural recursion and
also a generalization; both will be important later on.

\subsubsection*{Simplified simultaneous recursion}
In a recursion on simultaneously defined algebras one may need to
recur on some of those algebras only.  Then we can simplify the type
of the recursion operator accordingly, as follows.
\begin{enumeratei}
\item Only consider the relevant constructors, i.e., those mapping into
  relevant algebras.
\item Shorten their types by omitting all argument types containing
  irrelevant algebras.
\item Let $(\rho_{\nu}(\vec{\xi}\,))_{\nu < n} \typeTo \xi_j$ be a
  shortened $\vec{\xi}, j$-constructor type.  Form its simplified step type
  as $(\rho_{\nu}(\vec{\iota} \typeProd \vec{\tau}\,))_{\nu < n} \typeTo
  \tau_j$ where $\vec{\iota}$ are the relevant algebras and
  $\vec{\tau}$ the assigned value types.
\end{enumeratei}
If in the $(\typeTlist, \typeTree)$-example we want to recur on
$\typeTlist$ only, the step types are
\begin{equation*}
  \ST_0 \defeq \sigma,
  \quad
  \ST_1 \defeq \typeTlist \typeTo \sigma \typeTo \sigma.
\end{equation*}
Hence the type of the simplified recursion operator is
\begin{equation*}
  \rec_{\typeTlist} \colon \typeTlist \typeTo \ST_0 \typeTo \ST_1 \typeTo
  \sigma;
\end{equation*}
It is displayed as $\texttt{(Rec $\typeTlist \typeTo \sigma$)}$, where
the missing arrow type $\typeTree \typeTo \tau$ indicates that we
have a simplified simultaneous recursion.

An example is the recursive definition of the length of a tree list.
The recursion equations are
\begin{equation*}
  \lh{\emp} = 0,
  \quad
  \lh{\tcons\,b\,\bs} = \lh{\bs} + 1.
\end{equation*}
This length function can be defined by an ordinary (i.e.,
non-simplified) simultaneous recursion operator as
\begin{equation*}
  \lambda_{\as} \rec_{\typeTlist}^{(\typeTlist, \typeTree), (\typeN,\tau)}
  \as^{\typeTlist} 0 (\lambda_{a,y,\bs,n} n+1) (\lambda_x e^{\tau})
  (\lambda_{\as,n} e^{\tau}).
\end{equation*}
This simultaneous recursion simplifies to
\begin{equation*}
  \lambda_{\as} \rec_{\typeTlist}^{\typeN} \as^{\typeTlist} 0 (\lambda_{\bs,n} n+1).
\end{equation*}

%% In a recursion on simultaneously defined algebras one may need to
%% recur on some of those algebras only.  Then we can simplify the type
%% of the recursion operator accordingly, by
%% \begin{enumeratei}
%% \item omitting all step types $\ST_i^{\vec{\iota}, \vec{\tau}}$ with
%%   irrelevant value type $\tau_j$, and
%% \item simplifying the remaining step types by omitting from the
%%   recursive argument types $(\vec{\sigma}_{\nu} \typeTo
%%   \tau_{j_{\nu}})_{\nu < n}$ and also from their algebra-duplicates
%%   $(\vec{\sigma}_{\nu} \typeTo \iota_{j_{\nu}})_{\nu < n}$ all those
%%   with irrelevant $\tau_{j_{\nu}}$.
%% \end{enumeratei}
%% In the $(\typeTlist, \typeTree)$-example, if we only want to recur on
%% $\typeTlist$, then the step types are
%% \begin{equation*}
%%   \ST_0 \defeq \tau_0,
%%   \quad
%%   \ST_1 \defeq \typeTlist \typeTo \tau_0 \typeTo \tau_0.
%% \end{equation*}
%% Hence the type of the simplified recursion operator is
%% \begin{equation*}
%%   \rec_{\typeTlist} \colon \typeTlist \typeTo \ST_0 \typeTo \ST_1 \typeTo
%%   \tau_0.
%% \end{equation*}
%% An example is the recursive definition of the length of a tree list.
%% The recursion equations are
%% \begin{equation*}
%%   \lh{\emp} = 0,
%%   \quad
%%   \lh{\tcons\,b\,\bs} = \lh{\bs} + 1.
%% \end{equation*}
%% The step terms are
%% \begin{equation*}
%%   M_0 \defeq 0,
%%   \quad
%%   M_1 \defeq \lambda_{\bs, p} (p+1).
%% \end{equation*}

\subsubsection*{Cases}
There is an important variant of recursion, where no recursive calls
occur.  This variant is called the \emph{cases operator}%
\index{cases-operator@$\Cases$-operator}; it distinguishes
cases according to the outer constructor form.  Here all step types
have the form
\begin{equation*}
  \ST_i^{\vec{\iota}, \vec{\tau}} \defeq \vec{\rho} \typeTo
  (\vec{\sigma}_{\nu} \typeTo \iota_{j_{\nu}})_{\nu < n} \typeTo \tau_j.
\end{equation*}
The intended meaning of the cases operator is given by the conversion
rule
\begin{equation}
  \label{E:CasesConv}
  \Cases_j (\constr_i^{\vec{\iota}} \vec{N}) \vec{M} \cnv M_i \vec{N}.
\end{equation}
Notice that only those step terms are used whose value type is the
present $\tau_j$; this is due to the fact that there are no recursive
calls.  Therefore the type of the cases operator is
\begin{equation*}
  \Cases_{\iota_j \typeTo \tau_j}^{\vec{\iota}} \colon \iota_j \typeTo
  \ST_{i_0}\typeTo \dots \typeTo \ST_{i_{q-1}} \typeTo \tau_j,
\end{equation*}
where $\ST_{i_0}, \dots, \ST_{i_{q-1}}$ consists of all $\ST_i$ with value
type $\tau_j$.  We write $\Cases_{\iota_j}^{\tau_j}$ or even
$\Cases_j$ for $\Cases_{\iota_j \typeTo \tau_j}^{\vec{\iota}}$.

The simplest example (for type $\typeB$) is \emph{if-then-else}.
Another example is
\begin{equation*}
  \Cases_{\typeN}^{\tau} \colon
  \typeN \typeTo \tau \typeTo (\typeN \typeTo \tau) \typeTo \tau.
\end{equation*}
It can be used to define the \emph{predecessor} function on $\typeN$,
i.e., $\pred 0 \defeq 0$ and $\pred(\suc n) \defeq n$, by the term
\begin{equation*}
  \pred m \defeq \Cases_{\typeN}^{\typeN} m 0 (\lambda_n n).
\end{equation*}
In the $(\typeTlist, \typeTree)$-example we have
\begin{align*}
  \Cases_{\typeTlist}^{\tau_0} \colon \typeTlist \typeTo \tau_0
  \typeTo (\typeTree \typeTo \typeTlist \typeTo \tau_0) \typeTo
  \tau_0.
\end{align*}

When computing the value of a cases term, we do not want to (eagerly)
evaluate all arguments, but rather compute the test argument first and
depending on the result (lazily) evaluate at most one of the other
arguments.  This phenomenon is well known in functional languages; for
instance, in \textsc{Scheme} the \texttt{if}-construct is called a
\emph{special form} (as opposed to an operator).  Therefore instead of
taking the cases operator applied to a full list of arguments, one
rather uses a \texttt{if}-construct%
\index{if-construct@\texttt{if}-construct} to build this term; it
differs from the former only in that it employs lazy evaluation.
Hence the predecessor function is written $[\texttt{if}\ m\ 0\
\lambda_n n]$ (which is often written in the form $\caseof {m} {0 \mid
  \lambda_n n}$).

\subsubsection*{General recursion with respect to a measure}
\label{SSS:GenRec}
In practice it often happens that one needs to recur to an argument
which is not an immediate component of the present constructor object;
this is not allowed in structural recursion.  Of course, in order to
ensure that the recursion terminates we have to assume that the
recurrence is w.r.t.\ a given well-founded set; for simplicity we
restrict ourselves to the algebra $\typeN$.  However, we do allow that
the recurrence is with respect to a measure function $\mu$, with
values in $\typeN$.  The operator $\grec$ of \emph{general recursion}%
\index{recursion!general} then is defined by
\begin{equation}
  \label{E:GRec}
  \grec \mu x G = G x (
  \lambda_y \ifthenelse{\mu y<\mu x}{\grec \mu y G}{\nullterm}),
\end{equation}
where $\nullterm$ denotes a canonical inhabitant of the range.  One
can see easily that $\grec$ is definable from an appropriate
structural recursion operator.

\subsection{Corecursion}
\label{SS:CoRec}
It is well known that an arbitrary \inquotes{reduction sequence}
beginning with a term in Gödel's $\T$ terminates.  For this to hold it
is essential that the constants allowed in $\T$ are restricted to
constructors $\constr$ and recursion operators $\rec$.  A consequence
will be that every closed term of a base type denotes a total ideal.
The conversion rules for $\rec$ (cf.\ \ref{SS:TCFConv}) work from the
leaves towards the root, and terminate because total ideals are
well-founded.  If however we deal with cototal ideals (infinitary
derivations for example), then a similar operator is available to
define functions with cototal ideals as values, namely
\inquotes{corecursion}.

To understand the type of a corecursion operator recall the
constructor types $\kappa_i(\iota)$ of an algebra $\iota =
\mu_{\xi}(\kappa_0, \dots, \kappa_{k-1})$:
\begin{equation*}
  (\rho_{i \nu}(\iota))_{\nu < n_i} \typeTo \iota \quad \hbox{($i<k$)}.
\end{equation*}
The product of these $k$ constructor types is isomorphic to
\begin{equation*}
  \sum_{i<k} \prod_{\nu < n_i} \rho_{i \nu}(\iota) \typeTo \iota
\end{equation*}
and the type of the recursion operator $\rec_{\iota}^{\tau}$ is
isomorphic to
\begin{equation*}
  \iota \typeTo
  (\sum_{i<k} \prod_{\nu < n_i} \rho_{i \nu}(\iota \typeProd \tau) \typeTo \tau)
  \typeTo \tau.
\end{equation*}
Dually for the algebra $\iota$ the type of its \emph{destructor}%
\index{destructor type} $D_{\iota}$ (defined below) is
\begin{equation*}
  \iota \typeTo \sum_{i<k} \prod_{\nu < n_i} \rho_{i \nu}(\iota).
\end{equation*}
The corecursion operator\index{corecursion!operator}
$\corec_{\iota}^{\tau}$ is used to construct a mapping from $\tau$ to
$\iota$ by \inquotes{corecursion} on the structure of $\iota$.
Its type is
\begin{equation*}
  \tau \typeTo
  (\tau \typeTo \sum_{i<k} \prod_{\nu < n_i} \rho_{i \nu}(\iota \typeSum \tau))
  \typeTo \iota.
\end{equation*}

We list the types of the corecursion operators for some algebras:
\begin{align*}
  &\corec_{\typeB}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum \typeUnit) \typeTo \typeB,
  \\
  &\corec_{\typeN}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum (\typeN \typeSum \tau)) \typeTo \typeN,
  \\
  &\corec_{\typeP}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum (\typeP \typeSum \tau) \typeSum
   (\typeP \typeSum \tau)) \typeTo \typeP,
  \\
  &\corec_{\typeBin}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum (\typeBin \typeSum \tau) \typeProd
  (\typeBin \typeSum \tau))
  \typeTo \typeBin,
  \\
  &\corec_{\typeL{\rho}}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum \rho \typeProd
  (\typeL{\rho} \typeSum \tau)) \typeTo \typeL{\rho},
  \\
  &\corec_{\typeIntv}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum (\typeIntv \typeSum \tau) \typeSum
  (\typeIntv \typeSum \tau) \typeSum (\typeIntv \typeSum \tau)) \typeTo
  \typeIntv.
\end{align*}
The conversion relation for each of these is defined below.  For $f
\colon \rho \typeTo \tau$ and $g \colon \sigma \typeTo \tau$ we denote
$\lambda_x (\rec_{\rho \typeSum \sigma}^{\tau} x f g)$ of type $\rho
\typeSum \sigma \typeTo \tau$ by $[f,g]$, and similary for ternary
sumtypes etcetera.  $x_1$, $x_2$ are shorthand for the two projections
of $x$ of type $\rho \typeProd \sigma$.  The identity functions $\id$
below are of type $\iota \typeTo \iota$ with $\iota$ the respective
algebra.
\begin{align*}
  \corec_{\typeB}^{\tau} N M \cnv {}
  &[\lambda_{\_} \true, \lambda_{\_} \false] (M N),
  \\
  \corec_{\typeN}^{\tau} N M \cnv {}
  &[\lambda_{\_} 0, \lambda_x( \suc( [\id^{\typeN \typeTo \typeN},
  \lambda_y (\corec_{\typeN}^{\tau} y M)] x ))]
  (M N),
  \\
  \corec_{\typeP}^{\tau} N M \cnv {}
  &[\lambda_{\_} \one,
  \lambda_x( \s_0( [\id, P_{\typeP}] x )),
  \lambda_x( \s_1( [\id, P_{\typeP}] x ))](M N),
  \\
  \corec_{\typeBin}^{\tau} N M \cnv {}
  &[\lambda_{\_} 0,
  \lambda_x( \constr
  ([\id, P_{\typeBin}]x_1)
  ([\id, P_{\typeBin}]x_2))] (M N),
  \\
  \corec_{\typeL{\rho}}^{\tau} N M \cnv {}
  &[\lambda_{\_} \nil,
  \lambda_x( x_1 ::
  [\id, \lambda_y (\corec_{\typeL{\rho}}^{\tau} y M)] x_2)] (M N),
  \\
  \corec_{\typeIntv}^{\tau} N M \cnv {}
  &[\lambda_{\_} \D{I},
  \lambda_x( \constr_{-1}( [\id,  P_{\typeIntv} ] x )),
  \lambda_x( \constr_{0}( [\id,  P_{\typeIntv} ] x )),
  \lambda_x( \constr_{1}( [\id,  P_{\typeIntv} ] x ))]
  \\
  &(M N)
\end{align*}
with $P_{\alpha} \defeq \lambda_y( \corec_{\alpha}^{\tau} y M)$ for
$\alpha \in \{ \typeP, \typeBin, \typeIntv \}$.

The types of the corecursion operators for $\typeTree$ and
$\typeWrite$ are
\begin{align*}
  &\corec_{\typeTree}^{\tau} \colon \tau \typeTo
  (\tau \typeTo
  \typeL{\typeTree \typeSum \tau}) \typeTo \typeTree,
  \\
  &\corec_{\typeWrite}^{\tau} \colon \tau \typeTo
  (\tau \typeTo \typeUnit \typeSum
  \typeRead(\typeWrite \typeSum \tau)) \typeTo \typeWrite.
\end{align*}
The conversion relation for each of these is defined by
\begin{align*}
  \corec_{\typeTree}^{\tau} N M \cnv {}
  &\branch( \Map_{\typeL{\typeTree \typeSum \tau}}^{\typeTree}
  [\id^{\typeTree \typeTo \typeTree}, \lambda_z
  \corec_{\typeTree}^{\tau} z M](M N)^{\typeL{\typeTree \typeSum \tau}}),
%   \\
%   \hbox{was}
%   \\
%   \corec_{\typeTree}^{\tau} N M \cnv {}
%   &\branch( [\id^{\typeL{\tau} \typeTo \typeL{\tau}},
%   \Map_{\typeL{\tau}}^{\tau}
%   (\corec_{\typeTree}^{\tau} \cdot M)](M N)),
  \\
  \corec_{\typeWrite}^{\tau} N M \cnv {}
  &[\lambda_{\_} W_0, \lambda_x(
  W( \Map_{\typeRead(\typeWrite \typeSum \tau)}^{\typeWrite}
  [\id^{\typeWrite \typeTo \typeWrite},
  \lambda_z \corec_{\typeWrite}^{\tau} z M)] x ))](M N).
%   \\
%   \hbox{was}
%   \\
%   \corec_{\typeWrite}^{\tau} N M \cnv {}
%   &[\lambda_{\_} W_0, \lambda_x(
%   W( [\id^{\typeRead(\typeWrite) \typeTo \typeRead(\typeWrite)},
%   \Map_{\typeRead(\tau)}^{\typeWrite}
%   (\corec_{\typeWrite}^{\tau} \cdot M)] x ))](M N).
\end{align*}
An alternative notation for the former term is
\begin{align*}
  \corec_{\typeTree}^{\tau} N M \cnv
  \branch( \Map_{\typeL{\typeTree \typeSum \tau}}^{\typeTree}
  &(\lambda_p \caseof {p^{\typeTree \typeSum \tau}}
  {\\
    &\hspace{6mm} \termSumIntroLeft\; a^{\typeTree} \mapsto a \mid
    \\
    &\hspace{6mm} \termSumIntroRight\; z^{\tau} \mapsto
    \corec_{\typeTree}^{\tau} z M})
  \\
  &(M N)^{\typeL{\typeTree \typeSum \tau}})
\end{align*}
and for the latter
\begin{align*}
  \corec_{\typeWrite}^{\tau} N M \cnv {}
  &\caseof {(M N)^{\typeUnit \typeSum
      \typeRead(\typeWrite \typeSum \tau)}}
  {\\
    &\termSumIntroLeft\; \_ \mapsto W_0 \mid
    \\
    &\termSumIntroRight\; x \mapsto W
    (\Map_{\typeRead(\typeWrite \typeSum \tau)}^{\typeWrite}
    (\lambda _p
    \caseof {p^{\typeWrite \typeSum \tau}}
    {\\
      &\hspace{46mm} \termSumIntroLeft\; y^{\typeWrite} \mapsto y \mid
      \\
      &\hspace{46mm} \termSumIntroRight\; z^{\tau} \mapsto
      \corec_{\typeWrite}^{\tau} z M})
  \\
  &\hspace{40mm}x^{\typeRead(\typeWrite \typeSum \tau)}}.
\end{align*}

The conversion rule for $\corec_{\iota}^{\tau} N M$ in the general
case is defined similarly: we distinguish cases on $MN$ of type $\sum
\bigl( \prod \vec{\rho}(\iota \typeSum \tau) \bigr)$.  Suppose we are
in the case of the $i$-th injection of a term $x$ of product type
$\prod \vec{\rho}(\iota \typeSum \tau)$.  Then we apply the $i$-th
constructor $\constr_i$ of type $\vec{\rho}(\iota) \typeTo \iota$ as
follows.  The $\nu$-th argument of type $\rho_{\nu}(\iota)$ is
obtained from the $\nu$-th component $x_{\nu}$ of $x$ as
\begin{align*}
  \Map_{\lambda_{\alpha}\rho_{\nu}(\alpha)}^{\iota \typeSum \tau \to \iota}
  x_{\nu}^{\rho_{\nu}(\iota \typeSum \tau)}
  &(\lambda _p
  \caseof {p^{\iota \typeSum \tau}}
  {\\
    &\hspace{6mm} \termSumIntroLeft\; y^{\iota} \mapsto y \mid
    \\
    &\hspace{6mm} \termSumIntroRight\; z^{\tau} \mapsto
    \corec_{\iota}^{\tau} z M}).
\end{align*}

As an example of a function defined by corecursion (due to
\cite{Berger09}\index{Berger}) consider the transformation of an
\inquotes{abstract} real\index{real!abstract} in the interval $[-1,1]$
into a stream representation%
\index{stream representation} using signed digits%
\index{signed digit} from $\{ -1, 0, 1 \}$.  Assume that we work in an
abstract (axiomatic) theory of reals, having an unspecified type
$\rho$, and that we have a type $\sigma$ for rationals as well.
Assume that the abstract theory provides us with a function $g \colon
\rho \typeTo \sigma \typeTo \sigma \typeTo \typeB$ comparing a real
$x$ with a proper rational interval $p<q$:
\begin{align*}
  &g(x,p,q) = \true \to x \le q,
  \\
  &g(x,p,q) = \false \to p \le x.
\end{align*}
From $g$ we define a function $h \colon \rho \typeTo \typeUnit
\typeSum (\typeIntv \typeSum \rho) \typeSum (\typeIntv \typeSum \rho)
\typeSum (\typeIntv \typeSum \rho)$ by
\begin{equation*}
  h(x) \defeq
  \begin{cases}
    \hbox{$2x+1$ in rhs of left $\typeIntv \typeSum \rho$}
    &\hbox{if $g(x, - \frac{1}{2}, 0) = \true$},
    \\
    \hbox{$2x$ in rhs of middle $\typeIntv \typeSum \rho$}
    &\hbox{if $g(x, - \frac{1}{2}, 0) = \false$,
      $g(x, 0, \frac{1}{2}) = \true$},
    \\
    \hbox{$2x-1$ in rhs of right $\typeIntv \typeSum \rho$}
    &\hbox{if $g(x, 0, \frac{1}{2}) = \false$}.
  \end{cases}
\end{equation*}
$h$ is definable by a closed term $M$ in Gödel's $\T$.  Then the
desired function $f \colon \rho \typeTo \typeIntv$ transforming an
abstract real $x$ into a cototal ideal (i.e., a stream) in $\typeIntv$
can be defined by
\begin{equation*}
  f(x) \defeq \corec_{\typeIntv}^{\rho} x M.
\end{equation*}
This $f(x)$ will thus be a stream of digits $-1, 0, 1$.

We give another example of a function defined by corecursion, this
time on the nested algebra $\typeWrite$.  It uses
$\corec_{\typeWrite}^{\tau} \colon \tau \typeTo (\tau \typeTo
\typeUnit \typeSum \typeRead(\typeWrite \typeSum \tau)) \typeTo
\typeWrite$ with $\tau$ an abstract type of continuous functions.  We
assume that for every $f^{\tau}$ we have $\omega(f) \colon \typeN
\typeTo \typeN$ (the uniform modulus of continuity) and $h(f) \colon
\typeN \typeTo \D{Q} \typeTo \D{Q}$ (the approximating function).  Our
example is the computational content of the proof of proposition (a)
below, assigning to every continuous $f$ an ideal in $\typeWrite$.
This ideal is given as $\corec_{\typeWrite}^{\tau} f M$ with $M$ of
type $\tau \typeTo \typeUnit \typeSum \typeRead(\typeWrite \typeSum
\tau)$ defined by
\begin{equation*}
  M f \defeq \termSumIntroRight( \Phi(\omega f 2)f(h f 0)).
\end{equation*}
Here $\Phi \colon \typeN \typeTo \tau \typeTo (\D{Q} \typeTo \D{Q})
\typeTo \typeRead(\typeWrite \typeSum \tau)$ is recursively defined by
\begin{align*}
  &\Phi 0 f g \defeq R_d(\termSumIntroRight(\realout_d \circ f))
  \quad \hbox{with $d \defeq \mathrm{head}(g (\frac{1}{2^2}))$},
  \\
  &\Phi l f g \defeq
  R(\Phi (l-1) (f \circ \realin_d) (g \circ \realin_d))_{d \in \{-1,0,1\}},
\end{align*}
where $\mathrm{head} \colon \D{Q} \to \typeSD$ is defined by
\begin{equation*}
  \mathrm{head}(q) \defeq
  \begin{cases}
    -1 &\hbox{if $q < -\frac{1}{4}$}
    \\
    0 &\hbox{if $-\frac{1}{4} \le q \le \frac{1}{4}$}
    \\
    1 &\hbox{if $\frac{1}{4} < q$}.
  \end{cases}
\end{equation*}
$\Phi$ can be explicitly defined using $\rec_{\typeN}^{\tau \typeTo
  (\D{Q} \typeTo \D{Q}) \typeTo \typeRead(\typeWrite \typeSum \tau)}$:
as base term take
\begin{equation*}
  \lambda_{f,g} R_d(\termSumIntroRight(\realout_d \circ f))
  \quad \hbox{with $d \defeq \mathrm{head}(g (\frac{1}{2^2}))$}
\end{equation*}
and as step term
\begin{equation*}
  \lambda_{\_,p,f,g}
  R(p (f \circ \realin_d) (g \circ \realin_d))_{d \in \{-1,0,1\}}.
\end{equation*}

Simultaneous corecursion operators can be introduced similarly.  For
$\vec{\iota} \defeq \mu_{\vec{\xi}}(\kappa_0, \dots, \kappa_{k-1})$
let $k = \sum_{j<N} k_j$ with $k_j \ge 1$ and $m_j \defeq \sum_{l<j}
k_j$, hence m$_j + k_j = m_{j+1}$.  Recall the constructor type
\begin{equation*}
  (\rho_{i \nu}(\vec{\iota}\,))_{\nu < n_i} \typeTo \iota_j
  \quad \hbox{($m_j \le i < m_{j+1}$)}.
\end{equation*}
The product of these $k$ constructor types is isomorphic to
\begin{equation*}
  \sum_{m_j \le i < m_{j+1}} \prod_{\nu < n_i} \rho_{i \nu}(\iota) \typeTo \iota_j
\end{equation*}
and the type of the recursion operator $\rec_j^{\vec{\iota}, \vec{\tau}}$ is
isomorphic to
\begin{equation*}
  \iota_j \typeTo
  (\sum_{m_l \le i < m_{l+1}} \prod_{\nu < n_i}
  \rho_{i \nu}(\vec{\iota} \typeProd \vec{\tau}) \typeTo \tau_l)_{i<k}
  \typeTo \tau_j.
\end{equation*}
Dually for the algebras $\vec{\iota}$ the types of the \emph{destructor}%
\index{destructor type} $D_j^{\vec{\iota}}$ (defined below) for the algebra
$\iota_j$ is
\begin{equation*}
  \iota_j \typeTo \sum_{m_j \le i < m_{j+1}} \prod_{\nu < n_i}
  \rho_{i \nu}(\vec{\iota}\,).
\end{equation*}
The $j$-th simultaneous corecursion
operator\index{corecursion!operator} $\corec_j^{\vec{\iota},
  \vec{\tau}}$ is used to construct a mapping from $\tau_j$ to $\iota_j$
by \inquotes{corecursion} on the structure of $\iota$.  Its type is
\begin{equation*}
  \tau_j \typeTo
  (\tau_l \typeTo \sum_{m_l \le i < m_{l+1}} \prod_{\nu < n_i}
  \rho_{i \nu}(\vec{\iota} \typeSum \vec{\tau}))_{l<N}
  \typeTo \iota_j.
\end{equation*}

We give an example of a simultaneous corecursion on tree lists and trees.
Recall the simultaneously defined algebras
$(\typeTlist(\alpha), \typeTree(\alpha))$ %in \ref{SS:AlgsTypes}
(we write them without the parameter $\alpha$ here), whose constructors
$\constr_i^{(\typeTlist, \typeTree)}$ for $i \in \{0, \dots, 3\}$ are
\begin{equation*}
  \emp^{\typeTlist}, \quad
  \tcons^{\typeTree \typeTo \typeTlist \typeTo \typeTlist}, \quad
  \leaf^{\alpha \typeTo \typeTree}, \quad
  \branch^{\typeTlist \typeTo \typeTree}.
\end{equation*}
The elements of the algebra $\typeTree$ (i.e., $\typeTree(\alpha)$)
are just the finitely branching trees, which carry objects of type
$\alpha$ on their leaves.

We compute the types of the corecursion operators w.r.t.\ the argument
types $\sigma, \tau$, i.e., of $\corec_{\typeTlist}^{(\typeTlist,
  \typeTree), (\sigma,\tau)}$ and $\corec_{\typeTree}^{(\typeTlist,
  \typeTree), (\sigma,\tau)}$, or shortly $\corec_{\typeTlist}$ and
$\corec_{\typeTree}$.  The step types are
\begin{equation*}
  \ST_0 \defeq \sigma \typeTo \typeUnit \typeSum
  (\typeTree \typeSum \tau) \typeProd (\typeTlist \typeSum \sigma),
  \qquad
  \ST_1 \defeq \tau \typeTo \alpha \typeSum (\typeTlist \typeSum \sigma).
\end{equation*}
Hence the types of the corecursion operators are
\begin{align*}
  \corec_{\typeTlist} &\colon \sigma \typeTo
  \ST_0 \typeTo \ST_1 \typeTo \typeTlist,
  \\
  \corec_{\typeTree} &\colon \tau \typeTo \ST_0 \typeTo \ST_1 \typeTo \typeTree.
\end{align*}
The corecursion operator $\corec_{\typeTree}$ or explicitly
$\corec_{\typeTree}^{(\typeTlist, \typeTree), (\sigma,\tau)}$ is
displayed as
\begin{equation*}
  \texttt{(CoRec $\tau \typeTo \typeTree$ $\sigma \typeTo \typeTlist$)},
\end{equation*}
where the first arrow type indicates the type of the corecursion and
its argument type, and the remaining arrow types provide the argument
types for the simultaneously defined algebras.

\emph{Simplified simultaneous corecursion.}
In a corecursion on simultaneously defined algebras one may need to
recur on some of those algebras only.  Then we can simplify the type
of the corecursion operator accordingly, as follows.
\begin{enumeratei}
\item Only consider the relevant constructors, i.e., those mapping into
  relevant algebras.
\item Shorten their types by omitting all argument types containing
  irrelevant algebras.
\item Out of these shortened relevant constructor types form the dual
  type
  \begin{equation*}
    \iota_j \typeTo \sum_{m_j \le i < m_{j+1}} \prod \vec{\rho}(\vec{\iota})
  \end{equation*}
  with $\vec{\iota}$ the relevant algebras.  The simplified step type
  then is
  \begin{equation*}
    \tau_j \typeTo \sum_{m_j \le i < m_{j+1}}
    \prod \vec{\rho}(\vec{\iota} \typeSum \vec{\tau})
  \end{equation*}
  with $\vec{\tau}$ corresponding to $\vec{\iota}$.
\end{enumeratei}
In the $(\typeTlist, \typeTree)$-example, if we want to do corecursion
on $\typeTlist$ only, then there is a single step type
\begin{equation*}
  \ST_0 \defeq \sigma \typeTo \typeUnit \typeSum (\typeTlist \typeSum \sigma),
\end{equation*}
and the type of the simplified corecursion operator is
\begin{equation*}
  \corec_{\typeTlist} \colon \typeTlist \typeTo \ST_0 \typeTo \sigma.
\end{equation*}

\begin{remark*}
  There is yet another situation where one might want to simplify the
  type of corecursion, namely when the argument type $\tau$ is the
  unit type $\typeUnit$.  For instance for $\corec_{\typeN}^{\tau}$
  its type
  \begin{equation*}
    \tau \typeTo
    (\tau \typeTo \typeUnit \typeSum (\typeN \typeSum \tau)) \typeTo \typeN
  \end{equation*}
  can then be simplified to
  \begin{equation*}
    (\typeUnit \typeSum (\typeN \typeSum \typeUnit)) \typeTo \typeN.
  \end{equation*}
\end{remark*}

\subsection{A common extension $\T^{+}$ of Gödel's $\T$ and Plotkin's $\PCF$}
\label{SS:Terms}
\emph{Terms}\index{term!of $\T^{+}$} of $\T^{+}$ are built from
(typed) variables and (typed) constants (constructors $\constr$ or
defined constants $D$, see below) by (type-correct) application and
abstraction:
\begin{equation*}
  M,N \BNFdef x^{\rho} \BNFor \constr^{\rho} \BNFor D^{\rho}
    \BNFor (\lambda_{x^{\rho}} M^{\sigma})^{\rho \typeTo \sigma}
    \BNFor (M^{\rho \typeTo \sigma} N^{\rho})^{\sigma}.
\end{equation*}

\begin{definition*}[Computation rule]
  Every defined constant $D$ comes with a system of \emph{computation
    rules}%
  \index{computation rule}, consisting of finitely many equations
  \begin{equation}
    \label{E:CompRule}
    D \vec{P}_i(\vec{y}_i) = M_i
    \qquad \hbox{($i=1, \dots, n$)}
  \end{equation}
  with free variables of $\vec{P}_i(\vec{y}_i)$ and $M_i$ among
  $\vec{y}_i$, where the arguments on the left hand side must be
  \inquotes{constructor patterns}%
  \index{constructor pattern}, i.e., lists of applicative terms built
  from constructors and distinct variables.  To ensure consistency of
  the defining equations, we require that for $i \ne j$ either
  $\vec{P}_i$ and $\vec{P}_j$ are non-unifiable (i.e., there is no
  substitution which identifies them), or else $\vec{P}_i$ and
  $\vec{P}_j$ have disjoint free variables, and for the most general
  unifier $\xi$ of $\vec{P}_i$ and $\vec{P}_j$ we have $M_i \xi = M_j
  \xi$.  Notice that the substitution $\xi$ assigns to the variables
  $\vec{y}_i$ in $M_i$ constructor patterns $\vec{R}_k(\vec{z}\,)$ ($k
  = i,j$).  A further requirement on a system of computation rules $D
  \vec{P}_i(\vec{y}_i) = M_i$ is that the lengths of all
  $\vec{P}_i(\vec{y}_i)$ are the same; this number is called the
  \emph{arity} of $D$, denoted by $\ar(D)$.  A substitution instance
  of a left hand side of \eqref{E:CompRule} is called a
  \emph{$D$-redex}\index{conversion!$D$-}.
\end{definition*}

More formally, constructor patterns are defined inductively by (we
write $\vec{P}(\vec{x}\,)$ to indicate all variables in $\vec{P}$)
\begin{enumeratea}
\item $x$ is a constructor pattern.
\item The empty list $\emptyseq$ is a constructor pattern.
\item If $\vec{P}(\vec{x}\,)$ and $Q(\vec{y}\,)$ are constructor
  patterns whose variables $\vec{x}$ and $\vec{y}$ are disjoint,
  then $(\vec{P},Q)(\vec{x},\vec{y}\,)$ is a constructor
  pattern.
\item If $\constr$ is a constructor and $\vec{P}$ a constructor
  pattern, then so is $\constr \vec{P}$, provided it is of ground
  type.
\end{enumeratea}

\begin{remark*}
  The requirement of disjoint variables in unifiable constructor
  patterns $\vec{P}_i$ and $\vec{P}_j$ used in computation rules of a
  defined constant $D$ is needed to ensure that applying the most
  general unifier produces constructor patterns again.  However, for
  readability we take this as an implicit convention, and write
  computation rules with possibly non-disjoint variables.
\end{remark*}

Examples of constants $D$ defined by computation rules are abundant.
The defining equations in \ref{SS:TCFConv} can all be seen as
computation rules, for
\begin{enumeratei}
\item the append-function $\listappend$,
\item list reversal $\listrev$,
\item the simultaneously defined functions $\even, \odd \colon \typeN
  \typeTo \typeB$ and
\item the two simultaneously defined functions $\oplus \colon \typeTlist
  \typeTo \typeTree \typeTo \typeTlist$ and $+ \colon \typeTree
  \typeTo \typeTree \typeTo \typeTree$.
\end{enumeratei}
Moreover, the structural recursion operators themselves can be viewed
as defined by computation rules, which in this case are called
\emph{conversion}\index{conversion} rules; cf.\ \ref{SS:TCFConv}.

The boolean connectives $\andb$, $\impb$ and $\orb$ are defined by
\begin{equation*}
  \begin{split}
    \true \andb y &= y,
    \\
    x \andb \true &= x,
    \\
    \false \andb y &= \false,
    \\
    x \andb \false &= \false,
  \end{split}
  \qquad
  \begin{split}
    \false \impb y &= \true,
    \\
    \true \impb y &= y,
    \\
    x \impb \true &= \true,
  \end{split}
  \qquad
  \begin{split}
    \true \orb y &= \true,
    \\
    x \orb \true &= \true,
    \\
    \false \orb y &= y,
    \\
    x \orb \false &= x.
  \end{split}
\end{equation*}
Notice that when two such rules overlap, their right hand sides are
equal under any unifier of the left hand sides.

Decidable \emph{equality}\index{equality!decidable} $=_{\iota} \colon
\iota \typeTo \iota \typeTo \typeB$ for a finitary algebra $\iota$ is
defined by
\begin{equation*}
  \begin{split}
    &(\constr_i \vec{x} =_{\iota} \constr_j \vec{y}\,) = \false
    \quad \hbox{if $i \ne j$},
    \\
    &(\constr_i \vec{x} =_{\iota} \constr_i \vec{y}\,) =
    ( \vec{x}^P =_{\vec{\rho}} \vec{y}^P \andb
    \bigland_{\nu < n} (\vec{x}^R_{m+\nu} =_{\iota_{j_{\nu}}}
    \vec{y}^R_{m+\nu})).
  \end{split}
\end{equation*}
(For a constructor term $\constr \vec{r}$ we denote by $\vec{r}^P$
its parameter arguments and by $\vec{r}^R$ its recursive arguments.)
For example,
\begin{equation*}
  \begin{split}
    &(0 =_{\typeN} 0) = \true,
    \\
    &(0 =_{\typeN} \suc n) = \false,
  \end{split}
  \qquad
  \begin{split}
    &(\suc m =_{\typeN} 0) = \false,
    \\
    &(\suc m =_{\typeN} \suc n) = \;(m =_{\typeN} n).
  \end{split}
\end{equation*}

The \emph{predecessor} functions introduced in \ref{SS:StrucRec} by
means of the cases-operator $\Cases$ can also be viewed as defined
constants:
\begin{equation*}
  \pred 0 = 0, \qquad \pred(\suc n) = n.
\end{equation*}
Another example is the \emph{destructor}\index{destructor} function,
disassembling a constructor-built argument into its parts.  For the
type $\typeTree_1 \defeq \mu_{\xi}(\xi, (\typeN \typeTo \xi) \typeTo
\xi)$ the destructor $D_{\typeTree_1}$ has type
\begin{equation*}
  D_{\typeTree_1} \colon \typeTree_1 \typeTo
  \typeUnit \typeSum (\typeN \typeTo \typeTree_1)
\end{equation*}
and is defined by the computation rules
\begin{equation*}
  D_{\typeTree_1} 0 = \termSumIntroLeft(\termUnit),
  \qquad
  D_{\typeTree_1}(\Sup (f)) = \termSumIntroRight(f).
\end{equation*}
Generally, the type of the destructor $D_{\iota}$ function for $\iota
\defeq \mu_{\xi}(\kappa_0, \dots, \kappa_{k-1})$ with $\kappa_i =
\vec{\rho}_i \typeTo \iota$ is
\begin{equation*}
  \iota \typeTo \sum_{i<k} \prod_{\nu < n_i} \rho_{i \nu}(\iota).
\end{equation*}
Its conversion rules map $D_{\iota}(\constr_i \vec{x}\,)$ to the
$i$-th injection into the sum type of the product of the $\vec{x}$.

\subsection{Implementation}
Every object constant has the internal representation
\begin{align*}
  \texttt{(const}\
  &\hbox{\textsl{object-or-arity} \textsl{name} \textsl{kind}
    \textsl{uninst-type} \textsl{tsubst}}
  \\
  &\hbox{\textsl{t-deg} \textsl{token-type} \textsl{repro-data}\texttt{)}}.
\end{align*}
The type of the constant is the result of carrying out the type
substitution \textsl{tsubst} in \textsl{uninst-type}; free type
variables may again occur in this type.  The type substitution
\textsl{tsubst} must be restricted to the type variables in
\texttt{uninst-type}.  An examples for an object constant is
\begin{alignat*}{2}
  &\texttt{(const Compose $(\alpha {\to} \beta) {\to} (\beta {\to}
    \gamma) {\to} \alpha {\to} \gamma$ $(\alpha \mapsto \rho, \beta
    \mapsto \sigma, \gamma \mapsto \tau)$ \dots)}.%
  \index{Compose@\texttt{Compose}}
\end{alignat*}
\textsl{object-or-arity} is an object if this object cannot be
changed, e.g., by allowing user defined rules for the constant;
otherwise, the associated object needs to be updated whenever a new
rule is added, and we have the arity of those rules instead.  The
rules are of crucial importance for the correctness of a proof, and
should not be invisibly buried in the denoted object taken as part of
the constant (hence of any term involving it).  Therefore we keep the
rules of a program constant and also its denoted objects (depending on
type substitutions) at a central place, a global variable
\texttt{PROGRAM-CONSTANTS} which assigns to every name of such a
constant the constant itself (with uninstantiated type), the rules
presently chosen for it, its denoted objects (as association list with
type substitutions as keys) and possibly (as an optinal final entry)
the (Scheme) code of an \emph{external}%
\index{external code} function mapping a type substitution and an
object list to either an object to be returned immediately, or else to
\texttt{\#f}, in which case the rules are tried next.  When a new rule
has been added, the new objects for the program constant are computed,
and the new list to be associated with the program constant is written
in \texttt{PROGRAM-CONSTANTS} instead.  All information on a program
constant except its denoted object and its computation and rewrite
rules (i.e., its type, degree of totality, arity and token type) is
stable and hence can be kept as part of it.  The \emph{token type}%
\index{token type} can be either \texttt{const} (i.e., constant
written as application) or one of: \texttt{postfix-op},
\texttt{prefix-op}, \texttt{binding-op}, \texttt{add-op},
\texttt{mul-op}, \texttt{rel-op}, \texttt{and-op}, \texttt{or-op},
\texttt{imp-op} and \texttt{pair-op}.

Repro-data are (only) necessary in \texttt{proof.scm}, for
normalization of proofs: a (general) induction, efq, introduction or
elimination axiom is translated into an appropriate constant, then
normalized, and finally from the constant and its repro data the axiom
is reproduced.  The repro-data are of the following forms.
\begin{enumerate}
\item For a recursion constant.
  \begin{enumeratea}
  \item A list of all-formulas.  This form only occurs when
    translating an axiom for (simultaneous) induction into a recursion
    constant, in order to achieve normalization of proofs via term
    normalization.  We have to consider the free variables in the
    scheme formulas, and let the type of the recursion constant depend
    on them.  This is needed to have the allnc-conversion be
    represented in term normalization.  The relevant operation is
    \begin{equation*}
      \hbox{\texttt{all-formulas-to-rec-const}%
      \index{all-formulas-to-rec-const@\texttt{all-formulas-to-rec-const}}}.
    \end{equation*}
  \item A list of implication formulas $I \vec{x}\hbox{\verb#^#} \to
    A(\vec{x}\hbox{\verb#^#})$, where all idpcs are simultaneously
    inductively defined.  This form only occurs when translating an
    elimination axiom into a recursion constant, in order to achieve
    normalization of proofs via term normalization.  We again have to
    consider the free variables in the scheme formulas, and let the
    type of the recursion constant depend on them.  This is needed to
    have the allnc-conversion be represented in term normalization.
    The relevant operation is
    \begin{equation*}
      \hbox{\texttt{imp-formulas-to-rec-const}.%
      \index{imp-formulas-to-rec-const@\texttt{imp-formulas-to-rec-const}}}.
    \end{equation*}
  \end{enumeratea}
\item For a cases constant.  Here a single arrow-type or all-formula
  suffices.  One uses
  \begin{equation*}
    \hbox{\texttt{all-formula-to-cases-const}%
      \index{all-formula-to-cases-const@\texttt{all-formula-to-cases-const}}}.
  \end{equation*}
\item For a guarded general recursion constant: an all-formula.  This
  form only occurs when translating a general induction axiom into a
  guarded general recursion constant, in order to achieve
  normalization of proofs via term normalization.  We have to consider
  the free variables in the scheme formulas, and let the type of the
  guarded general recursion constant depend on them.  This is needed
  to have the allnc-conversion be represented in term normalization.
  One uses
  \begin{equation*}
    \hbox{\texttt{all-formula-and-number-to-grecguard-const}%
      \index{all-formula-and-number-to-grecguard-const@\texttt{all-for{\dots}-to-grecguard-const}}}.
  \end{equation*}
\item For an efq-constant (of kind \texttt{'fixed-rules}): a formula.
  This form only occurs when translating an efq-aconst into an
  efq-constant, in order to achieve normalization of proofs via term
  normalization.  One uses
  \begin{equation*}
    \hbox{\texttt{formula-to-efq-const}%
      \index{formula-to-efg-const@\texttt{formula-to-efq-const}}}.
  \end{equation*}
\item For a constructor associated with an \inquotes{Intro} axiom.
  \begin{enumeratea}
  \item A number $i$ of a clause for an inductively defined predicate
    constant, and the constant idpc.  One uses
    \begin{equation*}
      \hbox{\texttt{number-and-idpredconst-to-intro-const}%
      \index{number-and-idpredconst-to-intro-const@\texttt{number{\dots}-to-intro-const}}}.
    \end{equation*}
  \item An ex-formula for an \inquotes{ExIntro} axiom.  One uses
    \begin{equation*}
      \hbox{\texttt{ex-formula-to-ex-intro-const}%
      \index{ex-formula-to-ex-intro-const@\texttt{ex-formula-to-ex-intro-const}}}.
    \end{equation*}
  \end{enumeratea}
\item For an ExElim constant (of kind \texttt{'fixed-rules}): an
  ex-formula and a conclusion.  One uses
  \begin{equation*}
    \hbox{\texttt{ex-formula-and-concl-to-ex-elim-const}%
      \index{ex-formula-and-concl-to-ex-elim-const@\texttt{ex-for{\dots}-to-ex-elim-const}}}.
  \end{equation*}
\end{enumerate}

Constructor, accessors and tests for all kinds of constants:
\begin{alignat*}{2}
  &\texttt{(make-const \textsl{obj-or-arity} \textsl{name} \textsl{kind}
    \textsl{uninst-type} \textsl{tsubst}}
  \\
  &\quad \texttt{\textsl{t-deg} \textsl{token-type}
    .\ \textsl{repro-data})},%
  \index{make-const@\texttt{make-const}}
  \\
  &\texttt{(const-to-object-or-arity \textsl{const})},%
  \index{const-to-object-or-arity@\texttt{const-to-object-or-arity}}
  \\
  &\texttt{(const-to-name \textsl{const})},%
  \index{const-to-name@\texttt{const-to-name}}
  \\
  &\texttt{(const-to-kind \textsl{const})},%
  \index{const-to-kind@\texttt{const-to-kind}}
  \\
  &\texttt{(const-to-uninst-type \textsl{const})},%
  \index{const-to-uninst-type@\texttt{const-to-uninst-type}}
  \\
  &\texttt{(const-to-tsubst \textsl{const})},%
  \index{const-to-tsubst@\texttt{const-to-tsubst}}
  \\
  &\texttt{(const-to-t-deg \textsl{const})},%
  \index{const-to-t-deg@\texttt{const-to-t-deg}}
  \\
  &\texttt{(const-to-token-type \textsl{const})},%
  \index{const-to-token-type@\texttt{const-to-token-type}}
  \\
  &\texttt{(const-to-repro-data \textsl{const})},%
  \index{const-to-repro-data@\texttt{const-to-arrow-types-or{\dots}}}
\end{alignat*}
From these we can define
\begin{alignat*}{2}
  &\texttt{(const-to-type \textsl{const})},%
  \index{const-to-type@\texttt{const-to-type}}
  \\
  &\texttt{(const-to-tvars \textsl{const})}.%
  \index{const-to-tvars@\texttt{const-to-tvars}}
\end{alignat*}
The test functions are
\begin{alignat*}{2}
  &\texttt{(const-form?\ \textsl{x})},%
  \index{const-form?@\texttt{const-form?}}
  \\
  &\texttt{(check-const\ \textsl{x})},%
  \index{check-const@\texttt{check-const}}
  \\
  &\texttt{(const?\ \textsl{x})},%
  \index{const?@\texttt{const?}}
  \\
  &\texttt{(const=?\ \textsl{x} \textsl{y})}.%%
  \index{const=?@\texttt{const=?}}
\end{alignat*}
\texttt{check-const} assumes that the constant is not one of those
used during proof normalization.  Hence \texttt{repro-data} must be empty.

A \emph{constructor}\index{constructor} is a special constant with no
rules.  We maintain an association list \texttt{CONSTRUCTORS}
assigning to every name of a constructor an association list
associating with every type substitution (restricted to the type
parameters) the corresponding instance of the constructor.  We provide
\begin{alignat*}{2}
  &\texttt{(constr-name?\ \textsl{string})},%
  \index{constr-name?@\texttt{constr-name?}}
  \\
  &\texttt{(constr-name-to-constr \textsl{name} <\textsl{tsubst}>)},%
  \index{constr-name-to-constr@\texttt{constr-name-to-constr}}
  \\
  &\texttt{(constr-name-and-tsubst-to-constr \textsl{name} \textsl{tsubst})}%
  \index{constr-name-and-tsubst-to-constr@\texttt{constr-name-and-tsubst{\dots}}}
\end{alignat*}
where in \texttt{(constr-name-to-constr \textsl{name}
  <\textsl{tsubst}>)}, \textsl{name} is a string or else of the
form \texttt{(ExIntro \textsl{formula})}.  If the optional
\textsl{tsubst} is not present, the empty substitution is used.

For given algebras one can display the associated constructors with their
types by calling
\begin{alignat*}{2}
  &\texttt{(display-alg \textsl{alg-name1} \dots)}%
  \index{display-alg@\texttt{display-alg}}.
\end{alignat*}

Recall that program constants allow user defined rules, and that we
keep the rules of a program constant and also its denoted objects
(depending on type substitutions) at a central place, a global
variable \texttt{PROGRAM-CONSTANTS}.  We have procedures recovering
information from the string denoting a program constant (via
\texttt{PROGRAM-CONSTANTS}):
\begin{alignat*}{2}
  &\texttt{(pconst-name?\ \textsl{string})},%
  \index{pconst-name?@\texttt{constr-name?}}
  \\
  &\texttt{(pconst-name-to-pconst \textsl{name})},%
  \index{pconst-name-to-pconst@\texttt{pconst-name-to-pconst}}
  \\
  &\texttt{(pconst-name-to-comprules \textsl{name})},%
  \index{pconst-name-to-comprules@\texttt{pconst-name-to-comprules}}
  \\
  &\texttt{(pconst-name-to-rewrules \textsl{name})},%
  \index{pconst-name-to-rewrules@\texttt{pconst-name-to-rewrules}}
  \\
  &\texttt{(pconst-name-to-inst-objs \textsl{name})},%
  \index{pconst-name-to-inst-objs@\texttt{pconst-name-to-inst-objs}}
  \\
  &\texttt{(pconst-name-and-tsubst-to-object \textsl{name} \textsl{tsubst})},%
  \index{pconst-name-and-tsubst-to-object@\texttt{pconst-name-and-tsubst-to-object}}
  \\
  &\texttt{(pconst-name-to-object \textsl{name})},%
  \index{pconst-name-to-object@\texttt{pconst-name-to-object}}
  \\
  &\texttt{(pconst-name-to-external-code \textsl{name})}.%
  \index{pconst-name-to-external-code@\texttt{pconst-name-to-external-code}}
\end{alignat*}

One can display the program constants together with their current
computation and rewrite rules by calling
\begin{alignat*}{2}
  &\texttt{(display-pconst \textsl{name1} \dots)}.%
  \index{display-pconst@\texttt{display-pconst}}
\end{alignat*}

To add and remove program constants we use
\begin{align*}
  &\texttt{(add-program-constant \textsl{name} \textsl{type} <\textsl{rest}>)},%
  \index{add-program-constant@\texttt{add-program-constant}}
  \\
  &\texttt{(remove-program-constant \textsl{string1} \dots)};
  \index{remove-program-constant@\texttt{remove-program-constant}}
\end{align*}
\textsl{rest} consists of an initial segment of the following list:
\texttt{t-deg} (default $0$), \texttt{token-type} (default
\texttt{const}) and \texttt{arity} (default maximal number of argument
types).

The degree of totality of a program constant can be changed from $0$
to $1$ provided we have proved that the program constant is in fact
total.  This change is done by calling \texttt{change-t-deg-to-one}%
\index{change-t-deg-to-one@\texttt{change-t-deg-to-one}} with the name
of the program constant.

To make program constants more readable we provide
\begin{alignat*}{2}
  &\texttt{(add-prefix-display-string \textsl{name1} \textsl{name2})},%
  \index{add-prefix-display-string@\texttt{add-prefix-display-string}}
  \\
  &\texttt{(add-postfix-display-string \textsl{name1} \textsl{name2})},%
  \index{add-postfix-display-string@\texttt{add-postfix-display-string}}
  \\
  &\texttt{(add-infix-display-string \textsl{name1} \textsl{name2})}.%
  \index{add-infix-display-string@\texttt{add-infix-display-string}}
\end{alignat*}
To add and remove computation and rewrite rules and also external code
we have
\begin{align*}
  &\texttt{(add-computation-rule \textsl{lhs} \textsl{rhs})},%%
  \index{add-computation-rule@\texttt{add-computation-rule}}
  \\
  &\texttt{(add-rewrite-rule \textsl{lhs} \textsl{rhs})},%
  \index{add-rewrite-rule@\texttt{add-rewrite-rule}}
  \\
  &\texttt{(add-external-code \textsl{name} \textsl{code})},%
  \index{add-external-code@\texttt{add-external-code}}
  \\
  &\texttt{(remove-computation-rules-for \textsl{lhs})},%
  \index{remove-computation-rules-for@\texttt{remove-computation-rules-for}}
  \\
  &\texttt{(remove-rewrite-rules-for \textsl{lhs})},%
  \index{remove-rewrite-rules-for@\texttt{remove-rewrite-rules-for}}
  \\
  &\texttt{(remove-external-code \textsl{name}).}
  \index{remove-external-code@\texttt{remove-external-code}}
\end{align*}

To generate our constants with fixed rules we use
\begin{alignat*}{2}
  &\texttt{(finalg-to-=-const \textsl{finalg})}%
  \index{finalg-to-=-const@\texttt{finalg-to-=-const}}
  &\quad& \text{equality},
  \\
  &\texttt{(finalg-to-e-const \textsl{finalg})}%%
  \index{finalg-to-e-const@\texttt{finalg-to-e-const}}
  && \text{existence},
  \\
  &\texttt{(arrow-types-to-rec-const .\ \textsl{arrow-types})}%
  \index{arrow-types-to-rec-const@\texttt{arrow-types-to-rec-const}}
  && \text{recursion},
  \\
  &\texttt{(alg-to-destr-const \textsl{alg})}%
  \index{alg-to-destr-const@\texttt{alg-to-destr-const}}
  &&\text{destructor},
  \\
  &\texttt{(ex-formula-and-concl-to-ex-elim-const}
  \\
  &\texttt{\qquad \textsl{ex-formula}
    \textsl{concl})}.%
  \index{ex-formula-and-concl-to-ex-elim-const@\texttt{ex-for{\dots}-to-ex-elim-const}}
\end{alignat*}
Corecursion will be treated below.

Similar to \texttt{arrow-types-to-rec-const} we can also define the
procedure \texttt{all-formulas-to-rec-const}.  It will be used to
achieve normalization of proofs via translating them in terms.

Similarly we have \texttt{arrow-type-to-cases-const}%
\index{arrow-type-to-cases-const@\texttt{arrow-type-to-cases-const}}
and on the proof level \texttt{all-formula-to-cases-const}%
\index{all-formula-to-cases-const@\texttt{all-formula-to-cases-const}}.
For elimination axioms we have
\begin{align*}
  &\texttt{(imp-formulas-to-rec-const .\ \textsl{imp-formulas})}.%
  \index{imp-formulas-to-rec-const@\texttt{imp-formulas-to-rec-const}}
\end{align*}

\emph{General recursion and induction}\index{general recursion}%
\index{recursion!general}%
\index{general induction}\index{induction!general} (work of Simon
Huber\index{Huber})
\begin{verbatim}
(GRecGuard rhos tau) :
(rhos=>nat)=>rhos=>(rhos=>(rhos=>tau)=>tau)=>boole=>tau
GRecGuard mu xs G True ->
   G xs([ys]GRecGuard mu ys G(mu ys<mu xs))
GRecGuard mu xs G False -> Inhab
\end{verbatim}
For convenience we add GRec with
\begin{verbatim}
GRec mu xs G -> GRecGuard mu xs G True
\end{verbatim}
There is also a variant with type parameters:
\begin{verbatim}
(GRecGuard m alphas rhos tau) :
alphas=>(rhos=>nat)=>rhos=>(rhos=>(rhos=>atomic=>tau)=>tau)=>
boole=>atomic=>tau
GRecGuard ts mu xs G True u ->
G xs ([ys,atomic]GRecGuard ts mu ys G (mu ys<mu xs) atomic)
GRecGuard ts mu xs G False u -> Efq u
\end{verbatim}
Note that this variant is only used to normalize proofs.  Here we need
that Efq is a constant.  Induction:
\begin{verbatim}
GInd : allnc zs all mu,xs(Prog_mu{xs|A(xs)} ->
         all boole(atom(boole) -> A(xs))), where
Prog_mu{xs|A(xs)} =
all xs(all ys(mu ys<mu xs -> A(ys)) -> A(xs))
\end{verbatim}
We get the ordinary general induction \texttt{GInd'} by:
\begin{verbatim}
GInd' ts mu xs M = GInd ts mu xs M True Truth
\end{verbatim}
Internally we have
\begin{alignat*}{2}
  &\texttt{(type-info-to-grecguard-const \textsl{type-info})},%
  \index{type-info-to-grecguard-const@\texttt{type-info-to-grecguard-const}}
  \\
  &\texttt{(type-info-to-grec-const \textsl{type-info})}
  \index{type-info-to-grec-const@\texttt{type-info-to-grec-const}}
\end{alignat*}
\texttt{all-formula-and-number-to-grecguard-const}%
\index{all-formula-and-number-to-grecguard-const@\texttt{all-for{\dots}-to-grecguard-const}}
is used to achieve normalization of proofs via translating them in
terms, to translate a gind-aconst.  In addition we need the number m
of quantifiers used for the axioms.

\emph{Corecursion}.
To generate the corecursion constants we use
\begin{alignat*}{2}
  &\texttt{(alg-or-arrow-types-to-corec-consts .\
    \textsl{arrow-types})},%%
  \index{alg-or-arrow-types-to-corec-consts@\texttt{alg-or-arrow-types-to-corec-consts}}
  \\
  &\texttt{(alg-or-arrow-types-to-corec-const .\
    \textsl{arrow-types})}.%%
  \index{alg-or-arrow-types-to-corec-const@\texttt{alg-or-arrow-types-to-corec-const}}
\end{alignat*}
To avoid being trapped in non-termination of the conversion rule for
corecursion, we now aim at a bounded reduction of corec constants.

In \texttt{corec-const-and-bound-to-bcorec-term}%
\index{corec-const-and-bound-to-bcorec-term@\texttt{corec-const-...-to-bcorec-term}}
we begin with constructing \texttt{corec-consts} (as in
\texttt{corec-const-to-corec-consts} above), for the base case of the
\texttt{bcorec-term}.  The product of their types is the value type of
the recursion operator (over $\typeN$).  Next the step-term
\begin{verbatim}
lambda (n prev)(abstr-if-term1 pair .. pair abstr-if-termN)
\end{verbatim}
is built.  We need variables \texttt{us} for the covals and
\texttt{vs} for the steps.  Each \texttt{(vi ui)} has type
\texttt{ysum-without-unit-of-product-types}.  For each product type we
introduce a product-variable $y$.  The components of the term
corresponding to $y$ are called \texttt{param-comps} and
\texttt{test-comps}.  Using the instantiated constructors we can build
the \texttt{abstr-constr-terms} for the constructors of the $i$-th
algebra, and using these the $i$-th if-term is constructed via
\texttt{corec-test-and-abstr-constr-terms-to-if-term}.

Finally \texttt{undelay-delayed-corec}%
\index{undelay-delayed-corec@\texttt{undelay-delayed-corec}} takes a
term and a non-negative integer (a bound) as arguments.  It replaces
every corecursion constant in the given term by the result of applying
\texttt{corec-const-and-bound-to-bcorec-term} to it and the given
bound.

\section{Predicates}
\label{S:Psyms}
Every predicate has an arity (i.e., a list of types) and denotes a
property of tuples of functionals of these types.  We have the following
three kinds of predicates:
\begin{enumeratei}
\item predicate variables;
\item predicate constants;
\item inductively and coinductively defined predicate constants.
\end{enumeratei}
\texttt{(predicate-to-arity \textsl{predicate})}%
\index{predicate-to-arity@\texttt{predicate-to-arity}} returns the
arity of a predicate.  A test for equality is
\texttt{(predicate-equal?\ \textsl{pred1} \textsl{pred2})}%
\index{predicate-equal?@\texttt{predicate-equal?}}.

\subsection{Predicate variables}
\label{SS:PredVars}
A predicate variable of arity\index{arity!of a predicate variable}
$\rho_1, \dots, \rho_n$ is a placeholder for a formula $A$ with
distinguished (different) variables $x_1, \dots, x_n$ of types
$\rho_1, \dots, \rho_n$.  Such an entity is called a
\emph{comprehension term}, written $\set{x_1, \dots, x_n}{A}$.
Totality matters for the abstracted variables of a comprehension term,
because of the inductively defined existential quantifier.  The
default is the use of partial variables.

Predicate variable names are provided in the form of an association
list, which assigns to the names their arities.  By default we have
the predicate variable \texttt{bot}\index{bottom} of arity
\texttt{(arity)}, called (logical) falsity.  It is viewed as a
predicate variable rather than a predicate constant, since (when
translating a classical proof into a constructive one) we want to
substitute for \texttt{bot}.

Often we will argue about \emph{Harrop formulas}%
\index{Harrop formula} only, i.e., formulas without computational
content.  For convenience we use a special sort of predicate variables
intended to range over comprehension terms with Harrop formulas only.
For example, $\verb#P^0#, \verb#P^1#, \verb#P^2#, \dots$ range over
comprehension terms with Harrop formulas, and $\texttt{P0},
\texttt{P1}, \texttt{P2}, \dots, \texttt{Q0}, \dots$ are general
predicate variables.  We say that \emph{Harrop degree}%
\index{Harrop degree} for the former is $1$, and for the latter $0$.

In the context of Gödel's Dialectica intepretation \cite{Goedel58} we
also need to deal with \inquotes{negative} computational content.
Therefore we also need a \inquotes{degree of negativity}%
\index{degree!of negativity} and denote it by \texttt{n-deg}, and we
call the Harrop degree the \inquotes{degree of positivity}%
\index{degree!of positivity} denoted \texttt{h-deg}.  We use
$\texttt{P'0}, \texttt{P'1}, \texttt{P'2}, \dots, \texttt{Q'0}, \dots$
for predicate variables of \texttt{h-deg} $0$ and \texttt{n-deg} $1$,
and $\verb#P^'0#, \verb#P^'1#, \verb#P^'2#, \dots$ for predicate
variables whose \texttt{h-deg} and \texttt{n-deg} are both $1$.

%% \subsection*{Interface}
We need constructors and accessors for arities
\begin{align*}
  &\texttt{(make-arity \textsl{type1} \dots)},%
  \index{make-arity@\texttt{make-arity}}
  \\
  &\texttt{(arity-to-types \textsl{arity})}.%
  \index{arity-to-types@\texttt{arity-to-types}}
\end{align*}
To display an arity we have
\begin{equation*}
  \texttt{(arity-to-string \textsl{arity})}.%
  \index{arity-to-string@\texttt{arity-to-string}}
\end{equation*}

We can test whether a string is a name for a predicate variable, and
if so compute its associated arity:
\begin{align*}
  &\texttt{(pvar-name?\ \textsl{string})},%
  \index{pvar-name?@\texttt{pvar-name?}}
  \\
  &\texttt{(pvar-name-to-arity \textsl{pvar-name})}.%
  \index{pvar-name-to-arity@\texttt{pvar-name-to-arity}}
\end{align*}

To add and remove names for predicate variables of a given arity
(e.g., $\texttt{Q}$ for predicate variables of arity \texttt{nat}), we
use
\begin{align*}
  &\texttt{(add-pvar-name \textsl{name1} \dots\ \textsl{arity})},%
  \index{add-pvar-name@\texttt{add-pvar-name}}
  \\
  &\texttt{(remove-pvar-name \textsl{name1} \dots)}.%
  \index{remove-pvar-name@\texttt{remove-pvar-name}}
\end{align*}

We need a constructor, accessors and tests for predicate variables.
\begin{alignat*}{2}
  &\texttt{(make-pvar \textsl{arity} \textsl{index} \textsl{h-deg}
    \textsl{n-deg} \textsl{name})}%
  \index{make-pvar@\texttt{make-pvar}} &\quad& \text{constructor},
  \\
  &\texttt{(pvar-to-arity \textsl{pvar})}%
  \index{pvar-to-arity@\texttt{pvar-to-arity}} && \text{accessor},
  \\
  &\texttt{(pvar-to-index \textsl{pvar})}%
  \index{pvar-to-index@\texttt{pvar-to-index}} && \text{accessor},
  \\
  &\texttt{(pvar-to-h-deg \textsl{pvar})}
  \index{pvar-to-h-deg@\texttt{pvar-to-h-deg}} && \text{accessor},
  \\
  &\texttt{(pvar-to-n-deg \textsl{pvar})}%
  \index{pvar-to-n-deg@\texttt{pvar-to-n-deg}} && \text{accessor},
  \\
  &\texttt{(pvar-to-name \textsl{pvar})}%
  \index{pvar-to-name@\texttt{pvar-to-name}} && \text{accessor},
  \\
  &\texttt{(pvar?\ \textsl{x})}\index{pvar?@\texttt{pvar?}}.
\end{alignat*}

For convenience we have the function
\begin{alignat*}{2}
  &\texttt{(mk-pvar \textsl{arity} <\textsl{index}> <\textsl{h-deg}>
     <\textsl{n-deg}> <\textsl{name}>)}.
\end{alignat*}
The arity is a required argument; the remaining arguments are
optional.  The default for \textsl{index} is $-1$, for \textsl{h-deg}
and \textsl{n-deg} it is $0$ and for \textsl{name} it is given by
\texttt{(default-pvar-name \textsl{arity})}.

It is guaranteed that parsing a displayed predicate variable
reproduces the predicate variable; the converse need not be the case
(we may want to convert it into some canonical form).

\subsection{Predicate constants}
\label{SS:PredConsts}
We also allow \emph{predicate constants}%%
\index{predicate constant}.  The general reason for having them is
that sometimes we want axiomatized predicates, which are \emph{not}
placeholders for formulas.  The main example is the totality predicate
constant, intended to denote the set of total objects of a given type.
We will see below (in section~\ref{SS:IDPredConsts}) that in case this
type is (i) an algebra we can define the totality predicate
inductively, and (ii) an arrow or a pair type we can define it
explicitly.  However, we also allow type variables $\alpha$ (and
substitutions for them), and certainly cannot know what property the
\inquotes{total} elements of type $\alpha$ should have.  Therefore we
provide a totality predicate constant $\GTotal_{\rho}$ of arity
$(\rho)$ at an arbitrary type $\rho$; this is necessary for to allow a
type substitution $\alpha \mapsto \rho$ in formulas involving
$\GTotal_{\alpha}$.  However, a formula $\GTotal_{\rho} r$ can be
\inquotes{unfolded} in case $\rho$ is an algebra, an arrow or a pair
type: $\GTotal_{\iota} r$ unfolds by means of the inductively defined
totality predicate for the algebra $\iota$, and
\begin{align*}
  &\GTotal_{\rho \typeTo \sigma} r \defeq
  \allnc_{\hat{x}} (\GTotal_{\rho} \hat{x} \to \GTotal_{\sigma}(r \hat{x})),
  \\
  &\GTotal_{\rho \typeProd \sigma} r \defeq
  \GTotal_{\rho} r_0 \land \GTotal_{\sigma} r_1.
\end{align*}
This unfolding is done by means of \texttt{(unfold-formula
  \textsl{formula})}\index{unfold-formula@\texttt{unfold-formula}}
(which also unfolds classical existential quantifiers).  We also
provide
\begin{equation*}
  \texttt{(term-to-totality-formula \textsl{term})},
  \index{term-to-totality-formula@\texttt{term-to-totality-formula}}
\end{equation*}
which when applied to a term $r$ of type $\rho$ returns the result of
unfolding $\GTotal_{\rho} r$.

The inductively defined totality predicate for an algebra $\iota$ is
computationally relevant (c.r.)\ and has its witnesses in the same
algebra $\iota$.  Therefore it is mandatory to consider
$\GTotal_{\rho}$ as c.r.\ as well, and let $\rho$ be the type of its
witnesses.

When later (in section~\ref{S:ExtrTerms}) we consider realizability it
will be necessary to define what $t \mr \GTotal_{\rho} s$ means.
Since again $\GTotal_{\alpha}$ is unknown we provide another predicate
constant $\GTotal^{\mrind}_{\rho}$ and define $t \mr \GTotal_{\rho} s
\defeq \GTotal^{\mrind}_{\rho} t s$.  Clearly
$\GTotal^{\mrind}_{\rho}$ has arity $(\rho, \rho)$ and is
computationally irrelevant.  Again $\GTotal^{\mrind}_{\rho} t s$
unfolds by means of an inductively defined predicate for the algebra
$\iota$, and
\begin{align*}
  &\GTotal^{\mrind}_{\rho \typeTo \sigma} t s \defeq
  \allnc_{\hat{x}, \hat{y}} (\GTotal^{\mrind}_{\rho} \hat{x} \hat{y} \to
  \GTotal^{\mrind}_{\sigma}(t \hat{x}, s \hat{y})),
  \\
  &\GTotal^{\mrind}_{\rho \typeProd \sigma} t s \defeq
  \GTotal^{\mrind}_{\rho} t_0 s_0 \land \GTotal^{\mrind}_{\sigma} t_1 s_1.
\end{align*}
This unfolding is done by calling
\begin{equation*}
  \texttt{(terms-to-mr-totality-formula \textsl{term1} \textsl{term2})},
  \index{terms-to-mr-totality-formula@\texttt{terms-to-mr-totality-formula}}
\end{equation*}
which when applied to terms $t$ and $s$ of type $\rho$ returns the
result of unfolding $\GTotal^{\mrind}_{\rho} t s$.

It is also possible to add (and later remove) further computationally
irrelevant predicate constants via
\begin{align*}
  &\texttt{(add-predconst-name \textsl{name1} \dots\ \textsl{arity})},%
  \index{add-predconst-name@\texttt{add-predconst-name}}
  \\
  &\texttt{(remove-predconst-name \textsl{name1} \dots)}.%
  \index{remove-predconst-name@\texttt{remove-predconst-name}}
\end{align*}
We have a constructor, accessors and tests for predicate constants.
\begin{alignat*}{2}
  &\texttt{(make-predconst \textsl{uninst-arity} \textsl{tsubst}
    \textsl{index} \textsl{name})}%
  \index{make-predconst@\texttt{make-predconst}}
  &\quad& \text{constructor},
  \\
  &\texttt{(predconst-to-uninst-arity \textsl{predconst})}%
  \index{predconst-to-uninst-arity@\texttt{predconst-to-uninst-arity}}
  && \text{accessor},
  \\
  &\texttt{(predconst-to-tsubst \textsl{predconst})}%
  \index{predconst-to-tsubst@\texttt{predconst-to-tsubst}}
  && \text{accessor},
  \\
  &\texttt{(predconst-to-index \textsl{predconst})}%
  \index{predconst-to-index@\texttt{predconst-to-index}}
  && \text{accessor},
  \\
  &\texttt{(predconst-to-name \textsl{predconst})}%
  \index{predconst-to-name@\texttt{predconst-to-name}}
  && \text{accessor},
  \\
  &\texttt{(predconst?\ \textsl{x})}.%
  \index{predconst?@\texttt{predconst?}}
\end{alignat*}
Moreover we provide
\begin{alignat*}{2}
  &\texttt{(predconst-name?\ \textsl{name})},%
  \index{predconst-name?@\texttt{predconst-name?}}
  \\
  &\texttt{(predconst-name-to-arity \textsl{predconst-name})},%
  \index{predconst-name-to-arity@\texttt{predconst-name-to-arity}}
  \\
  &\texttt{(predconst-to-string \textsl{predconst})}.%
  \index{predconst-to-string@\texttt{predconst-to-string}}
\end{alignat*}
A predicate constant does not change its name under a type
substitution; this is in contrast to predicate (and other) variables.
Notice also that the parser can infer from the arguments the types
$\rho_1 \dots \rho_n$ to be substituted for the type variables in the
uninstantiated arity of $P$.

\subsection{Inductively defined predicate constants}
\label{SS:IDPredConsts}
When we want to make propositions about computable functionals and
their domains of partial continuous functionals, it is perfectly
natural to take, as initial propositions, ones formed inductively or
coinductively.  However, for simplicity we omit the treatment of
coinductive definitions and deal with inductive definitions only.  For
example, in the algebra $\typeN$ we can inductively define
\emph{totality}\index{totality}%
\index{inductive definition!of totality} by the clauses
\begin{equation*}
  \Total_{\typeN} 0,
  \qquad
  \forall_n(\Total_{\typeN} n \to \Total_{\typeN}(\suc n)).
\end{equation*}
Its least-fixed-point scheme will now be taken in the form
\begin{equation*}
  \forall_n(\Total_{\typeN} n \to
  A(0) \to
  \forall_n(\Total_{\typeN} n \to A(n) \to A(\suc n)) \to
  A(n)).
\end{equation*}
The reason for writing it in this way is that it fits more
conveniently with the logical elimination rules, which will be useful
in the proof of the soundness theorem.  It expresses that every
\inquotes{competitor} $\set {n} {A(n)}$ satisfying the same clauses
contains $\Total_{\typeN}$.  This is the usual induction schema for
natural numbers, which clearly only holds for \inquotes{total} numbers
(i.e., total ideals in the information system for $\typeN$).  Notice
that we have used a \inquotes{strengthened}%
\index{induction!strengthened form} form of the \inquotes{step
  formula}, namely $\forall_n(\Total_{\typeN} n \to A(n) \to A(\suc
n))$ rather than $\forall_n(A(n) \to A(\suc n))$.  In applications of
the least-fixed-point axiom this simplifies the proof of the
\inquotes{induction step}, since we have the additional hypothesis
$\Total_{\typeN}(n)$ available.  Totality for an arbitrary algebra can
be defined similarly.  Consider for example the non-finitary algebra
$\typeOrd$ (cf.\ \ref{SS:AlgsTypes}), with constructors $0$, successor
$\suc$ of type $\typeOrd \typeTo \typeOrd$ and supremum $\Sup$ of type
$(\typeN \typeTo \typeOrd) \typeTo \typeOrd$.  Its clauses are
\begin{equation*}
  \Total_{\typeOrd} 0,
  \qquad
  \forall_x( \Total_{\typeOrd} x \to \Total_{\typeOrd}( \suc x)),
  \qquad
  \forall_f( \forall_{n \in \Total_{\typeN}} \Total_{\typeOrd}(f n) \to
  \Total_{\typeOrd}( \Sup(f))),
\end{equation*}
and its least-fixed-point scheme is
\begin{align*}
  \forall_x( \Total_{\typeOrd} x \to {}
  &A(0) \to {}
  \\
  &\forall_x( \Total_{\typeOrd} x \to A(x) \to A(\suc x)) \to {}
  \\
  &\forall_f(
  \forall_{n \in \Total} \Total_{\typeOrd}(f n) \to
  \forall_{n \in \Total} A(f n) \to
  A(\Sup(f))) \to {}
  \\
  &A(x)).
\end{align*}

Generally, an inductively defined predicate $I$ is given by $k$
clauses, which are of the form
\begin{equation*}
  K_i \defeq \forall_{\vec{x}}((A_{\nu}(I))_{\nu<n} \to I \vec{r}\,)
  \quad \hbox{$(i<k)$}.
\end{equation*}
It is not required that all universal quantifiers precede all
implications.

Our formulas will be defined by the operations of implication $A \to
B$ and universal quantification $\forall_{x^{\rho}} A$ from
inductively defined predicates $\mu_X \vec{K}$, where $X$ is a
\inquotes{predicate variable}, and the $K_i$ are \inquotes{clauses}.
Every predicate has an \emph{arity}, which is a possibly empty list of
types.

\begin{definition*}[Formulas and predicates]
  By simultaneous induction we define formula forms
  \begin{equation*}
    A,B \BNFdef P \vec{r} \BNFor A \to B \BNFor \forall_x A
  \end{equation*}
  and predicate forms
  \begin{equation*}
    P,Q \BNFdef X \BNFor \set {\vec{x}} {A} \BNFor
    \mu_X(\forall_{\vec{x}_i}((A_{i \nu})_{\nu < n_i} \to X \vec{r}_i))_{i<k}
  \end{equation*}
  with $X$ a predicate variable, $k \ge 1$ and $\vec{x}_i$ all free
  variables in $(A_{i \nu})_{\nu < n_i} \to X \vec{r}_i$ (it is not
  necessary to allow object parameters in inductively defined
  predicates, since they can be taken as extra arguments).  Let $C$
  denote both formula and predicate forms.  Let $\FPV(C)$ denote the set
  of free predicate variables in $C$.  We define $\SP(Y, C)$
  \inquotes{$Y$ occurs at most strictly positive in $C$} by induction on
  $C$.
  \begin{align*}
    &\frac  {\SP(Y, P)} {\SP(Y, P \vec{r}\,)}
    \qquad
    \frac{Y \notin \FPV(A)\quad \SP(Y, B)}
         {\SP(Y, A \to B)}
         \qquad
         \frac {\SP(Y,A)} {\SP(Y, \forall_x A)}
         \\
         &\SP(Y, X)
         \qquad
         \frac {\SP(Y,A)} {\SP(Y, \set {\vec{x}} {A})}
         \qquad
         \frac{\hbox{$\SP(Y, A_{i\nu})$ for all $i{<}k$, $\nu{<}n_i$}}
              {\SP(Y, \mu_{X}(\forall_{\vec{x}_i}
                ((A_{i \nu})_{\nu < n_i} \to X \vec{r}_i))_{i<k})}
  \end{align*}
  Now we can define $\formulas(A)$ \inquotes{$A$ is a formula} and
  $\preds(P)$ \inquotes{$P$ is a predicate}, again by simultaneous
  induction.
  \begin{align*}
    &\frac {\preds(P)} {\formulas(P \vec{r}\,)}
    \qquad
    \frac{\formulas(A) \quad \formulas(B)}
         {\formulas(A \to B)}
         \qquad
         \frac {\formulas(A)} {\formulas(\forall_x A)}
         \\
         &\preds(X)
         \qquad
         \frac {\formulas(A)} {\preds(\set {\vec{x}} {A})}
         \\
         &\frac{\hbox{$\formulas(A_{i\nu})$ and $\SP(X, A_{i\nu})$
             for all $i{<}k$, $\nu{<}n_i$
             \quad
             $X \notin \FPV(A_{0\nu})$ for all $\nu{<}n_0$}}
           {\preds(\mu_{X}(\forall_{\vec{x}_i}
             ((A_{i\nu})_{\nu < n_i} \to X \vec{r}_i))_{i<k})}
  \end{align*}
  We call
  \begin{equation*}
    I \defeq \mu_{X}(\forall_{\vec{x}_i} ((A_{i\nu})_{\nu < n_i}
    \to X \vec{r}_i))_{i<k}
  \end{equation*}
  an inductive (or inductively defined) predicate.  Sometimes it is
  helpful to display the predicate parameters and write $I(\vec{Y},
  \vec{Z}\,)$, where $\vec{Y}, \vec{Z}$ are all predicate variables free
  in some $A_{i\nu}$ except $X$, and $\vec{Y}$ are the ones occuring
  only strictly positive.  If we write the $i$-th component of $I$ in
  the form $\forall_{\vec{x}} ((A_{\nu}(X))_{\nu < n} \to X \vec{r}\,)$,
  then we call
  \begin{equation}
    \label{E:IntroID}
    K_i \defeq \forall_{\vec{x}}((A_{\nu}(I))_{\nu<n} \to I \vec{r}\,)
  \end{equation}
  the $i$-th \emph{clause}\index{clause} (or \emph{introduction axiom})
  of $I$, denoted $I^{+}_i$.

  Here $\vec{A} \to B$ means $A_0 \to \dots \to A_{n-1} \to B$,
  associated to the right.  The terms $\vec{r}$ are those introduced
  in section~\ref{S:Terms}, i.e., typed terms built from variables and
  constants by abstraction and application, and (importantly) those
  with a common reduct are identified.  In $\forall_{\vec{x}}
  ((A_{\nu}(X))_{\nu < n} \to X \vec{r}\,)$ we call $A_{\nu}(X)$ a
  \emph{parameter} premise%
  \index{parameter premise} if $X$ does not occur in it, and a
  \emph{recursive} premise%
  \index{recursive premise} otherwise.
  A recursive premise $A_{\nu}(X)$ is \emph{nested} if it has an
  occurrence of $X$ in a strictly positive parameter position of another
  (previously defined) inductive predicate, and unnested otherwise.  An
  inductive predicate $I$ is called \emph{nested}%
  \index{predicate!nested} if it has a clause with at least one nested
  recursive premise, and \emph{unnested}%
  \index{predicate!unnested} otherwise.
\end{definition*}

A predicate of the form $\set{\vec{x}}{C}$ is called a
\emph{comprehension term}%
\index{comprehension term}.
We identify $\set {\vec{x}} {C(\vec{x}\,)}\vec{r}$ with
$C(\vec{r}\,)$.  An inductively defined predicate is \emph{finitary}
if its clauses have recursive premises of the form $X \vec{s}$ only.

\begin{definition*}[Theory of computable functionals, $\TCF$]
  $\TCF$\index{$\TCF$} is the system in minimal logic for $\to$ and
  $\forall$, whose formulas are those in $\formulas$ above, and whose
  axioms are the following.  For each inductively defined predicate,
  there are \inquotes{closure} or introduction axioms, together with a
  \inquotes{least-fixed-point}%
  \index{least-fixed-point axiom} or elimination%
  \index{elimination axiom} axiom.  In more detail, consider an
  inductively defined predicate $I \defeq \mu_X( K_0, \dots,
  K_{k-1})$.  For each of the $k$ clauses we have the introduction
  axiom \eqref{E:IntroID}.  Moreover, we have an \emph{elimination
    axiom} $I^{-}$:
  \begin{equation}
    \label{E:ElimID}
    \forall_{\vec{x}} ( I \vec{x} \to
    (\forall_{\vec{x}_i}(
    (A_{i \nu}(I \cap X))_{\nu < n_i} \to X \vec{r}_i))_{i<k} \to X \vec{x}\,)
  \end{equation}
  where $I \cap X$ abbreviates $\set {\vec{x}} {I \vec{x} \land X
    \vec{x}}$ with $\land$ defined (inductively) below.  Here $X$ can
  be thought of as a \inquotes{competitor} predicate.
\end{definition*}

\subsection{Examples of inductive predicates}
\label{SS:ExamplesID}
As an important example we now give the inductive definition of
Leibniz equality.  However, a word of warning is in order here: we
need to distinguish four separate, but closely related equalities.
\begin{enumeratei}
\item Firstly, defined function constants $D$ are introduced by
  computation rules, written $l=r$, but intended as left-to-right
  rewrites.
\item Secondly, we have Leibniz equality $\Eq$ inductively defined
  below.
\item Thirdly, pointwise equality between partial continuous
  functionals will be defined inductively as well.
\item Fourthly, if $l$ and $r$ have a finitary algebra as their type,
  $l=r$ can be read as a boolean term, where $=$ is the decidable
  equality \index{equality!decidable} defined in section~\ref{S:Terms}
  as a boolean-valued binary function.
\end{enumeratei}

\subsubsection*{Leibniz equality}
We define Leibniz equality\index{equality!Leibniz}%
\index{Leibniz equality} by
\begin{equation*}
  \Eq(\rho) \defeq \mu_X( \forall_x X(x^{\rho}, x^{\rho}) ).
\end{equation*}
The introduction axiom is
\begin{equation*}
%%   \Eq^{+} \colon
  \forall_x(\eqd {x^{\rho}} {x^{\rho}})
\end{equation*}
and the elimination axiom
\begin{equation*}
%%   \Eq^{-} \colon
  \forall_{x,y} ( \eqd {x} {y} \to
  \forall_x X x x \to X x y ),
\end{equation*}
where $\eqd {x} {y}$ abbreviates $\Eq(\rho)(x^{\rho}, y^{\rho})$.  In
Minlog this is displayed as $\verb#x eqd y#$.

\begin{lemma*}[Compatibility of $\Eq$]%[CompatEq]
  $\forall_{x, y} (
  \eqd {x} {y} \to A(x) \to A(y) )$.
\end{lemma*}

\begin{proof}
  Use the elimination axiom with $P x y \defeq (A(x) \to A(y))$.
\end{proof}

Using compatibility of $\Eq$ one easily proves symmetry and
transitivity.  Define \emph{falsity}\index{$\falsityF$}%
\index{falsity $\falsityF$} by $\falsityF \defeq (\eqd {\false}
{\true}$).  Then we have

\begin{theorem*}[Ex-Falso-Quodlibet\index{ex-falso-quodlibet}]
  For every formula $A$ without predicate parameters we can derive
  $\falsityF \to A$.
\end{theorem*}

\begin{proof}
  We first show that $\falsityF \to \eqd {x^{\rho}} {y^{\rho}}$.  To
  see this, we first obtain $\eqd {\rec_{\typeB}^{\rho} \false x y}
  {\rec_{\typeB}^{\rho} \false x y}$ from the introduction axiom.
  %% since $\rec_{\typeB}^{\rho} \false x y$ is an allowed term,
  Then from $\eqd {\false} {\true}$ we get $\eqd {\rec_{\typeB}^{\rho}
    \true x y} {\rec_{\typeB}^{\rho} \false x y}$ by compatibility.
  Now $\rec_{\typeB}^{\rho} \true x y$ converts to $x$ and
  $\rec_{\typeB}^{\rho} \false x y$ converts to $y$.  Hence $\eqd
  {x^{\rho}} {y^{\rho}}$, since we identify terms with a common reduct.

  The claim can now be proved by induction on $A \in \formulas$.
  \emph{Case} $I \vec{r}$.  By definition the clause $K_0$ is
  \inquotes{nullary}, i.e., of the form $\forall_{\vec{x}}(
  (A_{\nu})_{\nu<n} \to I \vec{s}\,)$ with no occurrence of $I$ in the
  $A_{\nu}$.  By induction hypothesis from $\falsityF$ we can derive
  all premises $A_{\nu}$.  Hence $I \vec{s}$.  From $\falsityF$ we
  also obtain $\eqd {r_i} {s_i}$, by the remark above.  Hence $I
  \vec{r}$ by compatibility.  The cases $A \to B$ and $\forall_x A$
  are obvious.
\end{proof}

A crucial use of Leibniz equality is that it allows to lift a boolean
term $r^{\typeB}$ to a formula, by considering $\eqd {r^{\typeB}}
{\true}$ instead.  For convenience we introduce a new predicate
constant $\atom$ of arity $(\typeB)$ and define $\atom(r^{\typeB})$ as
an abbreviation of $\eqd {r^{\typeB}} {\true}$.  Formally, we use the
axioms
\begin{align*}
  \mathtt{AtomToEqDTrue}\index{AtomToEqDTrue@\texttt{AtomToEqDTrue}}
  &\colon \forall_{p^{\typeB}}(\atom(p^{\typeB}) \to \eqd {p^{\typeB}} {\true}),
  \\
  \mathtt{EqDTrueToAtom}\index{EqDTrueToAtom@\texttt{EqDTrueToAtom}}
  &\colon \forall_{p^{\typeB}}(\eqd {p^{\typeB}} {\true} \to \atom(p^{\typeB})).
\end{align*}
This opens up a convenient way to deal with equality on finitary
algebras.  The computation rules ensure that for instance the boolean
term $\suc r =_{\typeN} \suc s$ or more precisely, ${=_{\typeN}}(\suc
r, \suc s)$, is identified with $r =_{\typeN} s$.  We can now turn
this boolean term into the formula $\eqd {(\suc r =_{\typeN} \suc s)}
{\true}$, which again is abbreviated by $\suc r =_{\typeN} \suc s$,
but this time with the understanding that it is a formula.  Then
(importantly) the two formulas $\suc r =_{\typeN} \suc s$ and $r
=_{\typeN} s$ are identified because the latter is a reduct of the
first.  Consequently there is no need to prove the implication $\suc r
=_{\typeN} \suc s \to r =_{\typeN} s$ explicitly.

\subsubsection*{Pointwise equality\index{equality!pointwise} $=_{\rho}$}
For every constructor $\constr_i$ of an algebra $\iota$ we have an
introduction axiom
\begin{equation*}
  \forall_{\vec{y}, \vec{z}} (
  \vec{y}^{P} =_{\vec{\rho}} \vec{z}^{P} \to
  ( \forall_{\vec{x}_{\nu}}
  (y^{R}_{m+\nu} \vec{x}_{\nu} =_{\iota}
  z^{R}_{m+\nu} \vec{x}_{\nu}) )_{\nu < n} \to
  \constr_i \vec{y}^{P}\vec{y}^{R} =_{\iota} \constr_i \vec{z}^{P}\vec{z}^{R}\,).
\end{equation*}
For an arrow type $\rho \typeTo \sigma$ the introduction axiom is
explicit, in the sense that it has no recursive premise:
\begin{equation*}
  \forall_{x_1, x_2} (
  \forall_y (x_1 y =_{\sigma} x_2 y) \to
   x_1 =_{\rho \typeTo \sigma} x_2 ).
\end{equation*}

For example, $=_{\typeN}$ is inductively defined by
\begin{equation*}
  \begin{split}
    &0 =_{\typeN} 0,
    \\
    &\forall_{n_1, n_2 } (n_1 =_{\typeN} n_2 \to
    \suc n_1 =_{\typeN} \suc n_2),
  \end{split}
\end{equation*}
and the elimination axiom is
\begin{equation*}
  \begin{split}
    \forall_{n_1, n_2} (n_1 =_{\typeN} n_2 \to {}
    &X 0 0 \to
    \\
    &\forall_{n_1, n_2} (n_1 =_{\typeN} n_2 \to
    X n_1 n_2 \to
    X(\suc n_1, \suc n_2)) \to {}
    \\
    &X n_1 n_2 ).
  \end{split}
\end{equation*}
The main purpose of pointwise equality is that it allows to formulate
the extensionality axiom: we express the extensionality%
\index{axiom!of extensionality} of our intended model by stipulating
that pointwise equality is equivalent to Leibniz
equality\index{equality!Leibniz}%
\index{Leibniz equality}.

\begin{axiom*}[Extensionality]
  $\forall_{x_1, x_2}
  (x_1 =_{\rho} x_2 \leftrightarrow \eqd {x_1} {x_2})$.
\end{axiom*}

We write $\ETCF$ when the extensionality axioms are present.  --- One
of the main points of $\TCF$ is that it allows the logical connectives
existence, conjunction and disjunction to be inductively defined as
predicates.  This was first discovered by Martin-Löf
\cite{MartinLoef71}\index{Martin-Löf}.

\subsubsection*{Existential quantifier}
\index{inductive definition!of existence}
\begin{equation*}
  \Ex(Y) \defeq
  \mu_X ( \forall_x (Y x^{\rho} \to X) )
\end{equation*}
(\inquotes{D} indicates that the existential quantifier is inductively
defined; it also reminds on \inquotes{double}, since both parts -- the
variable $x$ and the kernel $A$ -- are of computational significance.
Later when considering decorations\index{decoration} we will define
other computational variants of the existential quantifier).

 The introduction axiom is
\begin{equation*}
  \forall_x (A \to \exists_x A),
\end{equation*}
where $\exists_x A$ (displayed $\verb#exd x A#$) abbreviates $\Ex(\set
{x^{\rho}} {A})$, and the elimination axiom is
\begin{equation*}
  \exists_x A \to \forall_x (A \to X) \to X.
\end{equation*}


\subsubsection*{Conjunction\index{conjunction}}
\index{inductive definition!of conjunction}
We define
\begin{equation*}
  \LAnd(Y, Z) \defeq \mu_X ( Y \to Z \to X ).
\end{equation*}
The introduction axiom is
\begin{equation*}
  A \to B \to A \land B
\end{equation*}
where $A \land B$ (displayed $\verb#A andd B#$) abbreviates
$\LAnd(\set {} {A}, \set {} {B})$, and the elimination axiom is
\begin{equation*}
  A \land B \to (A \to B \to X) \to X.
\end{equation*}

\begin{remark*}
  In addition to the inductively defined existential quantifier%
  \index{existential quantifier!primitive} and conjunction%
  \index{conjunction!primitive}, in Minlog there are also
  \inquotes{primitive} variants, displayed $\verb#ex x A#$ and
  $\verb#A & B#$.  Both make use of a (again \inquotes{primitive})
  version of the product type (displayed $\verb#rho@@sigma#$)%
  \index{product type!promitive}, which is based on the pairing
  operation of the underlying progamming language (Scheme).  The
  reason to have them is that sometimes this allows a more efficient
  evaluation (i.e., normalization) of extracted terms.
\end{remark*}

\subsubsection*{Disjunction\index{disjunction}}
\index{inductive definition!of disjunction}
We define
\begin{equation*}
  \LOr(Y, Z) \defeq \mu_X ( Y \to X, Z \to X ).
\end{equation*}
The introduction axioms are
\begin{equation*}
  A \to A \lor B,
  \qquad
  B \to A \lor B,
\end{equation*}
where $A \lor B$ (displayed $\verb#A ord B#$) abbreviates $\LOr(\set
{} {A}, \set {} {B})$, and the elimination axiom is
\begin{equation*}
  A \lor B \to (A \to X) \to (B \to X) \to X.
\end{equation*}

\begin{remark*}
  Alternatively, disjunction\index{disjunction} $A \lor B$ could be
  defined by the formula $\ex_p((p \to A) \land (\neg p \to B))$
  with $p$ a boolean variable.  However, for an analysis of the
  computational content of coinductively defined predicates it is
  better to define it inductively.
\end{remark*}

We give some more familiar examples of inductively defined predicates.

\subsubsection*{The even numbers}
The introduction axioms are
\begin{equation*}
  \Even(0),
  \qquad
  \forall_n (\Even(n) \to \Even(\suc (\suc n)))
\end{equation*}
and the elimination axiom is
\begin{equation*}
  \forall_n ( \Even(n) \to {}
  X 0 \to
  \forall_n (\Even(n) \to X n \to X(\suc (\suc n))) \to X n ).
\end{equation*}

\subsubsection*{Reflexive transitive closure}
Let $\prec$ be a binary relation.  The \emph{reflexive transitive
  closure} of $\prec$ is inductively defined as follows.  The
introduction axioms are
\begin{align*}
  &\forall_x \TrCl(x,x)),
  \\
  &\forall_{x,y,z}(y \prec z \to \TrCl(x,y) \to \TrCl(x,z))
\end{align*}
and the elimination axiom is
\begin{equation*}
  \begin{split}
    \forall_{x, y} ( \TrCl(x, y) \to {}
    &\forall_x X x x \to
    \\
    &\forall_{x,y,z}(y \prec z \to
    \TrCl(x,y) \to X y z \to X x z) \to
    \\
    &X x y ).
  \end{split}
\end{equation*}

\subsubsection*{Accessible part}
Let $\prec$ again be a binary relation.  The \emph{accessible part}%
\index{relation!accessible part} of $\prec$ is inductively defined as
follows.  The introduction axioms are
\begin{align*}
  &\forall_x (\falsityF \to \Acc(x)),
  \\
  &\forall_x (\forall_{y \prec x} \Acc(y) \to \Acc(x)),
\end{align*}
and the elimination axiom is
\begin{equation*}
  \begin{split}
    \forall_x ( \Acc(x) \to {}
    &\forall_x (\falsityF \to X x) \to {}
    \\
    &\forall_x (
    \forall_{y \prec x} \Acc(y) \to
    \forall_{y \prec x} X y \to X x ) \to {}
    \\
    &X x ).
  \end{split}
\end{equation*}

\subsection{Totality and induction}
\label{SS:TotalInd}
We now inductively define general totality predicates.  Let us first
look at some examples.  The clauses defining totality\index{totality}
for the algebra $\typeN$ are
\begin{equation*}
  \Total_{\typeN} 0,
  \qquad
  \forall_n(\Total_{\typeN} n \to \Total_{\typeN}(\suc n)).
  %% \allnc_n(\Total_{\typeN} n \toc \Total_{\typeN}(\suc n)).
\end{equation*}
The least-fixed-point axiom is is according to \eqref{E:ElimID}
\begin{equation*}
  \forall_n(\Total_{\typeN} n \to
  X 0 \to
  \forall_n((\Total_{\typeN} \land X)n \to X(\suc n)) \to
  X n).
\end{equation*}
Written differently  (with \inquotes{duplication}) we obtain
\begin{equation*}
  \forall_n(\Total_{\typeN} n \to
  X 0 \to
  \forall_n(\Total_{\typeN} n \to X n \to X(\suc n)) \to
  X n).
  %% \allnc_n(\Total_{\typeN} n \toc
  %% X 0 \toc
  %% \allnc_n(\Total_{\typeN} n \toc X n \toc X(\suc n)) \toc
  %% X n).
\end{equation*}
We call this least-fixed-point axiom an \emph{induction} axiom, and
write $\ind_{\typeN}^{n, X}$ or $\ind_{n,X}$ for
$\Total_{\typeN}^{-}$.  The indices $n, X$ are omitted when they can
be inferred from the context.  Clearly the partial continuous
functionals with $\Total_{\typeN}$ interpreted as the total ideals for
$\typeN$ provide a model of $\TCF$ extended by these axioms.

For the algebra $\typeBin$ of derivations totality is inductively
defined by the clauses
\begin{equation*}
  \Total_{\typeBin} 0^{\typeBin},
  \qquad
  \forall_x(\Total_{\typeBin} x \to
  \forall_y(\Total_{\typeBin} y \to \Total_{\typeBin}(
  \constr^{\typeBin \typeTo \typeBin \typeTo \typeBin} x y))),
\end{equation*}
with least-fixed-point axiom
\begin{align*}
  \forall_x(
  \Total_{\typeBin} x \to {}
  &X 0^{\typeBin} \to {}
  \\
  &\forall_x(\Total_{\typeBin} x \to X x \to
  \forall_y(\Total_{\typeBin} y \to X y \to
  X(\constr^{\typeBin \typeTo \typeBin \typeTo \typeBin} x y))) \to {}
  \\
  &X x).
\end{align*}
Again, the partial continuous functionals with $\Total_{\typeBin}$
interpreted as the total ideals for $\typeBin$ (i.e., the finite
derivations) provide a model.

Generally we define $\RTotal_{\rho}$ called \emph{relative totality}%
\index{totality!relative}, and its special case
$\Total_{\rho}$ called (absolute) \emph{totality}%
\index{totality!absolute}.
The definition of $\RTotal_{\rho}$ is relative to an assigment of
predicate variables $Y$ of arity $(\alpha)$ to type variables
$\alpha$.

\begin{definition*}[Relative totality $\RTotal$]
  Let $\iota = \mu_{\xi} (\kappa_0, \dots, \kappa_{k-1}) \in
  \mathrm{Alg}(\vec{\alpha}\,)$ with $\kappa_i =
  (\rho_{\nu}(\vec{\alpha}, \xi))_{\nu < n} \typeTo \xi$.  Then
  $\RTotal_{\iota} \defeq \mu_X( K_0, \dots, K_{k-1})$, with
  \begin{equation*}
    K_i \defeq \forall_{\vec{x}}(
    (\RTotal_{\rho_{\nu}}(\vec{Y}, X)x_{\nu})_{\nu < n} \to
    X(\constr_i \vec{x}\,))
    %% K_i \defeq \allnc_{\vec{x}}(
    %% (\RTotal_{\rho_{\nu}}(\vec{Y}, X)x_{\nu})_{\nu < n} \toc
    %% X(\constr_i \vec{x}\,))
  \end{equation*}
  and
  \begin{align*}
    \RTotal_{\alpha_j}(\vec{Y}, X) &\defeq Y_j,
    \\
    \RTotal_{\xi}(\vec{Y}, X) &\defeq X,
    \\
    \RTotal_{\sigma \typeTo \rho}(\vec{Y}, X) &\defeq
    \set {f} {\forall_{\vec{x}}(\RTotal_{\sigma} \vec{x} \to
      \RTotal_{\rho}(\vec{Y}, X)(f \vec{x}\,))}.
  \end{align*}
\end{definition*}

As an example of a finitary algebra with parameters consider lists
$\typeL{\alpha}$.  The clauses for the predicate
$\RTotal_{\typeL{\alpha}}(Y)$ expressing relative totality w.r.t.\ the
predicate variable $Y$ are
\begin{equation*}
  \RTotal_{\typeL{\alpha}}(Y)( \nil),
  \qquad
  \forall_x(Y x \to \forall_l(\RTotal_{\typeL{\alpha}}(Y) l \to
  \RTotal_{\typeL{\alpha}}(Y)(\con {} {x} {l}))),
\end{equation*}
and the least-fixed-point axiom is
\begin{align*}
  \forall_l (\RTotal_{\typeL{\alpha}}(Y) l \to &X(\nil) \to {}
  \\
  &\forall_x(
  Y x \to
  \forall_l(\RTotal_{\typeL{\alpha}}(Y) l \to X l \to X(\con {} {x} {l}))) \to {}
  \\
  &X l^{\typeL{\alpha}} ).
\end{align*}

For important special cases of the parameter predicates $\vec{Y}$ we
introduce a separate notation.  Suppose we want to argue about total
ideals only.  Note that this only makes sense when when no type
variables occur.  However, to allow a certain amount of abstract
reasing (involving type variables to be substituted later by concrete
closed types), we introduce special predicate variables
$\Total_{\alpha}$ which under a substitution $\alpha \mapsto \rho$
with $\rho$ closed turn into the inductively defined predicate
$\Total_{\rho}$.  Using this convention we define totality for an
arbitrary algebra by specializing $Y$ of arity $(\rho)$ to
$\Total_{\rho}$.

\begin{definition*}[Absolute totality $\Total$]
  Let $\iota = \mu_{\xi} (\kappa_0, \dots, \kappa_{k-1}) \in
  \mathrm{Alg}(\vec{\alpha}\,)$ with $\kappa_i =
  (\rho_{\nu}(\vec{\alpha}, \xi))_{\nu < n} \typeTo \xi$.  Then
  $\Total_{\iota} \defeq \mu_X( K_0, \dots, K_{k-1})$, with
  \begin{equation*}
    K_i \defeq \forall_{\vec{x}}(
    (\Total_{\rho_{\nu}}(X)x_{\nu})_{\nu < n} \to
    X(\constr_i \vec{x}\,))
    %% K_i \defeq \allnc_{\vec{x}}(
    %% (\Total_{\rho_{\nu}}(X)x_{\nu})_{\nu < n} \toc
    %% X(\constr_i \vec{x}\,))
  \end{equation*}
  and
  \begin{align*}
    \Total_{\alpha_j}(X) &\defeq \Total_{\alpha_j},
    \\
    \Total_{\xi}(X) &\defeq X,
    \\
    \Total_{\sigma \typeTo \rho}(X) &\defeq
    \set {f} {\forall_{\vec{x}}(\Total_{\sigma} \vec{x} \to
      \Total_{\rho}(X)(f \vec{x}\,))}.
    %% \Total_{\sigma \typeTo \rho}(X) &\defeq
    %% \set {f} {\allnc_{\vec{x}}(\Total_{\sigma} \vec{x} \toc
    %%   \Total_{\rho}(X)(f \vec{x}\,))}.
  \end{align*}
\end{definition*}

Another important special case occurs when we substitute the predicate
variables $Y$ by truth predicates $\set {y} {\truth}$.  The resulting
totality predicate is called \emph{structural
  totality}\index{totality!structural}.  For example, the clauses for
the predicate $\RTotal_{\typeL{\alpha}}(\set {y} {\truth}) \eqdef
\STotal_{\typeL{\alpha}}$ expressing structural totality are
\begin{equation*}
  \STotal_{\typeL{\alpha}}( \nil),
  \qquad
  \forall_x(\set {y} {\truth} x \to \forall_l(\STotal_{\typeL{\alpha}} l \to
  \STotal_{\typeL{\alpha}}(\con {} {x} {l}))),
\end{equation*}
and the least-fixed-point axiom is
\begin{align*}
  \forall_l (\STotal_{\typeL{\alpha}} l \to &X(\nil) \to {}
  \\
  &\forall_x(
  \set {y} {\truth} x \to
  \forall_l(\STotal_{\typeL{\alpha}} l \to X l \to X(\con {} {x} {l}))) \to {}
  \\
  &X l^{\typeL{\alpha}} ).
\end{align*}
Here the premises $\set {y} {\truth} x$ can clearly be omitted, and
the least-fixed-point turns into
\begin{equation*}
  \forall_l (\STotal_{\typeL{\alpha}} l \to X(\nil) \to
  \forall_{x,l}(\STotal_{\typeL{\alpha}} l \to X l \to X(\con {} {x} {l})) \to
  X l^{\typeL{\alpha}} ),
\end{equation*}
called \emph{structural induction}\index{induction!structural} on lists.

Note that we allow usage of totality predicates for previously
introduced algebras $\iota'$.  An example is totality
$\Total_{\typeTree}$ for the algebra $\typeTree$ of finitely branching
trees.  It is defined by the single clause
\begin{equation*}
  \allnc_{\as}(\RTotal_{\typeL{\typeTree}}(\Total_{\typeTree})(\as) \toc
  \Total_{\typeTree}(\branch(\as))).
\end{equation*}

In practice one often wants to reason about total objects only.  To
make this more convenient, Minlog distinguishes between \emph{general}
variables (written $\verb#x^#$) and \emph{total} variables (written
$\verb#x#$, without a hat).  The latter are (implicitly) restricted to
the relative totality predicate of the respective type.  Formally, these
conventions appear as abbreviating axioms
\begin{alignat*}{2}
  &\forall_x P x \to
  \forall_{\hat{x}}(\Total_{\rho} \hat{x} \to P\hat{x})
  &\quad&\texttt{AllTotalElim},\index{AllTotalElim@\texttt{AllTotalElim}}
  \\
  &\forall_{\hat{x}}(\Total_{\rho} \hat{x} \to P\hat{x}) \to \forall_x P x
  &\quad&\texttt{AllTotalIntro}\index{AllTotalIntro@\texttt{AllTotalIntro}}
\end{alignat*}
where $\Total_{\rho}$ is the absolute totality predicate defined
above, which depends on the type $\rho$ of $x$.  For instance,
$\Total_{\typeL{\typeN}}$ is
$\RTotal_{\typeL{\typeN}}(\Total_{\typeN})$, and
$\Total_{\typeL{\alpha}}$ is $\RTotal_{\typeL{\alpha}}(\Total_{\alpha})$.

Parallel to general recursion, one can also consider \emph{general
  induction}%
\index{induction!general}, which allows recurrence to \emph{all}
points \inquotes{strictly below} the present one.  For applications it
is best to make the necessary comparisons w.r.t.\ a \inquotes{measure
  function} $\mu$.  Then it suffices to use an initial segment of the
ordinals instead of a well-founded set.  For simplicity we here
restrict ourselves to the segment given by $\omega$, so the ordering
we refer to is just the standard $<$-relation on the natural numbers.
The principle of general induction then is
\begin{equation}
  \label{E:GInd}
  \forall_{\mu, x \in \GTotal} ( \Prog^{\mu}_x P x \to P x )
\end{equation}
where $\Prog^{\mu}_x P x$ expresses \inquotes{progressiveness}%
\index{progressive} w.r.t.\ the measure function $\mu$ and the
ordering $<$:
\begin{equation*}
  \Prog^{\mu}_x P x \defequiv
  \forall_{x \in \GTotal} (
  \forall_{y \in \GTotal; \mu y < \mu x} P y \to P x ).
\end{equation*}
It is easy to see that in our special case of the $<$-relation we can
\emph{prove} \eqref{E:GInd} from ordinary induction.  However, it will
be convenient to use general induction as a primitive axiom.

\subsection{Coinductive definitions}
\label{SS:CoInd}
We now extend $\TCF$ by allowing coinductive definitions as well as
inductive ones.  For instance, in the algebra $\typeN$ we can
coinductively define \emph{cototality}\index{cototality}%
\index{coinductive definition!of cototality} by the clause
\begin{equation*}
  \CoGTotal_{\typeN} n \to \eqd{n}{0} \lor
  \ex_m(\CoGTotal_{\typeN} m \land \eqd{n}{\suc m}).
\end{equation*}
Its greatest-fixed-point axiom%
\index{greatest-fixed-point axiom} is
\begin{equation*}
  X n \to \forall_n(
  X n \to \eqd{n}{0} \lor \ex_m(
  (\CoGTotal_{\typeN} m \lor X m) \land \eqd{n}{\suc m}) \to
  \CoGTotal_{\typeN} n.
\end{equation*}
It expresses that every \inquotes{competitor} $X$ satisfying the same
clause is a subset of $\CoGTotal_{\typeN}$.  The partial continuous
functionals with $\CoGTotal_{\typeN}$ interpreted as the cototal ideals
for $\typeN$ provide a model of $\TCF$ extended by these axioms.  The
greatest-fixed-point axiom is called the
\emph{coinduction}\index{coinduction} axiom for natural numbers.

Similarly, for the algebra $\typeBin$ of derivations with constructors
$0^{\typeBin}$ and $\constr^{\typeBin \typeTo \typeBin \typeTo
  \typeBin}$ cototality is coinductively defined by the clause
\begin{equation*}
  \CoGTotal_{\typeBin} x \to \eqd {x} {0} \lor
  \ex_y(\CoGTotal_{\typeBin} y \land \ex_z(\CoGTotal_{\typeBin} z
  \land \eqd {x} {\constr y z})).
\end{equation*}
Its greatest-fixed-point axiom is
\begin{align*}
  X x \to \forall_x(
  X x \to \eqd {x} {0} \lor {} &\ex_y((\CoGTotal_{\typeBin} x \lor X y) \land {}
  \\
  &\ex_z((\CoGTotal_{\typeBin} x \lor X z) \land
  \eqd {x} {\constr y z}))) \to \CoGTotal_{\typeBin} x.
\end {align*}
The partial continuous functionals with $\CoGTotal_{\typeBin}$
interpreted as the cototal ideals for $\typeBin$ (i.e., the finite or
infinite locally correct derivations) provide a model.

For the algebra $\typeIntv$ of standard rational intervals cototality
is defined by
\begin{align*}
  \CoGTotal_{\typeIntv} x \to \eqd {x} {\D{I}} \lor {}
  &\ex_y(\CoGTotal_{\typeIntv} y \land \eqd {x} {\constr_{-1} y}) \lor {}
  \\
  &\ex_y(\CoGTotal_{\typeIntv} y \land \eqd {x} {\constr_0 y}) \lor {}
  \\
  &\ex_y(\CoGTotal_{\typeIntv} y \land \eqd {x} {\constr_1 y}).
\end{align*}
A model is provided by the set of all finite or infinite streams
of signed digits from $\{ -1, 0, 1 \}$, i.e., the well-known (non-unique)
stream representation%
\index{stream representation} of real numbers.

Generally, every inductive predicate $I$ gives rise to a coinductive
predicate, its \emph{dual}\index{dual} or \emph{companion}%
\index{companion} $\coI$.  Let $I$ be inductively defined by the
clauses
\begin{equation*}
  \forall_{\vec{x}_i} (( A_{i \nu}(I))_{\nu < n_i} \to I \vec{t}_i)
  \quad \hbox{($i<k$)}.
\end{equation*}
The conjunction of these $k$ clauses is equivalent to
\begin{equation*}
  \forall_{\vec{x}}(
  \biglor_{i<k}
  \ex_{\vec{x}_i}((\bigland_{\nu < n_i} A_{i \nu}(I) \land
   \eqd{\vec{x}} {\vec{t}_i}) \to I \vec{x}\,).
\end{equation*}
Now the dual $\coI$ of $I$ is coinductively defined by its closure axiom
$\coI^{-}$:
\begin{equation*}
  \forall_{\vec{x}}(\coI \vec{x} \to \biglor_{i<k}
  \ex_{\vec{x}_i}(\bigland_{\nu < n_i} A_{i \nu}(\coI) \land
   \eqd{\vec{x}} {\vec{t}_i})).
\end{equation*}
Its greatest-fixed-point axiom $\coI^{+}$ is
\begin{equation*}
  \forall_{\vec{x}}(X \vec{x} \to
  \forall_{\vec{x}}( X \vec{x} \to \biglor_{i<k}
  \ex_{\vec{x}_i}(\bigland_{\nu < n_i} A_{i \nu}(\coI \cup X) \land
   \eqd{\vec{x}} {\vec{t}_i})) \to \coI \vec{x}\,)
\end{equation*}
where $\coI \cup X$ abbreviates $\set {\vec{x}} {\coI \vec{x} \lor X
  \vec{x}}$.

Notice that the proof of the Ex-Falso-Quodlibet theorem above can
easily be extended by a case $\coI \vec{r}$: use the
greatest-fixed-point axiom for $\coI$ with $X \vec{x} \defeq
\falsityF$.  Since we have a nullary clause $\forall_{\vec{x}}(
(A_{\nu})_{\nu<n} \to I \vec{s}\,)$ with no occurrence of $I$ in the
$A_{\nu}$, it suffices to prove $\falsityF \to \ex_{\vec{y}_i}
\bigland_{\nu<n} \vec{A}_{\nu}$.  But this follows from the induction
hypothesis.

We extend this to the simultaneous case.  For $\vec{I} \defeq
\mu_{\vec{X}}(K_0, \dots, K_{k-1})$ let $k = \sum_{j<N} k_j$ with $k_j
\ge 1$ and $m_j \defeq \sum_{l<j} k_j$, hence m$_j + k_j = m_{j+1}$.
Recall the clauses or introduction axioms $I_i^{+}$:
\begin{equation*}
  \forall_{\vec{x}_i}((A_{i \nu}( \vec{I}\,) )_{\nu<n_i}
  \to I_j \vec{t}_i)
  \quad \hbox{($m_j \le i < m_{j+1}$)}.
\end{equation*}
The conjunction of these $k_j$ clauses is equivalent to
\begin{equation*}
  \forall_{\vec{x}}(
  \biglor_{m_j \le i < m_{j+1}}
  \ex_{\vec{x}_i}(\bigland_{\nu < n_i} A_{i \nu}(\vec{I}\,) \land
   \eqd{\vec{x}} {\vec{t}_i}) \to I_j \vec{x}\,).
\end{equation*}
The dual $\coI_j$ of $I_j$ is coinductively defined by its closure axiom
$\coI_j^{-}$:
\begin{equation*}
  \forall_{\vec{x}}(\coI_j \vec{x} \to \biglor_{m_j \le i < m_{j+1}}
  \ex_{\vec{x}_i}(\bigland_{\nu < n_i} A_{i \nu}(\vec{\coI}) \land
   \eqd{\vec{x}} {\vec{t}_i})).
\end{equation*}
Its greatest-fixed-point axiom $\coI_j^{+}$ is
\begin{align*}
  \forall_{\vec{x}}(X_j \vec{x} \to {}
  &(\forall_{\vec{x}}( X_j \vec{x} \to \biglor_{m_j \le i < m_{j+1}}
  \ex_{\vec{x}_i}(\bigland_{\nu < n_i} A_{i \nu}(\vec{\coI} \lor \vec{X}) \land
   \eqd{\vec{x}} {\vec{t}_i})))_{j<N}
  \\
  &\to \coI_j \vec{x}\,).
\end{align*}

The most important coinductively defined predicates for us will be
those of cototality\index{cototality}; we have seen some examples
above.  Generally, for a finitary algebra $\iota$ cototality is
coinductively defined by
\begin{equation*}
  \CoGTotal_{\iota} x \to \biglor_{i<k} \ex_{\vec{y}_i}(
  \CoGTotal_{\iota} \vec{y}_i \land \eqd {x} {\constr_i \vec{y}_i}).
\end{equation*}

\subsection{Implementation}
We maintain an association list \texttt{IDS}\index{IDS@\texttt{IDS}}
(a global variable), which assigns all relevant information to the
name of an inductively defined predicate constant.  This information
consists of
\begin{enumeratei}
\item the names of idpredconsts simultaneously defined with the
  present one,
\item an algebra name (for computational content, in case there is one),
\item the clauses with their names.
\end{enumeratei}
These data can be read off by
\begin{align*}
  &\texttt{(idpredconst-name-to-simidpc-names \textsl{name})}%
  \index{idpredconst-name-to-simidpc-names@\texttt{idpredconst-name-to-simidpc-names}},
  \\
  &\texttt{(idpredconst-name-to-alg-name \textsl{name})}%
  \index{idpredconst-name-to-alg-name@\texttt{idpredconst-name-to-alg-name}},
  \\
  &\texttt{(idpredconst-name-to-clauses \textsl{name})}%
  \index{idpredconst-name-to-clauses@\texttt{idpredconst-name-to-clauses}}.
\end{align*}

Every inductively defined predicate constant has the internal
representation
\begin{equation*}
  \texttt{(idpredconst \textsl{name} \textsl{types} \textsl{cterms})}.
\end{equation*}
\textsl{types} and \textsl{cterms} are to be substituted for the type
and predicate variables in the clauses.  To create this substitution
use \texttt{idpredconst-to-tpsubst}%
\index{idpredconst-to-tpsubst@\texttt{idpredconst-to-tpsubst}}.

We provide a constructor, accessors and a test:
\begin{alignat*}{2}
  &\texttt{(make-idpredconst \textsl{name} \textsl{types} \textsl{cterms})}
  &\quad& \text{constructor},
  \\
  &\texttt{(idpredconst-to-name \textsl{idpredconst})} && \text{accessor},
  \index{idpredconst-to-name@\texttt{idpredconst-to-name}}
  \\
  &\texttt{(idpredconst-to-types \textsl{idpredconst})} && \text{accessor},
  \index{idpredconst-to-types@\texttt{idpredconst-to-types}}
  \\
  &\texttt{(idpredconst-to-cterms \textsl{idpredconst})} && \text{accessor},
  \index{idpredconst-to-cterms@\texttt{idpredconst-to-cterms}}
  \\
  &\texttt{(idpredconst?\ \textsl{x})}.%
  \index{idpredconst?@\texttt{idpredconst?}}
\end{alignat*}

To introduce inductively defined predicates we use \texttt{add-ids}%
\index{add-ids@\texttt{add-ids}}, for example
\begin{verbatim}
(add-ids (list (list "Even" (make-arity (py "nat")) "nat"))
         '("Even 0" "InitEven")
         '("allnc n^(Even n^ -> Even(n^ +2))" "GenEven"))
\end{verbatim}
This introduces the inductively defined predicate constant
\texttt{Even}, by the clauses given.  The presence of an algebra name
after the arity (here \texttt{nat}) indicates that this inductively
defined predicate constant has computational content.  If this is an
already known algebra, the clauses with this constant in the
conclusion must have the same types for their extracted terms as the
constructors of the algebra.  If no such algebra is known, we can also
write \texttt{algEven} (instead of \texttt{nat}) to create one.  The
clauses can be given names (here \texttt{InitEven}, \texttt{GenEven}),
and are saved as theorems under these names.

For the inductive definition of the reflexive transitive closure of a
binary relation $\prec$ we have two variants of this definition,
depending on whether possible computational content of the relation
$\prec$ is taken into account or not.  If not we take
%% In Minlog we have two variants of this definition, depending on whether
%% the relation $\prec$ comes with computational content or not.  In case
%% it has no computational content we take
\begin{verbatim}
(add-ids
 (list (list "RTClNc" (make-arity (py "alpha") (py "alpha"))
       "nat"))
 '("allnc x^(RTClNc x^ x^)" "InitRTClNc")
 '("allnc x^,y^,z^(R y^ z^ --> RTClNc x^ y^ -> RTClNc x^ z^)"
   "GenRTClNc"))
\end{verbatim}
and otherwise
\begin{verbatim}
(add-ids
 (list (list "TClCr" (make-arity (py "alpha") (py "alpha"))
       "list"))
 '("allnc x^(TClCr x^ x^)" "InitTClCr")
 '("allnc x^,y^,z^(R y^ z^ -> TClCr x^ y^ -> TClCr x^ z^)"
   "GenTClCr"))
\end{verbatim}
The difference between the \inquotes{non-computational implication}
(displayed $\verb#-->#$) and the computational one (displayed
$\verb#->#$) is explained in section~\ref{SS:Deco}.

For an inductive definition of the accessible part of a binary
relation $\pred$ we consider the case that the relation $\prec$
is decidable, i.e., given by a boolean-valued binary function
$\verb#r^#$
\begin{verbatim}
(add-ids
 (list (list "Acc" (make-arity (py "alpha=>alpha=>boole")
                               (py "alpha"))
             "algAcc"))
 '("allnc r^,x^(F -> Acc r^ x^)" "EfqAcc")
 '("allnc r^,x^(all y^(r^ y^ x^ -> Acc r^ y^) -> Acc r^ x^)"
	   "GenAccSup"))
\end{verbatim}

We may also have the string \texttt{identity}%
\index{identity@\texttt{identity}} in the field where an algebra name
is expected.  This is allowed if and only if there is exactly one
clause where the type of its extracted term is essentially the
identity.  Then no new algebra is created.  Later $\lambda_x x$ will
be taken as realizer for the (single) clause, and $\lambda_{x,f}(f x)$
as realizer for the elimination axiom.  Examples are computational
variants \texttt{ExL}\index{ExL}, \texttt{ExR}\index{ExR} and
\texttt{AndR}\index{AndR} of the (inductively defined) existential
quantifier and conjunction.

We also allow non-computational (n.c.)\ inductively defined
predicates.  Then no algebra name is provided.  Important special
cases are:
\begin{enumeratei}
\item For every \texttt{I} its witnessing predicate \texttt{IMR}.
  It is special in the sense that \verb#(IMR t ss)# just states the
  fact that \verb#t# is a realizer for \verb#I ss#.
\item By providing just one nullary clause with $\allnc$, $\tonc$ only
  and no algebra name one can introduce a \inquotes{uniform one clause
    defined}%
  \index{uniform one clause defined} idpredconst which is n.c.
  Examples are Leibniz equality%
  \index{Leibniz equality} \texttt{EqD}\index{EqD}, and uniform
  variants \texttt{ExU}\index{ExU} and \texttt{AndU}\index{AndU} of
  the existential quantifier and conjunction.
\end{enumeratei}
In all other cases the elimination scheme must be restricted to n.c.\
formulas.  Also, all (n.c.)\ clauses must be invariant.  This ensures
that the soundness theorem holds: every introduction and elimination
axiom is invariant, i.e., $\nullterm \mr A$ is the same as $A$.

It is also possible to introduce simultaneously
inductively defined predicates:
\begin{verbatim}
(add-ids (list (list "Ev" (make-arity (py "nat")) "algEv")
               (list "Od" (make-arity (py "nat")) "algOd"))
	 '("Ev 0" "InitEv")
	 '("allnc n^(Od n^ -> Ev(n^ +1))" "GenEv")
	 '("allnc n^(Ev n^ -> Od(n^ +1))" "GenOd"))
\end{verbatim}
However, for simplicity we have restricted the discussion above to the
non-simultaneous case.

An important example for an inductively defined predicate is the
totality predicate for an algebra, for instance \texttt{TotalNat}%
\index{TotalNat@\texttt{TotalNat}} for the algebra \texttt{nat}.  It
can be created by calling \texttt{(add-totality \textsl{alg-name})}%
\index{add-totality@\texttt{add-totality}}.  The same can be done for
relative totality by calling \texttt{(add-rtotality
  \textsl{alg-name})}\index{add-rtotality@\texttt{add-rtotality}},
yielding for instance \texttt{RTotalList}%
\index{RTotalList@\texttt{RTotalList}} for the algebra \texttt{list}.

To remove a name for an inductively defined predicate constant
(and also the ones defined simultaneously with it), we
use
\begin{align*}
  &\texttt{(remove-idpc-name \textsl{name1} \dots)}.%
  \index{remove-idpc-name@\texttt{remove-idpc-name}}
\end{align*}

Coinductively defined predicates are in many aspects similar to
inductively defined ones, and it seems easiest to use most of the
functions with \verb#idpredconst# in their name for both.  We even
insert the names of the coinductively defined predicates in
\texttt{IDS}\index{IDS}; however, there is also a global variable
\texttt{COIDS}\index{COIDS} (with the same format) for the
coinductively defined ones only.  \texttt{add-co}%
\index{add-co@\texttt{add-co}} adds dualized \inquotes{companions} for
inductively defined predicate constants to \texttt{COIDS}.  Examples
are cototality predicates for the corresponding total ones, but also
for instance \texttt{CoEv}, \texttt{CoOd} for \texttt{Ev},
\texttt{Od}.  The optional algebra names are the same as for the
corresponding inductively defined predicate constants.  Realizers for
cototality predicates are the cototal ideals of the algebra.

The Minlog command for coinduction is $\texttt{coind}$%
\index{coind@\texttt{coind}} (cf.\ section~\ref{SS:coind}).

\section{Terms and objects}
\label{S:Terms}

\subsection{Constructors and accessors}
Terms are built from (typed) variables and constants by abstraction,
application, pairing, formation of left and right components (i.e.,
projections) and the \texttt{if}-construct.

The \texttt{if}-construct\index{if-construct@\texttt{if}-construct}
distinguishes cases according to the outer constructor form; the
simplest example (for the type \texttt{boole}) is \emph{if-then-else}.
Here we do not want to evaluate all arguments right away, but rather
evaluate the test argument first and depending on the result evaluate
at most one of the other arguments.  This phenomenon is well known in
functional languages; e.g., in Scheme the \texttt{if}-construct is
called a \emph{special form} as opposed to an operator.  In accordance
with this terminology we also call our \texttt{if}-construct a special
form\index{special form}.  It will be given a special treatment in
\texttt{nbe-term-to-object}.

Usually it will be the case that every closed term of an algebra
ground type reduces via the computation rules to a constructor term,
i.e., a closed term built from constructors only.  However, we do not
require this.

%% \subsection*{Interface}
We have constructors, accessors and tests for variables
\begin{alignat*}{2}
  &\texttt{(make-term-in-var-form var)}
  \index{make-term-in-var-form@\texttt{make-term-in-var-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-var-form-to-var \textsl{term})}
  \index{term-in-var-form-to-var@\texttt{term-in-var-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(term-in-var-form?\ \textsl{term})}
  \index{term-in-var-form?@\texttt{term-in-var-form?}}
  && \text{test,}
\end{alignat*}
for constants
\begin{alignat*}{2}
  &\texttt{(make-term-in-const-form \textsl{const})}
  \index{make-term-in-const-form@\texttt{make-term-in-const-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-const-form-to-const \textsl{term})},%%
  \index{term-in-const-form-to-const@\texttt{term-in-const-form-to-const}}
  && \text{accessor},
  \\
  &\texttt{(term-in-const-form?\ \textsl{term})}
  \index{term-in-const-form?@\texttt{term-in-const-form?}}
  && \text{test,}
\end{alignat*}
for abstractions
\begin{alignat*}{2}
  &\texttt{(make-term-in-abst-form \textsl{var} \textsl{term})}
  \index{make-term-in-abst-form@\texttt{make-term-in-abst-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-abst-form-to-var \textsl{term})}
  \index{term-in-abst-form-to-var@\texttt{term-in-abst-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(term-in-abst-form-to-kernel \textsl{term})}
  \index{term-in-abst-form-to-kernel@\texttt{term-in-abst-form-to-kernel}}
  && \text{accessor},
  \\
  &\texttt{(term-in-abst-form?\ \textsl{term})}
  \index{term-in-abst-form?@\texttt{term-in-abst-form?}}
  && \text{test,}
\end{alignat*}
for applications
\begin{alignat*}{2}
  &\texttt{(make-term-in-app-form \textsl{term1} \textsl{term2})},%%
  \index{make-term-in-app-form@\texttt{make-term-in-app-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-app-form-to-op \textsl{term})}
  \index{term-in-app-form-to-op@\texttt{term-in-app-form-to-op}}
  && \text{accessor},
  \\
  &\texttt{(term-in-app-form-to-arg \textsl{term})}
  \index{term-in-app-form-to-arg@\texttt{term-in-app-form-to-arg}}
  && \text{accessor},
  \\
  &\texttt{(term-in-app-form?\ \textsl{term})}
  \index{term-in-app-form?@\texttt{term-in-app-form?}}
  && \text{test,}
\end{alignat*}
for pairs
\begin{alignat*}{2}
  &\texttt{(make-term-in-pair-form \textsl{term1} \textsl{term2})}
  \index{make-term-in-pair-form@\texttt{make-term-in-pair-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-pair-form-to-left \textsl{term})}
  \index{term-in-pair-form-to-left@\texttt{term-in-pair-form-to-left}}
  && \text{accessor},
  \\
  &\texttt{(term-in-pair-form-to-right \textsl{term})}
  \index{term-in-pair-form-to-right@\texttt{term-in-pair-form-to-right}}
  && \text{accessor},
  \\
  &\texttt{(term-in-pair-form?\ \textsl{term})}
  \index{term-in-pair-form?@\texttt{term-in-pair-form?}}
  && \text{test,}
\end{alignat*}
for the left and right component of a pair
\begin{alignat*}{2}
  &\texttt{(make-term-in-lcomp-form \textsl{term})}
  \index{make-term-in-lcomp-form@\texttt{make-term-in-lcomp-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(make-term-in-rcomp-form \textsl{term})}
  \index{make-term-in-rcomp-form@\texttt{make-term-in-rcomp-form}}
  && \text{constructor},
  \\
  &\texttt{(term-in-lcomp-form-to-kernel \textsl{term})}
  \index{term-in-lcomp-form-to-kernel@\texttt{term-in-lcomp-form-to-kernel}}
  && \text{accessor},
  \\
  &\texttt{(term-in-rcomp-form-to-kernel \textsl{term})}
  \index{term-in-rcomp-form-to-kernel@\texttt{term-in-rcomp-form-to-kernel}}
  && \text{accessor},
  \\
  &\texttt{(term-in-lcomp-form?\ \textsl{term})}
  \index{term-in-lcomp-form?@\texttt{term-in-lcomp-form?}}
  && \text{test},
  \\
  &\texttt{(term-in-rcomp-form?\ \textsl{term})}
  \index{term-in-rcomp-form?@\texttt{term-in-rcomp-form?}}
  && \text{test},
\end{alignat*}
and for \texttt{if}-constructs
\begin{alignat*}{2}
  &\texttt{(make-term-in-if-form \textsl{test} \textsl{alts} .\ \textsl{rest})}
  \index{make-term-in-if-form@\texttt{make-term-in-if-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-if-form-to-test \textsl{term})}
  \index{term-in-if-form-to-test@\texttt{term-in-if-form-to-test}}
  && \text{accessor},
  \\
  &\texttt{(term-in-if-form-to-alts \textsl{term})}
  \index{term-in-if-form-to-alts@\texttt{term-in-if-form-to-alts}}
  && \text{accessor},
  \\
  &\texttt{(term-in-if-form-to-rest \textsl{term})}
  \index{term-in-if-form-to-rest@\texttt{term-in-if-form-to-rest}}
  && \text{accessor},
  \\
  &\texttt{(term-in-if-form?\ \textsl{term})}
  \index{term-in-if-form?@\texttt{term-in-if-form?}}
  && \text{test},
\end{alignat*}
where in \texttt{make-term-in-if-form}, \textsl{rest} is either empty
or an all-formula.

It is convenient to have more general application constructors and
accessors available, where application takes arbitrary many arguments
and works for ordinary application as well as for component formation.
\begin{alignat*}{2}
  &\texttt{(mk-term-in-app-form \textsl{term} \textsl{term1} \dots)}
  \index{mk-term-in-app-form@\texttt{mk-term-in-app-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(term-in-app-form-to-final-op \textsl{term})},%%
  \index{term-in-app-form-to-final-op@\texttt{term-in-app-form-to-final-op}}
  && \text{accessor},
  \\
  &\texttt{(term-in-app-form-to-args \textsl{term})},%%
  \index{term-in-app-form-to-args@\texttt{term-in-app-form-to-args}}
  && \text{accessor}.
\end{alignat*}
For abstraction it is convenient to have a more general constructor
taking arbitrary many variables to be abstracted one after the other
\begin{alignat*}{2}
  &\texttt{(mk-term-in-abst-form \textsl{var1} \dots\ \textsl{term})}.
  \index{mk-term-in-abst-form@\texttt{mk-term-in-abst-form}}
\end{alignat*}
We also allow vector notation for recursion (cf.\ Joachimski and
Matthes \cite{JoachimskiMatthes03}).  Moreover we provide
\begin{alignat*}{2}
  &\texttt{(term=?\ \textsl{term1} \textsl{term2})},%
  \index{term=?@\texttt{term=?}}
  \\
  &\texttt{(terms=?\ \textsl{terms1} \textsl{terms2})},%
  \index{terms=?@\texttt{terms=?}}
  \\
  &\texttt{(term-to-type \textsl{term})},%
  \index{term-to-type@\texttt{term-to-type}}
  \\
  &\texttt{(term-to-free \textsl{term})},%
  \index{term-to-free@\texttt{term-to-free}}
  \\
  &\texttt{(term-to-bound \textsl{term})},%
  \index{term-to-bound@\texttt{term-to-bound}}
  \\
  &\texttt{(term-to-tvars \textsl{term})},%
  \index{term-to-tvars@\texttt{term-to-tvars}}
  \\
  &\texttt{(term-to-t-deg \textsl{term})},%
  \index{term-to-t-deg@\texttt{term-to-t-deg}}
  \\
  &\texttt{(synt-total?\ \textsl{term})}.
  \index{synt-total?@\texttt{synt-total?}}
\end{alignat*}
For displaying terms we have
\begin{equation*}
  \texttt{(term-to-string \textsl{term})},
  \index{term-to-string@\texttt{term-to-string}}
\end{equation*}
which is defined by
\begin{equation*}
  \texttt{(token-tree-to-string (term-to-token-tree \textsl{term}))}.%
  \index{term-to-token-tree@\texttt{term-to-token-tree}}
  \index{token-tree-to-string@\texttt{token-tree-to-string}}
\end{equation*}
For better line breaks in the display one can use
\begin{equation*}
  \texttt{(pp \textsl{term})}\index{pp@\texttt{pp}},
\end{equation*}
which is defined by
\begin{equation*}
  \texttt{(token-tree-to-pp-tree (term-to-token-tree \textsl{term}))}.
  \index{term-to-token-tree@\texttt{term-to-token-tree}}
  \index{token-tree-to-pp-tree@\texttt{token-tree-to-pp-tree}}
\end{equation*}
Sometimes for readability it is helpful to have special support
for definitions by cases.  Then it is advisable to use
\begin{equation*}
  \texttt{(pretty-print-with-case-display \textsl{term})}%
  \index{pretty-print-with-case-display@\texttt{pretty-print-with-case-display}},
\end{equation*}
abbreviated \texttt{(ppc \textsl{term})}\index{ppc@\texttt{ppc}}.
Moreover we provide
\begin{alignat*}{2}
  &\texttt{(term-to-scheme-expr \textsl{term})}
  \index{term-to-scheme-expr@\texttt{term-to-scheme-expr}},
  \\
  &\texttt{(term-to-haskell-expr \textsl{term})}
  \index{term-to-haskell-expr@\texttt{term-to-haskell-expr}},
\end{alignat*}
\texttt{term-to-expr}\index{term-to-expr@\texttt{term-to-expr}} is
used as abbreviation for \texttt{term-to-scheme-expr}.  These
functions aim at producing a readable Scheme / Haskell expression that
can be evaluated.  For instance \texttt{term-to-expr} transforms an
application of a program constant \verb#c# to \verb#args#, where
\verb#c# has a corresponding built-in Scheme operator written in
uncurried form with length of \verb#args# many arguments, into the
corresponding Scheme expression.  If however \verb#c# is applied to
fewer arguments, then the default translation of \verb#c# is used.
Equality with name \inquotes{=} requires a special treatment: if there
are exactly two arguments, it is transformed into an =-expression if
the type of = refers to a number type (\texttt{nat}, \texttt{pos},
\texttt{int} or \texttt{rat}), and to an \texttt{equal?}-expression
otherwise.  If it is applied to fewer arguments, then one needs
\texttt{FinAlg=} as a special default name, since the internal name =
cannot be used.

\subsection{Normalization}
We need an operation which transforms a term into its normal form
w.r.t.\ the given computation and rewrite rules.  Here we base our
treatment on \emph{normalization by evaluation} introduced in
\cite{BergerSchwichtenberg91a}, and extended to arbitrary computation
and rewrite rules in \cite{BergerEberlSchwichtenberg03}.

For normalization by evaluation we need semantical \emph{objects}.
For an arbitrary ground type every term family of that type is an
object.  For an algebra ground type, in addition the constructors have
semantical counterparts.  The freeness of the constructors is
expressed by requiring that their ranges are disjoint and that they
are injective.  Moreover, we view the free algebra as a domain and
require that its bottom element is not in the range of the
constructors.  Hence the constructors are total and non-strict.  Then
by applying \texttt{nbe-reflect} followed by \texttt{nbe-reify} we can
normalize every term, where normalization refers to the computation as
well as the rewrite rules.

%% \subsection*{Interface}
An object consists of a semantical value and a type.
\begin{alignat*}{2}
  &\texttt{(nbe-make-object \textsl{type} \textsl{value})}%%
  \index{nbe-make-object@\texttt{nbe-make-object}}
  &\quad& \text{constructor},
  \\
  &\texttt{(nbe-object-to-type \textsl{object})}%
  \index{nbe-object-to-type@\texttt{nbe-object-to-type}}
  && \text{accessor},
  \\
  &\texttt{(nbe-object-to-value \textsl{object})}%%
  \index{nbe-object-to-value@\texttt{nbe-object-to-value}}
  && \text{accessor},
  \\
  &\texttt{(nbe-object?\ \textsl{x})}
  \index{nbe-object?@\texttt{nbe-object?}}
  && \text{test}.
\end{alignat*}
To work with objects, we need
\begin{alignat*}{2}
  &\texttt{(nbe-object-apply \textsl{function-obj} \textsl{arg-obj})}.%
  \index{nbe-object-apply@\texttt{nbe-object-apply}}
\end{alignat*}
Again it is convenient to have a more general application operation
available, which takes arbitrary many arguments and works for ordinary
application as well as for component formation. We also need an
operation composing two unary function objects.
\begin{alignat*}{2}
  &\texttt{(nbe-object-app \textsl{function-obj} \textsl{arg-obj1} \dots)},%
  \index{nbe-object-app@\texttt{nbe-object-app}}
  \\
  &\texttt{(nbe-object-compose \textsl{function-obj1} \textsl{function-obj2})}.%
  \index{nbe-object-compose@\texttt{nbe-object-compose}}
\end{alignat*}
For ground type values we need constructors, accessors and tests.  To
make constructors \inquotes{self-evaluating}, a constructor value has
the form
\begin{equation*}
  \hbox{\texttt{(constr-value \textsl{name} \textsl{objs}
      \textsl{delayed-constr})},}
\end{equation*}
where \textsl{delayed-constr} is a procedure of zero arguments which
evaluates to this very same constructor.  This is necessary to avoid
having a cycle (for nullary constructors, and only for those).
\begin{alignat*}{2}
  &\texttt{(nbe-make-constr-value \textsl{name} \textsl{objs})}%%
  \index{nbe-make-constr-value@\texttt{nbe-make-constr-value}}
  &\quad& \text{constructor},
  \\
  &\texttt{(nbe-constr-value-to-name \textsl{value})}
  \index{nbe-constr-value-to-name@\texttt{nbe-constr-value-to-name}}
  && \text{accessor}, \\
  &\texttt{(nbe-constr-value-to-args \textsl{value})}
  && \text{accessor},
  \\
  &\texttt{(nbe-constr-value-to-constr \textsl{value})}
  \index{nbe-constr-value-to-constr@\texttt{nbe-constr-value-to-constr}}
  && \text{accessor},
  \\
  &\texttt{(nbe-constr-value?\ \textsl{value})}
  \index{nbe-constr-value?@\texttt{nbe-constr-value?}}
  && \text{test},
  \\
  &\texttt{(nbe-fam-value?\ \textsl{value})}
  \index{nbe-fam-value?@\texttt{nbe-fam-value?}}
  && \text{test}.
\end{alignat*}
The essential function which \inquotes{animates}\index{animation} the
program constants according to the given computation and rewrite rules
is
\begin{align*}
  &\texttt{(nbe-pconst-and-tsubst-and-rules-to-object}
  \index{nbe-pconst-and-tsubst-and-rules-to-object@\texttt{nbe-pconst-{\dots}-to-object}}
  \\
  &\qquad \texttt{\textsl{pconst}\ \textsl{tsubst}\ \textsl{comprules}\
    \textsl{rewrules})}.
\end{align*}
Using it we can the define an \emph{evaluation} function, which
assigns to a term and an environment a semantical object:
\begin{alignat*}{2}
  &\texttt{(nbe-term-to-object \textsl{term} \textsl{bindings})}
  \index{nbe-term-to-object@\texttt{nbe-term-to-object}}
  &\quad& \text{evaluation.}
\end{alignat*}
Here \textsl{bindings} is an association list assigning objects of the
same type to variables.  In case a variable is not assigned anything
in \textsl{bindings}, by default we assign the constant term family of
this variable, which always is an object of the correct type.

The interpretation of the program constants requires some auxiliary
functions (cf.\ \cite{BergerEberlSchwichtenberg03}):
\begin{alignat*}{2}
  &\texttt{(nbe-constructor-pattern?\ \textsl{term})}
  \index{nbe-constructor-pattern?@\texttt{nbe-constructor-pattern?}}
  &\quad& \text{test},
  \\
  &\texttt{(nbe-inst?\ \textsl{constr-pattern} \textsl{obj})}
  \index{nbe-inst?@\texttt{nbe-inst?}}
  && \text{test},
  \\
  &\texttt{(nbe-genargs \textsl{constr-pattern} \textsl{obj})}
  \index{nbe-genargs@\texttt{nbe-genargs}}
  && \text{generalized arguments},
  \\
  &\texttt{(nbe-extract \textsl{termfam})}
  \index{nbe-extract@\texttt{nbe-extract}}
  && \text{extracts a term from a family},
  \\
  &\texttt{(nbe-match \textsl{pattern} \textsl{term})}.%
  \index{nbe-match@\texttt{nbe-match}}
\end{alignat*}
Then we can define
\begin{alignat*}{2}
  &\texttt{(nbe-reify \textsl{object})}
  \index{nbe-reify@\texttt{nbe-reify}}
  &\quad& \text{reification},
  \\
  &\texttt{(nbe-reflect \textsl{term})}
  \index{nbe-reflect@\texttt{nbe-reflect}}
  && \text{reflection}
\end{alignat*}
and by means of these
\begin{equation*}
  \texttt{(nbe-normalize-term-without-eta \textsl{term})}.
  \index{nbe-normalize-term-without-eta@\texttt{nbe-normalize-term-without-eta}}
\end{equation*}
The result is a term in long normal form; to transform it into
$\eta$-normal form one can use
\begin{equation*}
  \texttt{(term-to-eta-nf \textsl{term})}.
  \index{term-to-eta-nf@\texttt{term-to-eta-nf}}
\end{equation*}

We now aim at a full normalization of terms, including permutative
conversions.  Here the \texttt{if}-form needs a special treatment.  In
a preprocessing step, we $\eta$-expand the alternatives of
\texttt{if}-terms, using
\begin{equation*}
  \texttt{(term-to-term-with-eta-expanded-if-terms \textsl{term})}.
  \index{term-to-term-with-eta-expanded-if-terms@\texttt{term-to-term-with-eta-expanded-if-terms}}
\end{equation*}
The result contains \texttt{if}-terms with ground type alternatives
only.  Then permutative conversions for \texttt{if}-terms can be
performed.  Notice that this is not possible for recursion terms,
but is is if we have recursion terms with no recursive calls, i.e.,
essentially cases terms: they can be replaced by \texttt{if}-terms.
The relevant function is
\begin{equation*}
  \texttt{(normalize-term-pi-with-rec-to-if \textsl{term})}.
  \index{normalize-term-pi-with-rec-to-if@\texttt{normalize-term-pi-with-rec-to-if}}
\end{equation*}
Using these (and some other) auxiliary functions we finally define
\begin{equation*}
  \texttt{(nbe-normalize-term \textsl{term})},
  \index{nbe-normalize-term@\texttt{nbe-normalize-term}}
\end{equation*}
abbreviated \texttt{nt}\index{nt@\texttt{nt}}.

We also provide \texttt{term-to-term-without-predecided-ifs}%
\index{term-to-term-without-predecided-ifs@\texttt{term-to-term-without-predecided-ifs}}.
It simplifies all \texttt{if}-terms whose branch is known because we
are in a branch of an outer \texttt{if}-term with the same test term.

As an alternative to normalization by evaluation, we can also
normalize \inquotes{by hand}.  This is done via
\begin{align*}
  &\texttt{(term-to-one-step-beta-reduct \textsl{term})},%
  \index{term-to-one-step-beta-reduct@\texttt{term-to-one-step-beta-reduct}}
  \\
  &\texttt{(term-in-beta-normal-form?\ \textsl{term})},%
  \index{term-in-beta-normal-form?@\texttt{term-in-beta-normal-form?}}
  \\
  &\texttt{(term-to-beta-nf \textsl{term})},%
  \index{term-to-beta-nf@\texttt{term-to-beta-nf}}
  \\
  &\texttt{(term-to-beta-eta-nf \textsl{term})},%
  \index{term-to-beta-eta-nf@\texttt{term-to-beta-eta-nf}}
  \\
  &\texttt{(term-to-beta-pi-eta-nf \textsl{term})}
  \index{term-to-beta-pi-eta-nf@\texttt{term-to-beta-pi-eta-nf}}
\end{align*}
abbreviated \texttt{bpe-nt}\index{bpe-nt@\texttt{bpe-nt}}.

We also provide some auxiliary functions to analyze terms.  In
\begin{equation*}
  \texttt{(term-in-rec-normal-form?\ \textsl{term})}%
  \index{term-in-rec-normal-form?@\texttt{(term-in-rec-normal-form?}}
\end{equation*}
we assume that \textsl{term} is not one of those appearing during
proof normalization.  This means that all recursion constants are
without repro data.
\begin{equation*}
  \texttt{(term-to-consts \textsl{term})}%
  \index{term-to-consts@\texttt{(term-to-consts}}
\end{equation*}
returns a list of all constants in a term (without repetitions).  For
tests it can be useful to have a level-wise decomposition of terms
into subterms: one level transforms a term $N \lambda_{\vec{u}}(v M_1
\dots M_n)$ into the list $N, v, M_1, \dots, M_n$.  The general
function is
\begin{equation*}
  \texttt{(term-to-subterms \textsl{term} \textsl{opt-level})}.
  \index{term-to-subterms@\texttt{(term-to-subterms}}
\end{equation*}

Example (let introduction%
\index{let introduction}).  In practice it often happens that an
extracted term contains multiple occurrences of the same subterm.  One
can (and should) avoid this by using the \inquotes{identity theorem}%
\index{identity theorem} \texttt{Id}\index{Id@\texttt{Id}} (proving $P
\to P$ with a predicate variable $P$) at appropriate places in the
underlying proof.  This amounts to the introduction of a \inquotes{let}
in the term, which is also displayed in this form.  Here is an
example: let $f$ be variable of type $\typeN \typeTo \typeN$ and $g$
of type $\typeN \typeTo \typeB$.  Consider the proof
\begin{verbatim}
(set-goal "all f,g,n ex boole(
  (boole -> ex m(m<f n & g m) -> F) &
  ((ex m(m<f n & g m) -> F) -> boole))")
(assume "f" "g" "n")
(ex-intro (pt "NatLeast(f n)g=f n"))
(split) ;4,5
(assume "EqHyp" "ExHyp")
(by-assume "ExHyp" "m" "mProp")
(assert "NatLeast(f n)g<f n")
 (use "NatLeLtTrans" (pt "m"))
 (use "NatLeastLeIntro")
 (use "mProp")
 (use "mProp")
 (simp "EqHyp")
(assume "Absurd")
(use "Absurd")
;; Goal 5
(assume "NegExHyp")
(use "NatLeGeToEq")
(use "NatLeastBound")
(use "NatNotLtToLe")
(assume "LtHyp")
(use "NegExHyp")
(ex-intro (pt "NatLeast(f n)g"))
(split)
(use "LtHyp")
(use "NatLeastLtElim")
(use "LtHyp")
;; Proof finished.
(pp (nt (proof-to-extracted-term)))
;; [f0,g1,n2]NatLeast(f0 n2)g1=f0 n2
\end{verbatim}
A problem is that when evaluating this term one needs to compute
\texttt{f0 n2} twice.  To introduce the desired \inquotes{let}, at a
place where the term to be taken out can be constructed (here:
\texttt{f n}) one cuts in the formula $E \defeq$ \texttt{ex n0 n0=f
  n}.  This generates two new goals: an implication $E \to A$ (where A
is the present goal), and $A$, with the implication to be proved
first.  Now here one uses the identity theorem \texttt{Id}, and then
carries on with assuming the existential hypothesis, and taking an
\texttt{n0} with the definition \texttt{n0=f n} into the context.  In
the extracted term this will yield the constant \texttt{cId}
(evaluating to $\lambda_f f$) applied to $\lambda_{n_0}r(n_0)$ and $f
n$, displayed as \texttt{[let n0 (f n) r(n0)]}.  It is only after
\inquotes{animating} \texttt{Id} (i.e., adding the computation rule
\texttt{cId} $\mapsto$ $\lambda_f f$) that this term evaluates to
\texttt{r(f n)}, as desired.
\begin{verbatim}
(set-goal "all f,g,n ex boole(
  (boole -> ex m(m<f n & g m) -> F) &
  ((ex m(m<f n & g m) -> F) -> boole))")
(assume "f" "g" "n")
(cut "ex n0 n0=f n")
;; (use "Id") ;can be slow.  Use use-with instead:
(use-with
  "Id" (make-cterm (goal-to-formula (current-goal))) "?")
(assume "Exn0")
(by-assume "Exn0" "n0" "n0=f n")
(ex-intro (pt "NatLeast n0 g=n0"))
(split) ;11,12
(assume "EqHyp" "ExHyp")
(by-assume "ExHyp" "m" "mProp")
(assert "NatLeast(f n)g<f n")
 (use "NatLeLtTrans" (pt "m"))
 (use "NatLeastLeIntro")
 (use "mProp")
 (use "mProp")
 (simp "<-" "n0=f n")
 (simp "EqHyp")
(assume "Absurd")
(use "Absurd")
;; Goal 5
(assume "NegExHyp")
(use "NatLeGeToEq")
(use "NatLeastBound")
(use "NatNotLtToLe")
(assume "LtHyp")
(use "NegExHyp")
(ex-intro (pt "NatLeast(f n)g"))
(split)
(simp "<-" "n0=f n")
(use "LtHyp")
(use "NatLeastLtElim")
(simp "<-" "n0=f n")
(use "LtHyp")
;; Now we prove the formula cut in above.
(ex-intro (pt "f n"))
(use "Truth")
;; Proof finished.
(pp (nt (proof-to-extracted-term)))
;; [f0,g1,n2][let n3 (f0 n2) (NatLeast n3 g1=n3)]
\end{verbatim}

However, sometimes it is not easy to find the right places for
introducting a cut.  For such situations it can be helpful to hand
optimize a term by searching for its longest duplicate subterm, and
taking that subterm out via a \inquotes{let}.  The relevant function
is
\begin{equation*}
  \texttt{(term-to-term-with-let \textsl{term})}.
  \index{term-to-term-with-let@\texttt{(term-to-term-with-let}}
\end{equation*}

As test functions we provide
\begin{alignat*}{2}
  &\texttt{(term-form?\ \textsl{x})},%
  \index{term-form?@\texttt{term-form?}}
  \\
  &\texttt{(term?\ \textsl{x})},%
  \index{term?@\texttt{term?}}
  \\
  &\texttt{(check-term \textsl{x})}
  \index{check-term@\texttt{check-term}}
\end{alignat*}
abbreviated \texttt{ct}\index{ct@\texttt{ct}}.  Here \texttt{term?}
returns \texttt{\#t} or \texttt{\#f}, and \texttt{check-term} is a
complete test returning an error if the argument is not a term.

\subsection{Substitution}
Recall the generalities on substitutions\index{substitution} in
section~\ref{SS:GenSubst}.  Under the conditions stated there on
admissibility we define
\begin{alignat*}{2}
  &\texttt{(term-substitute \textsl{term} \textsl{tosubst})},%
  \index{term-substitute@\texttt{term-substitute}}
  \\
  &\texttt{(term-subst \textsl{term} \textsl{arg} \textsl{val})},%
  \index{term-subst@\texttt{term-subst}}
  \\
  &\texttt{(compose-substitutions \textsl{subst1} \textsl{subst2})}.
  \index{compose-substitutions@\texttt{compose-substitutions}}
\end{alignat*}
Display functions for substitutions are
\begin{align*}
  &\texttt{(pp-subst \textsl{topsubst})}\index{pp-subst@\texttt{pp-subst}}
  \\
  &\texttt{(display-substitutions \textsl{topsubst})},%
  \index{display-substitutions@\texttt{display-substitutions}}
  \\
  &\texttt{(substitution-to-string \textsl{subst})}.
  \index{substitution-to-string@\texttt{substitution-to-string}}
\end{align*}
We also provide
\begin{align*}
  &\texttt{(term-gen-substitute \textsl{term} \textsl{gen-subst})},%
  \index{term-gen-substitute@\texttt{(term-gen-substitute}}
  \\
  &\texttt{(term-gen-subst \textsl{term} \textsl{term1} \textsl{term2})}.%
  \index{term-gen-subst@\texttt{(term-gen-subst}}
\end{align*}
\texttt{term-gen-substitute} substitutes simultaneously the left hand
sides of the association list \textsl{gen-subst} (associating terms to
terms) at all occurrences in \textsl{term} with no free variables
captured by the corresponding right hand sides.  Renaming takes place
if and only if a free variable would become bound.

\subsection{Unification and matching}
For first order unification we use \texttt{unify}%
\index{unify@\texttt{unify}}.  It checks whether two terms can be
unified, returns \texttt{\#f} if this is impossible, and a most
general unifier otherwise.  \texttt{unify-list}%
\index{unify-list@\texttt{unify-list}} does the same for lists of
terms.  The implemented algorithm makes use of disagreement pairs, and
does not yield idempotent unifiers (as opposed to the
Martelli-Montanari algorithm \cite{Martelli82}, implemented in
\texttt{modules/type-inf.scm}).

For first order matching we use \texttt{match}%
\index{match@\texttt{match}}.  It checks whether a given pattern (term
or formula with type variables in its types) can be transformed by a
tosubst - respecting totality constraints - into a given instance,
such that (i) no type variable from a given set of identity variables,
and (ii) no object variable from a given set of signature variables
gets substituted.  It returns \texttt{\#f}, if this is impossible, and
the tosubst otherwise.

For higher-order unification we use Huet's \cite{Huet75} unification
algorithm, and (for the approriate fragment) also Miller's
\cite{Miller91b} pattern unification algorithm; both are discussed in
\ref{S:UnifSearch}.  Higher-order matching is implemented as
\texttt{huet-match}\index{huet-match@\texttt{huet-match}}, which is
defined as a special case of \texttt{huet-unifiers}%
\index{huet-unifiers@\texttt{huet-unifiers}}: no flexible variables
are allowed in the instance.  \texttt{huet-match} picks a most
detailed substitution.  Higher-order matching for type substitutions
is implemented as \texttt{pattern-and-instance-to-tsubst}%
\index{pattern-and-instance-to-tsubst@\texttt{pattern-and-instance-to-tsubst}}.

\section{Formulas and comprehension terms}
\label{S:Formulas}

\subsection{Constructors and accessors}
\label{SS:FormulaConstr}
A \emph{prime formula}\index{formula!prime} has the form
\begin{equation*}
\texttt{(predicate P r1 ... rn)}
\end{equation*}
with a predicate variable or constant \texttt{P} and terms \texttt{r1}
\dots \texttt{rn}.  \emph{Formulas}\index{formula} are built from
prime formulas by
\begin{enumeratei}
\item \texttt{(imp \textsl{formula1} \textsl{formula2})} implication%
  \index{implication},
\item \texttt{(all \textsl{x} \textsl{formula})} all quantification%
  \index{all quantification},
\item \texttt{(impnc \textsl{formula1} \textsl{formula2})} implication
  without computational content%
  \index{implication!without computational content},
\item \texttt{(allnc \textsl{x} \textsl{formula})} all quantification
  without computational content%
  \index{all quantification!without computational content},
\item \texttt{(exca (\textsl{x1} \dots \textsl{xn}) \textsl{formula})}
  \index{exca@\texttt{exca}} classical existential quantification
  (with the arithmetical form of falsity $\falsityF$),
\item \texttt{(excl (\textsl{x1} \dots \textsl{xn}) \textsl{formula})}
  \index{excl@\texttt{excl}} classical existential quantification
  (with the logical form of falsity $\falsum$),
\item \texttt{(excu (\textsl{x1} \dots \textsl{xn}) \textsl{formula})}
  \index{excu@\texttt{excu}} classical existential quantification
  (with the logical form of falsity $\falsum$, and using $\allnc$
  rather than $\forall$ in the unfolded form),
\item \texttt{(tensor \textsl{formula1} \textsl{formula2})} tensor%
  \index{tensor}, for proper unfolding of formulas containing
  \texttt{exca}, \texttt{excl} or \texttt{excu}.
\end{enumeratei}
We allow that quantified variables are formed without \verb#^#,
i.e., range over total objects only.

Formulas can be \emph{unfolded}\index{formula!unfolded} in the sense
that the all classical existential quantifiers are replaced according
to their definition.  Inversely a formula can be
\emph{folded}\index{formula!folded} in the sense that classical
existential quantifiers are introduced wherever possible.  Notice
that, since $\excl_x \excl_y A$ unfolds into a rather awkward formula,
we have extended the $\excl$-terminology to lists of variables:
\begin{equation*}
  \excl_{x_1, \dots,x_n} A \defeq
  \forall_{x_1, \dots, x_n}(A \to \falsum) \to \falsum.
\end{equation*}
In this context the tensor connective (written $\landcl$) allows to
abbreviate
\begin{equation*}
  \excl_{x_1, \dots,x_n}(A_1 \landcl \dots \landcl A_m) \defeq
  \forall_{x_1, \dots, x_n}(
  A_1 \to \dots \to A_m \to \falsum) \to \falsum.
\end{equation*}
This way we stay in the $\to, \forall$ part of the language.  Notice
that $\landcl$\index{$\landcl$} only makes sense in this context,
i.e., in connection with $\excl$.

Leibniz equality\index{equality!Leibniz}%
\index{Leibniz equality}, the existential quantifier, conjunction and
disjunction are provided by means of inductively defined predicates.
We also have the built-in versions:
\begin{enumeratei}
\item \texttt{(and \textsl{formula1} \textsl{formula2})} conjunction%
  \index{conjunction}
\item \texttt{(ex \textsl{x} \textsl{formula})} existential
  quantification%
  \index{existential quantification}
\end{enumeratei}
We also allow prime formulas of the form \texttt{(atom r)} with a term
\texttt{r} of type boole.  They are just shorthand for Leibniz
equality of \texttt{r} with the boolean constant \texttt{True},
written \texttt{True eqd r}.

\emph{Comprehension terms}%
\index{comprehension term} have the form \texttt{(cterm \textsl{vars}
  \textsl{formula})}.  Note that \textsl{formula} may contain further
free variables.

%% \subsection*{Interface}
Tests:
\begin{align*}
  &\texttt{(atom-form?\ \textsl{formula})},%%
  \index{atom-form?@\texttt{atom-form?}}
  \\
  &\texttt{(predicate-form?\ \textsl{formula})},%
  \index{predicate-form?@\texttt{predicate-form?}}
  \\
  &\texttt{(prime-form?\ \textsl{formula})},%
  \index{prime-form?@\texttt{prime-form?}}
  \\
  &\texttt{(imp-form?\ \textsl{formula})},%
  \index{imp-form?@\texttt{imp-form?}}
  \\
  &\texttt{(impnc-form?\ \textsl{formula})},%
  \index{impnc-form?@\texttt{impnc-form?}}
  \\
  &\texttt{(and-form?\ \textsl{formula})},%
  \index{and-form?@\texttt{and-form?}}
  \\
  &\texttt{(tensor-form?\ \textsl{formula})},%
  \index{tensor-form?@\texttt{tensor-form?}}
  \\
  &\texttt{(all-form?\ \textsl{formula})},%
  \index{all-form?@\texttt{all-form?}}
  \\
  &\texttt{(allnc-form?\ \textsl{formula})},%
  \index{allnc-form?@\texttt{allnc-form?}}
  \\
  &\texttt{(ex-form?\ \textsl{formula})},%
  \index{ex-form?@\texttt{ex-form?}}
  \\
  &\texttt{(exca-form?\ \textsl{formula})},%
  \index{exca-form?@\texttt{exca-form?}}
  \\
  &\texttt{(excl-form?\ \textsl{formula})},%
  \index{excl-form?@\texttt{excl-form?}}
  \\
  &\texttt{(excu-form?\ \textsl{formula})}%
  \index{excu-form?@\texttt{excu-form?}}
\end{align*}
and also
\begin{align*}
  &\texttt{(quant-prime-form?\ \textsl{formula})},%
  \index{quant-prime-form?@\texttt{quant-prime-form?}}
  \\
  &\texttt{(quant-free?\ \textsl{formula})}.%
  \index{quant-free?@\texttt{quant-free?}}
\end{align*}

We need constructors and accessors for prime formulas
\begin{alignat*}{2}
  &\texttt{(make-atomic-formula \textsl{boolean-term})},%
  \index{make-atomic-formula@\texttt{make-atomic-formula}}
  \\
  &\texttt{(make-predicate-formula \textsl{predicate} \textsl{term1} \dots)},%
  \index{make-predicate-formula@\texttt{make-predicate-formula}}
  \\
  &\texttt{atom-form-to-kernel},%
  \index{atom-form-to-kernel@\texttt{atom-form-to-kernel}}
  \\
  &\texttt{predicate-form-to-predicate},%
  \index{predicate-form-to-predicate@\texttt{predicate-form-to-predicate}}
  \\
  &\texttt{predicate-form-to-args}.%
  \index{predicate-form-to-args@\texttt{predicate-form-to-args}}
\end{alignat*}
We also have constructors for special atomic formulas
\begin{alignat*}{2}
  &\texttt{(make-eqd \textsl{term1} \textsl{term2})}%%
  \index{make-eqd@\texttt{make-eqd}}
  &\quad& \text{constructor for Leibniz equalities},
  \\
  &\texttt{(make-= \textsl{term1} \textsl{term2})}%
  \index{make-=@\texttt{make-=}}
  &\quad& \text{constructor for equalities (atomic or \texttt{eqd})},
  \\
  &\texttt{(make-total \textsl{term})}%
  \index{make-total@\texttt{make-total}}
  &\quad& \text{constructor for totalities},
  \\
  &\texttt{(make-e \textsl{term})}%
  \index{make-e@\texttt{make-e}}
  &\quad& \text{constructor for existence on finalgs},
  \\
  &\texttt{truth}%
  \index{truth@\texttt{truth}},
  \\
  &\texttt{falsity}%
  \index{falsity@\texttt{falsity}},
  \\
  &\texttt{falsity-log}%
  \index{falsity-log@\texttt{falsity-log}}.
\end{alignat*}
We need constructors and accessors for implications
\begin{alignat*}{2}
  &\texttt{(make-imp \textsl{premise} \textsl{conclusion})}
  \index{make-imp@\texttt{make-imp}}
  &\quad& \text{constructor},
  \\
  &\texttt{(imp-form-to-premise \textsl{imp-formula})}
  \index{imp-form-to-premise@\texttt{imp-form-to-premise}}
  && \text{accessor},
  \\
  &\texttt{(imp-form-to-conclusion \textsl{imp-formula})}
  \index{imp-form-to-conclusion@\texttt{imp-form-to-conclusion}}
  && \text{accessor},
\end{alignat*}
non-computational implications ($\tonc$; displayed \verb#-->#)
\begin{alignat*}{2}
  &\texttt{(make-impnc \textsl{premise} \textsl{conclusion})}
  \index{make-impnc@\texttt{make-impnc}}
  &\quad& \text{constructor},
  \\
  &\texttt{(impnc-form-to-premise \textsl{impnc-formula})}
  \index{impnc-form-to-premise@\texttt{impnc-form-to-premise}}
  && \text{accessor},
  \\
  &\texttt{(impnc-form-to-conclusion \textsl{impnc-formula})}
  \index{impnc-form-to-conclusion@\texttt{impnc-form-to-conclusion}}
  && \text{accessor},
\end{alignat*}
conjunctions
\begin{alignat*}{2}
  &\texttt{(make-and \textsl{formula1} \textsl{formula2})}
  \index{make-and@\texttt{make-and}}
  &\quad& \text{constructor},
  \\
  &\texttt{(and-form-to-left \textsl{and-formula})}
  \index{and-form-to-left@\texttt{and-form-to-left}}
  && \text{accessor},
  \\
  &\texttt{(and-form-to-right \textsl{and-formula})}
  \index{and-form-to-right@\texttt{and-form-to-right}}
  && \text{accessor},
\end{alignat*}
tensors
\begin{alignat*}{2}
  &\texttt{(make-tensor \textsl{formula1} \textsl{formula2})}
  \index{make-tensor@\texttt{make-tensor}}
  &\quad& \text{constructor},
  \\
  &\texttt{(tensor-form-to-left \textsl{tensor-formula})}
  \index{tensor-form-to-left@\texttt{tensor-form-to-left}}
  && \text{accessor},
  \\
  &\texttt{(tensor-form-to-right \textsl{tensor-formula})}
  \index{tensor-form-to-right@\texttt{tensor-form-to-right}}
  && \text{accessor},
\end{alignat*}
universally quantified formulas
\begin{alignat*}{2}
  &\texttt{(make-all \textsl{var} \textsl{formula})}
  \index{make-all@\texttt{make-all}}
  &\quad& \text{constructor},
  \\
  &\texttt{(all-form-to-var \textsl{all-formula})}
  \index{all-form-to-var@\texttt{all-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(all-form-to-kernel \textsl{all-formula})}
  \index{all-form-to-kernel@\texttt{all-form-to-kernel}}
  && \text{accessor},
\end{alignat*}
universally quantified formulas without computational content ($\allnc$)
\begin{alignat*}{2}
  &\texttt{(make-allnc \textsl{var} \textsl{formula})}
  \index{make-allnc@\texttt{make-allnc}}
  &\quad& \text{constructor},
  \\
  &\texttt{(allnc-form-to-var \textsl{allnc-formula})}
  \index{allnc-form-to-var@\texttt{allnc-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(allnc-form-to-kernel \textsl{allnc-formula})}
  \index{allnc-form-to-kernel@\texttt{allnc-form-to-kernel}}
  && \text{accessor},
\end{alignat*}
existentially quantified formulas
\begin{alignat*}{2}
  &\texttt{(make-ex \textsl{var} \textsl{formula})}
  \index{make-ex@\texttt{make-ex}}
  &\quad& \text{constructor},
  \\
  &\texttt{(ex-form-to-var \textsl{ex-formula})}
  \index{ex-form-to-var@\texttt{ex-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(ex-form-to-kernel \textsl{ex-formula})}
  \index{ex-form-to-kernel@\texttt{ex-form-to-kernel}}
  && \text{accessor},
\end{alignat*}
existentially quantified formulas in the sense of classical arithmetic
\begin{alignat*}{2}
  &\texttt{(make-exca \textsl{var} \textsl{formula})}
  \index{make-exca@\texttt{make-exca}}
  &\quad& \text{constructor},
  \\
  &\texttt{(exca-form-to-var \textsl{exca-formula})}
  \index{exca-form-to-var@\texttt{exca-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(exca-form-to-kernel \textsl{exca-formula})}
  \index{exca-form-to-kernel@\texttt{exca-form-to-kernel}}
  && \text{accessor},
\end{alignat*}
existentially quantified formulas in the sense of classical logic
\begin{alignat*}{2}
  &\texttt{(make-excl \textsl{var} \textsl{formula})}
  \index{make-excl@\texttt{make-excl}}
  &\quad& \text{constructor},
  \\
  &\texttt{(excl-form-to-var \textsl{excl-formula})}
  \index{excl-form-to-var@\texttt{excl-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(excl-form-to-kernel \textsl{excl-formula})}
  \index{excl-form-to-kernel@\texttt{excl-form-to-kernel}}
  && \text{accessor},
\end{alignat*}
existentially quantified formulas in the sense of classical logic w.r.t.\
$\allnc$
\begin{alignat*}{2}
  &\texttt{(make-excu \textsl{var} \textsl{formula})}
  \index{make-excu@\texttt{make-excu}}
  &\quad& \text{constructor},
  \\
  &\texttt{(excu-form-to-var \textsl{excu-formula})}
  \index{excu-form-to-var@\texttt{excu-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(excu-form-to-kernel \textsl{excu-formula})}
  \index{excu-form-to-kernel@\texttt{excu-form-to-kernel}}
  && \text{accessor.}
\end{alignat*}

By means of inductively defined predicate constants, we have defined
computationally sensitive forms of existential quantification
\texttt{exd}, \texttt{exl}, \texttt{exr}, \texttt{exu} written $\exd$,
$\exL$, $\exR$, $\exnc$ and also their total versions \texttt{exdt},
\texttt{exlt}, \texttt{exrt}, \texttt{exut}, conjunction
\texttt{andd}, \texttt{andr}, \texttt{andu} written $\landd$,
$\landR$, $\landnc$ (\texttt{andb} is used for the boolean operator),
disjunction \texttt{or}, \texttt{orl}, \texttt{orr}, \texttt{oru},
\texttt{ornc} written $\lord$, $\lorL$, $\lorR$, $\loru$, $\lornc$
(\texttt{orb} is used for the boolean operator).
For all these we have similar constructors and accessors.  There is
also some mild form of automatization for these computationally
sensitive forms of logical connectives, which provides for
computational content only if there is some:
\begin{alignat*}{2}
  &\texttt{(make-exi \textsl{var} \textsl{formula})},%%
  \index{make-exi@\texttt{make-exi}}
  \\
  &\texttt{(make-exnci \textsl{var} \textsl{formula})},%%
  \index{make-exnci@\texttt{make-exnci}}
  \\
  &\texttt{(make-andi \textsl{formula1} \textsl{formula2})},%%
  \index{make-andi@\texttt{make-andi}}
  \\
  &\texttt{(make-ori \textsl{formula1} \textsl{formula2})}.
  \index{make-ori@\texttt{make-ori}}
\end{alignat*}

For convenience we have as generalized constructors
\begin{alignat*}{2}
  &\texttt{(mk-imp \textsl{formula} \textsl{formula1} {\dots})}%
  \index{mk-imp@\texttt{mk-imp}}
  &\quad&\text{implication},
  \\
  &\texttt{(mk-impnc \textsl{formula} \textsl{formula1} {\dots})}%
  \index{mk-impnc@\texttt{mk-impnc}}
  &\quad&\text{n.c.\ implication},
  \\
  &\texttt{(mk-neg \textsl{formula1} {\dots})}%%
  \index{mk-neg@\texttt{mk-neg}}
  && \text{negation},
  \\
  &\texttt{(mk-neg-log \textsl{formula1} {\dots})}%%
  \index{mk-neg-log@\texttt{mk-neg-log}}
  && \text{logical negation},
  \\
  &\texttt{(mk-and \textsl{formula} \textsl{formula1} {\dots})}%%
  \index{mk-and@\texttt{mk-and}}
  && \text{conjunction},
  \\
  &\texttt{(mk-tensor \textsl{formula} \textsl{formula1}\! {\dots}\!)}%%
  \index{mk-tensor@\texttt{mk-tensor}}
  && \text{tensor},
  \\
  &\texttt{(mk-all \textsl{var1} {\dots}\ \textsl{formula})}%%
  \index{mk-all@\texttt{mk-all}}
  && \text{all-formula},
  \\
  &\texttt{(mk-allnc \textsl{var1} {\dots}\ \textsl{formula})}%%
  \index{mk-allnc@\texttt{mk-allnc}}
  && \text{allnc-formula},
  \\
  &\texttt{(mk-ex \textsl{var1} {\dots}\ \textsl{formula})}%%
  \index{mk-ex@\texttt{mk-ex}}
  && \text{ex-formula},
  \\
  &\texttt{(mk-exca \textsl{var1} {\dots}\ \textsl{formula})}%%
  \index{mk-exca@\texttt{mk-exca}}
  && \text{classical ex-formula (arithmetical)},
  \\
  &\texttt{(mk-excl \textsl{var1} {\dots}\ \textsl{formula})}%%
  \index{mk-excl@\texttt{mk-excl}}
  && \text{classical ex-formula (logical)},
  \\
  &\texttt{(mk-excu \textsl{var1} {\dots}\ \textsl{formula})}%%
  \index{mk-excu@\texttt{mk-excu}}
  && \text{classical ex-formula (logical, n.c.)}.
\end{alignat*}
and similar for the computationally sensitive logical connectives:
\texttt{mk-exd}\index{mk-exd@\texttt{mk-exd}},
\texttt{mk-exl}\index{mk-exl@\texttt{mk-exl}},
\texttt{mk-exr}\index{mk-exr@\texttt{mk-exr}},
\texttt{mk-exu}\index{mk-exu@\texttt{mk-exu}},
\texttt{mk-exdt}\index{mk-exdt@\texttt{mk-exdt}},
\texttt{mk-exlt}\index{mk-exlt@\texttt{mk-exlt}},
\texttt{mk-exrt}\index{mk-exrt@\texttt{mk-exrt}},
\texttt{mk-exut}\index{mk-exut@\texttt{mk-exut}},
\texttt{mk-andd}\index{mk-andd@\texttt{mk-andd}},
\texttt{mk-andr}\index{mk-andr@\texttt{mk-andr}},
\texttt{mk-andu}\index{mk-andu@\texttt{mk-andu}},
\texttt{mk-ord}\index{mk-ord@\texttt{mk-ord}},
\texttt{mk-orl}\index{mk-orl@\texttt{mk-orl}},
\texttt{mk-orr}\index{mk-orr@\texttt{mk-orr}},
\texttt{mk-oru}\index{mk-oru@\texttt{mk-oru}},
\texttt{mk-exi}\index{mk-exi@\texttt{mk-exi}},
\texttt{mk-exnci}\index{mk-exnci@\texttt{mk-exnci}},
\texttt{mk-andi}\index{mk-andi@\texttt{mk-andi}},
\texttt{mk-ori}\index{mk-ori@\texttt{mk-ori}}.
As generalized accessors we have
\begin{alignat*}{2}
  &\texttt{(imp-form-to-premises-and-final-conclusion \textsl{formula})},
  \\
  &\texttt{(tensor-form-to-parts \textsl{formula})},%
  \index{tensor-form-to-parts@\texttt{tensor-form-to-parts}}
  \\
  &\texttt{(all-form-to-vars-and-final-kernel \textsl{formula})},%
  \index{all-form-to-vars-and-final-kernel@\texttt{all-form-to-vars-and{\dots}}}
  \\
  &\texttt{(ex-form-to-vars-and-final-kernel \textsl{formula})}%
  \index{ex-form-to-vars-and-final-kernel@\texttt{ex-form-to-vars-and{\dots}}}
\end{alignat*}
and again similar for \texttt{impnc}-, \texttt{allnc}-, \texttt{exca}- and
\texttt{excl}-forms, and the computationally sensitive logical
connectives.  Occasionally it is convenient to have
\begin{alignat*}{2}
  &\texttt{(imp-form-to-premises \textsl{formula} <\textsl{n}>)},%
  \index{imp-form-to-premises@\texttt{imp-form-to-premises}}
  &\quad& \text{all (first $n$) premises}
  \\
  &\texttt{(imp-form-to-final-conclusion \textsl{formula} <\textsl{n}>)}
  \index{imp-form-to-final-conclusion@\texttt{imp-form-to-final-conclusion}}
\end{alignat*}
where the latter computes the final conclusion (conclusion after
removing the first $n$ premises) of the formula (similar for
\texttt{impnc}-forms).

It is also useful to have some general procedures working for
arbitrary binary connectives and quantified formulas.  We provide
\begin{alignat*}{2}
  &\texttt{(make-bicon \textsl{bicon} \textsl{formula1} \textsl{formula2})}%%
  \index{make-bicon@\texttt{make-bicon}}
  &\quad& \text{constructor},
  \\
  &\texttt{(bicon-form-to-bicon \textsl{bicon-form})}%%
  \index{bicon-form-to-bicon@\texttt{bicon-form-to-bicon}}
  &\quad& \text{accessor},
  \\
  &\texttt{(bicon-form-to-left \textsl{bicon-form})}%%
  \index{bicon-form-to-left@\texttt{bicon-form-to-left}}
  &\quad& \text{accessor},
  \\
  &\texttt{(bicon-form-to-right \textsl{bicon-form})}%%
  \index{bicon-form-to-right@\texttt{bicon-form-to-right}}
  &\quad& \text{accessor},
  \\
  &\texttt{(bicon-form?\ \textsl{x})}%%
  \index{bicon-form?@\texttt{bicon-form?}}
  &\quad& \text{test},
  \\
  &\texttt{(make-quant-formula \textsl{quant} \textsl{var1} \dots\
    \textsl{kernel})}%%
  \index{make-quant-formula@\texttt{make-quant-formula}}
  &\quad& \text{constructor},
  \\
  &\texttt{(quant-form-to-quant \textsl{quant-form})}%
  \index{quant-form-to-quant@\texttt{quant-form-to-quant}}
  && \text{accessor},
  \\
  &\texttt{(quant-form-to-vars \textsl{quant-form})}%%
  \index{quant-form-to-vars@\texttt{quant-form-to-vars}}
  && \text{accessor},
  \\
  &\texttt{(quant-form-to-kernel \textsl{quant-form})}%
  \index{quant-form-to-kernel@\texttt{quant-form-to-kernel}}
  && \text{accessor},
  \\
  &\texttt{(quant-form?\ \textsl{x})}%%
  \index{quant-form?@\texttt{quant-form?}}
  && \text{test}
\end{alignat*}
and for convenience also
\begin{equation*}
  \texttt{(mk-quant \textsl{quant} \textsl{var1} \dots\
    \textsl{formula})\index{mk-quant@\texttt{mk-quant}}.}
\end{equation*}
We also provide
\begin{align*}
  &\texttt{(prime-predicate-form?\ \textsl{x})},%%
  \index{prime-predicate-form?@\texttt{prime-predicate-form?}}
  \\
  &\texttt{(prime-form?\ \textsl{x})},%%
  \index{prime-form?@\texttt{prime-form?}}
  \\
  &\texttt{(quant-prime-form?\ \textsl{x})},%%
  \index{quant-prime-form?@\texttt{quant-prime-form?}}
  \\
  &\texttt{(quant-free?\ \textsl{x})}
  \index{quant-free?@\texttt{quant-free?}}
\end{align*}
and for prime, imp, impnc, all or allnc formulas
\begin{equation*}
  \texttt{(formula-to-head \textsl{formula})}.
  \index{formula-to-head@\texttt{formula-to-head}}
\end{equation*}
To fold and unfold (classical existential quantifiers in) formulas we
have
\begin{align*}
  &\texttt{(fold-formula \textsl{formula})},%%
  \index{fold-formula@\texttt{fold-formula}}
  \\
  &\texttt{(unfold-formula \textsl{formula}).}
  \index{unfold-formula@\texttt{unfold-formula}}
\end{align*}
To test equality of formulas up to normalization and $\alpha$-equality
we use
\begin{align*}
  &\texttt{(classical-formula=?\ \textsl{formula1} \textsl{formula2})},%%
  \index{classical-formula=?@\texttt{classical-formula=?}}
  \\
  &\texttt{(formula=?\ \textsl{formula1} \textsl{formula2})},
  \index{formula=?@\texttt{formula=?}}
  %% &\texttt{(formulas=?\ \textsl{formulas1} \textsl{formulas2}),}
\end{align*}
where in the first procedure we unfold before comparing.

Morever we need
\begin{align*}
  &\texttt{(formula-to-free \textsl{formula})},%%
  \index{formula-to-free@\texttt{formula-to-free}}
  \\
  &\texttt{(formula-to-bound \textsl{formula})},%%
  \index{formula-to-bound@\texttt{formula-to-bound}}
  \\
  &\texttt{(formula-to-tvars \textsl{formula})},%%
  \index{formula-to-tvars@\texttt{formula-to-tvars}}
  \\
  &\texttt{(formula-to-pvars \textsl{formula})},%%
  \index{formula-to-pvars@\texttt{formula-to-pvars}}
  \\
  &\texttt{(ex-free-formula?\ \textsl{formula})},%
  \index{ex-free-formula?@\texttt{ex-free-formula?}}
  \\
  &\texttt{(nbe-formula-to-type \textsl{formula})},%
  \index{nbe-formula-to-type@\texttt{nbe-formula-to-type}}
  \\
  &\texttt{(formula-to-prime-subformulas \textsl{formula})}.
  \index{formula-to-prime-subformulas@\texttt{formula-to-prime-subformulas}}
\end{align*}
\texttt{nbe-formula-to-type} needs a procedure associating type
variables to predicate variables, which remembers the assignment done
so far.  Therefore it refers to the global variable
\texttt{PVAR-TO-TVAR}\index{PVAR-TO-TVAR@\texttt{PVAR-TO-TVAR}}.
This machinery will be used to assign recursion constants to induction
constants.  There we need to associate type variables with predicate
variables, in such a way that we can later refer to this assignment.

We also provide
\begin{align*}
  &\texttt{(formula-to-prime-subformulas \textsl{formula})}.%
  \index{formula-to-prime-subformulas@\texttt{formula-to-prime-subformulas}}
\end{align*}

As an alternative to normalization by evaluation, we can also
normalize \inquotes{by hand}.  This is done via
\begin{align*}
  &\texttt{(formula-to-beta-nf \textsl{formula})},%
  \index{formula-to-beta-nf@\texttt{formula-to-beta-nf}}
  \\
  &\texttt{(cterm-to-beta-nf \textsl{cterm})},%
  \index{cterm-to-beta-nf@\texttt{cterm-to-beta-nf}}
  \\
  &\texttt{(formula-to-eta-nf \textsl{formula})},%
  \index{formula-to-eta-nf@\texttt{formula-to-eta-nf}}
  \\
  &\texttt{(cterm-to-eta-nf \textsl{cterm})},%
  \index{cterm-to-eta-nf@\texttt{cterm-to-eta-nf}}
  \\
  &\texttt{(formula-to-beta-eta-nf \textsl{formula})},%
  \index{formula-to-beta-eta-nf@\texttt{formula-to-beta-eta-nf}}
  \\
  &\texttt{(cterm-to-beta-eta-nf \textsl{cterm})}.
  \index{cterm-to-beta-eta-nf@\texttt{cterm-to-beta-eta-nf}}
\end{align*}

Clearly every quantifier-free formula can be converted into a term
of type boole; this is done by
\begin{equation*}
  \texttt{(qf-to-term \textsl{formula})}.
  \index{qf-to-term@\texttt{qf-to-term}}
\end{equation*}
We also provide
\begin{equation*}
  \texttt{(alpha-equal-formulas-to-renaming\
    \textsl{formula1} \textsl{formula2}).}
  \index{alpha-equal-formulas-to-renaming@\texttt{alpha-equal-formulas-to-renaming}}
\end{equation*}

The constructor and accessors for comprehension terms are
\begin{alignat*}{2}
  &\texttt{(make-cterm \textsl{var1} \dots\ \textsl{formula})}%%
  \index{make-cterm@\texttt{make-cterm}}
  &\quad& \text{constructor},
  \\
  &\texttt{(cterm-to-vars \textsl{cterm})}
  \index{cterm-to-vars@\texttt{cterm-to-vars}}
  && \text{accessor},
  \\
  &\texttt{(cterm-to-formula \textsl{cterm})}
  \index{cterm-to-formula@\texttt{cterm-to-formula}}
  && \text{accessor}.
\end{alignat*}
Moreover we need
\begin{align*}
  &\texttt{(cterm-to-arity \textsl{cterm})},%%
  \index{cterm-to-arity@\texttt{cterm-to-arity}}
  \\
  &\texttt{(cterm-to-free \textsl{cterm})},%%
  \index{cterm-to-free@\texttt{cterm-to-free}}
  \\
  &\texttt{(cterm-to-bound \textsl{cterm})},%%
  \index{cterm-to-bound@\texttt{cterm-to-bound}}
  \\
  &\texttt{(fold-cterm \textsl{cterm})},%%
  \index{fold-cterm@\texttt{fold-cterm}}
  \\
  &\texttt{(unfold-cterm \textsl{cterm})},%
  \index{unfold-cterm@\texttt{unfold-cterm}}
  \\
  &\texttt{(pvar-cterm-to-pvar \textsl{cterm})},%
  \index{pvar-cterm-to-pvar@\texttt{pvar-cterm-to-pvar}}
  \\
  &\texttt{(pvar-cterm?\ \textsl{cterm}).}
  \index{pvar-cterm?@\texttt{pvar-cterm?}}
\end{align*}

\subsection{Decoration}
\label{SS:Deco}
We think of (computationally relevant) implication $\to$ and universal
quantification $\forall$ as \inquotes{decorated} versions of their non
computational counterparts $\tonc$ and $\allnc$ (cf.\
\ref{SS:FormulaConstr}).  Moreover, existential quantifiers $\exd$,
$\exL$, $\exR$ can be seen as decorated versions of the non
computational quantifier $\exnc$, and similar for conjunction and
disjunction.  To \inquotes{undecorate} formulas and comprehension
terms we use
\begin{align*}
  &\texttt{(formula-to-undec-formula \textsl{formula} \textsl{id-deco?})},%
  \index{formula-to-undec-formula@\texttt{formula-to-undec-cterm}}
  \\
  &\texttt{(cterm-to-undec-cterm \textsl{cterm} \textsl{id-deco?})}.%
  \index{cterm-to-undec-cterm@\texttt{cterm-to-undec-cterm}}
\end{align*}
Both change all occurrences of $\to$, $\forall$ into $\tonc$,
$\allnc$, and in case \texttt{id-deco?}\ is true (\texttt{id} for
\inquotes{inductively defined logical connective}),
\begin{enumeratei}
\item existential quantification $\exd$, $\exL$, $\exR$ into $\exnc$,
\item conjunction $\landd$, $\landR$ into $\landnc$, and
\item disjunction $\lord$, $\lorL$, $\lorR$ into $\loru$.
\end{enumeratei}
They do not touch formulas of nulltype under extension, and in case
\texttt{id-deco?} is false do not touch any formula of nulltype.

Conversely, \texttt{formula-to-dec-formula}%
\index{formula-to-dec-formula@\texttt{formula-to-dec-formula}} changes
all occurrences of $\tonc$, $\allnc$ to $\to$, $\forall$.  We say that
\texttt{formula1} \emph{extends} \texttt{formula2} if some $\tonc$,
$\allnc$ have been changed into $\to$, $\forall$; then
\texttt{formula1} has a more complex type:
\begin{equation*}
  \texttt{(extending-dec-variants?\ \textsl{formula1} \textsl{formula2}
    \textsl{id-deco?})}%
    \index{extending-dec-variants?@\texttt{extending-dec-variants?}}.
\end{equation*}
In case \texttt{id-deco?}\ is true \inquotes{extension} is transferred
in the expected way to $\exd$, $\exL$, $\exR$, $\exnc$, $\landd$,
$\landR$, $\landnc$, $\lord$, $\lorL$, $\lorR$ and $\loru$.

\subsection{Normalization}
Normalization of formulas is done with
\begin{alignat*}{2}
  &\texttt{(normalize-formula \textsl{formula})},%
  \index{normalize-formula@\texttt{normalize-formula}}
  \\
  &\texttt{(normalize-cterm \textsl{cterm})}.
  \index{normalize-cterm@\texttt{normalize-cterm}}
\end{alignat*}
The former is abbreviated by \texttt{nf}\index{nf@\texttt{nf}}.

\subsection{Alpha-equality}
To check equality of formulas and comprehension terms we use
\begin{alignat*}{2}
  &\texttt{(classical-formula=?\ \textsl{formula1} \textsl{formula2}
    \textsl{opt-ignore-deco-flag})},%
  \index{classical-formula=?@\texttt{classical-formula=?}}
  \\
  &\texttt{(classical-cterm=?\ \textsl{cterm1} \textsl{cterm2}
    \textsl{opt-ignore-deco-flag})},%
  \index{classical-cterm=?@\texttt{classical-cterm=?}}
  \\
  &\texttt{(formula=?\ \textsl{formula1} \textsl{formula2}
    \textsl{opt-ignore-deco-flag})},%
  \index{formula=?@\texttt{formula=?}}
  \\
  &\texttt{(cterm=?\ \textsl{cterm1} \textsl{cterm2}
    \textsl{opt-ignore-deco-flag})}
  \index{cterm=?@\texttt{cterm=?}}
\end{alignat*}
where the \inquotes{classical} variants unfold classical existential
quantifiers and normalize all subterms in its formulas.

\texttt{rename-variables}%
\index{rename-variables@\texttt{rename-variables}} renames bound
variables in terms, formulas and comprehension terms.

\subsection{Display}
For a readable display of formulas we normally use
\begin{align*}
  &\texttt{(pp \textsl{formula})}\index{pp@\texttt{pp}}
\end{align*}
which is implemented using as auxiliary functions
\begin{align*}
  &\texttt{(predicate-to-token-tree \textsl{pred})},%
  \index{predicate-to-token-tree@\texttt{predicate-to-token-tree}}
  \\
  &\texttt{(formula-to-token-tree \textsl{formula})}.
  \index{formula-to-token-tree@\texttt{formula-to-token-tree}}
\end{align*}
Alternative display functions for formulas and comprehension terms are
\begin{alignat*}{2}
  &\texttt{(formula-to-string \textsl{formula})},%
  \index{formula-to-string@\texttt{formula-to-string}}
  \\
  &\texttt{(cterm-to-string \textsl{cterm})}.
  \index{cterm-to-string@\texttt{cterm-to-string}}
\end{alignat*}

\subsection{Check}
As test functions we provide
\begin{alignat*}{2}
  &\texttt{(formula-form?\ \textsl{x})},%
  \index{formula-form?@\texttt{formula-form?}}
  \\
  &\texttt{(cterm-form?\ \textsl{x})},%
  \index{cterm-form?@\texttt{cterm-form?}}
  \\
  &\texttt{(formula?\ \textsl{x})},%
  \index{formula?@\texttt{formula?}}
  \\
  &\texttt{(cterm?\ \textsl{x})},%
  \index{cterm?@\texttt{cterm?}}
  \\
  &\texttt{(check-formula \textsl{x})},
  \index{check-formula@\texttt{check-formula}}
\end{alignat*}
abbreviated \texttt{cf}\index{cf@\texttt{cf}}.  Here \texttt{formula?}
returns \texttt{\#t} or \texttt{\#f}, and \texttt{check-formula} is a
complete test returning an error if the argument is not a formula.

\subsection{Substitution}
We can simultaneously substitute\index{substitution} for type,
object and predicate variables in a formula or a comprehension
term:
\begin{alignat*}{2}
  &\texttt{(formula-substitute \textsl{formula} \textsl{topsubst})},%
  \index{formula-substitute@\texttt{formula-substitute}}
  \\
  &\texttt{(formula-subst \textsl{formula} \textsl{arg} \textsl{val})},%
  \index{formula-subst@\texttt{formula-subst}}
  \\
  &\texttt{(cterm-substitute \textsl{cterm} \textsl{topsubst})},%
  \index{cterm-substitute@\texttt{cterm-substitute}}
  \\
  &\texttt{(cterm-subst \textsl{cterm} \textsl{arg} \textsl{val})}.%
  \index{cterm-subst@\texttt{cterm-subst}}
\end{alignat*}
In a simultaneous substitution topsubst for type, object and predicate
variables in a formula or a comprehension term it is allowed that the
substitution affects variables whose type is changed by topsubst,
provided topsubst is admissible for the formula or the comprehension
term.

We also provide
\begin{align*}
  &\texttt{(formula-gen-substitute \textsl{formula} \textsl{gen-subst})},%
  \index{formula-gen-substitute@\texttt{formula-gen-substitute}}
  \\
  &\texttt{(formula-gen-subst \textsl{formula}
    \textsl{term1}  \textsl{term2})}.%
  \index{formula-gen-subst@\texttt{formula-gen-subst}}
\end{align*}
The former substitutes simultaneously the left hand sides of the
association list \textsl{gen-subst} at all occurrences in the formula
with no free variables captured by the corresponding right hand sides.
\textsl{gen-subst} is an association list associating terms to terms.
Renaming takes place if and only if a free variable would become
bound.

As display functions for substitutions we again use
\begin{align*}
  &\texttt{(pp-subst \textsl{topsubst})}\index{pp-subst@\texttt{pp-subst}}
  \\
  &\texttt{(display-substitutions \textsl{topsubst})}.%
  \index{display-substitutions@\texttt{display-substitutions}}
\end{align*}

\section{Assumption variables}
\label{S:AssumptionVar}
Assumption variables are for proofs what variables are for terms.  The
main difference, however, is that assumption variables have formulas
as types, and that formulas may contain free variables.  Therefore we
must be careful when substituting terms for variables in assumption
variables.  Our solution (as in Matthes' thesis \cite{Matthes98}) is
to consider an assumption variable as a pair of a (typefree)
identifier and a formula, and to take equality of assumption variables
to mean that both components are equal.  Rather than using symbols as
identifiers we prefer to use numbers (i.e., indices).  However,
sometimes it is useful to provide an optional string as name for
display purposes.

%% \subsection*{Interface}
We need a constructor, accessors and tests for assumption variables.
\begin{alignat*}{2}
  &\texttt{(make-avar \textsl{formula} \textsl{index} \textsl{name})}%
  \index{make-avar@\texttt{make-avar}}
  &\quad& \text{constructor},
  \\
  &\texttt{(avar-to-formula \textsl{avar})}%%
  \index{avar-to-formula@\texttt{avar-to-formula}}
  && \text{accessor},
  \\
  &\texttt{(avar-to-index \textsl{avar})}
  \index{avar-to-index@\texttt{avar-to-index}}
  && \text{accessor},
  \\
  &\texttt{(avar-to-name \textsl{avar})}
  \index{avar-to-name@\texttt{avar-to-name}}
  && \text{accessor},
  \\
  &\texttt{(avar?\ \textsl{x})}
  \index{avar?@\texttt{avar?}}
  && \text{test},
  \\
  &\texttt{(avar=?\ \textsl{avar1} \textsl{avar2})}
  \index{avar=?@\texttt{avar=?}}
  && \text{test}.
\end{alignat*}
Testing equality of assumption variables is often used, and it is
expensive since it involves an equality test for formulas (which
includes normalization).  To allow a more efficient equality test, we
maintain an \emph{avar-convention}\index{avar-convention}: whenever
two assumption variables have the same identifier, their formulas are
equal as well; therefore \texttt{avar=?} only checks equality of
identifiers.  For a full test one can use
\begin{equation*}
  \texttt{(avar-full=?\ \textsl{avar1} \textsl{avar2}
    \textsl{opt-ignore-deco-flag})}.%
  \index{avar-full=?@\texttt{avar-full=?}}
\end{equation*}

For convenience we have the function
\begin{alignat*}{2}
  &\texttt{(mk-avar \textsl{formula} <\textsl{index}> <\textsl{name}>)}.%
  \index{mk-avar@\texttt{mk-avar}}
\end{alignat*}
The formula is a required argument; the remaining arguments are
optional.  The default for the name string is \texttt{u}.  For display
we use
\begin{equation*}
  \texttt{(avar-to-string \textsl{avar})}.%
  \index{avar-to-string@\texttt{avar-to-string}}
\end{equation*}
We also
require that a function
\begin{align*}
  &\texttt{(formula-to-new-avar \textsl{formula})}
\end{align*}
is defined that returns an assumption variable of the requested
formula different from all assumption variables that have ever been
returned by any of the specified functions so far.

\section{Assumption constants}
\label{SS:AssConst}
An assumption constant appears in a proof, as an axiom, a theorem or a
global assumption.  Its formula is given as an
\inquotes{uninstantiated formula}\index{formula!uninstantiated}, where
only type and predicate variables can occur freely; these are
considered to be bound in the assumption constant.  An exception are
the elimination and greatest-fixed-point axioms, where the argument
variables of the (co)inductively defined predicate are formally free
in the uninstantiated formula; however, they are considered bound as
well.  In the proof the bound type variables are implicitly
instantiated by types, and the bound predicate variables by
comprehension terms (the arity of a comprehension term is the
type-instantiated arity of the corresponding predicate variable).
Since we do not have explicit type and predicate quantification in
formulas, the assumption constant contains these parts left implicit
in the proof, as \texttt{tpsubst}.

To normalize a proof we will first translate it into a term, then
normalize the term and finally translate the normal term back into a
proof.  To make this work, in case of axioms we pass to the term
appropriate \inquotes{reproduction data} to be used when after
normalization the axiom in question is to be reconstructed:
\texttt{all-formulas} for induction, a number \texttt{i} and an
inductively defined predicate constant \texttt{idpc} for its
\texttt{i}-th clause, \texttt{imp-formulas} for elimination, an
existential formula for existence introduction, and an existential
formula together with a conclusion for existence elimination.  During
normalization of the term these formulas are passed along.  When the
normal form is reached, we have to translate back into a proof.  Then
these reproduction data are used to reconstruct the axiom in question.

Internally, the formula of an assumption constant is split into an
uninstantiated formula where only type and predicate variables can
occur freely, and a substitution\index{substitution} for at most these
type and predicate variables.  The formula assumed by the constant is
the result of carrying out this substitution in the uninstantiated
formula.  Note that free variables may again occur in the assumed
formula.  For example, assumption constants axiomatizing the
existential quantifier will internally have the form
\begin{alignat*}{2}
  &\texttt{(aconst ExIntro $\forall_{\hat{x}^{\alpha}}(P\hat{x} \to
    \ex_{\hat{x}^{\alpha}} P\hat{x})$
    $(\alpha \mapsto \tau, P^{(\alpha)} \mapsto
    \set{\hat{z}^{\tau}}{A})$)},%
  \index{ExIntro@\texttt{ExIntro}}
  \\
  &\texttt{(aconst ExElim $\ex_{\hat{x}^{\alpha}} P\hat{x} \to
    \forall_{\hat{x}^{\alpha}}( P\hat{x}
    \to Q) \to Q$}
  \\
  &\qquad\qquad\qquad\qquad \texttt{$(\alpha \mapsto \tau, P^{(\alpha)}
    \mapsto \set{\hat{z}^{\tau}}{A}, Q \mapsto \set{}{B})$)}.%
  \index{ExElim@\texttt{ExElim}}
\end{alignat*}
Interface for general assumption constants:
\begin{alignat*}{2}
  &\texttt{(make-aconst \textsl{name} \textsl{kind} \textsl{uninst-formula}
    \textsl{tpsubst}}
  \\
  &\qquad \texttt{\textsl{repro-data1} \dots)}
  \index{make-aconst@\texttt{make-aconst}}
  &\quad& \text{constructor},
  \\
  &\texttt{(aconst-to-name \textsl{aconst})}
  \index{aconst-to-name@\texttt{aconst-to-name}}
  && \text{accessor},
  \\
  &\texttt{(aconst-to-kind \textsl{aconst})}
  \index{aconst-to-kind@\texttt{aconst-to-kind}}
  && \text{accessor},
  \\
  &\texttt{(aconst-to-uninst-formula \textsl{aconst})}
  \index{aconst-to-uninst-formula@\texttt{aconst-to-uninst-formula}}
  && \text{accessor},
  \\
  &\texttt{(aconst-to-tpsubst \textsl{aconst})}
  \index{aconst-to-tpsubst@\texttt{aconst-to-tpsubst}}
  && \text{accessor},
  \\
  &\texttt{(aconst-to-repro-data \textsl{aconst})}
  \index{aconst-to-repro-data@\texttt{aconst-to-repro-data}}
  && \text{accessor},
  \\
  &\texttt{(aconst-form?\ \textsl{x})}
  \index{aconst-form?@\texttt{aconst-form?}}
  && \text{test}.
\end{alignat*}
To construct the formula associated with an aconst, it is useful to
separate the instantiated formula from the variables to be
generalized.  The latter can be obtained as free variables in
inst-formula.  We therefore provide
\begin{alignat*}{2}
  &\texttt{(aconst-to-inst-formula \textsl{aconst})},%
  \index{aconst-to-inst-formula@\texttt{aconst-to-inst-formula}}
  \\
  &\texttt{(aconst-to-formula \textsl{aconst})}.%
  \index{aconst-to-formula@\texttt{aconst-to-formula}}
\end{alignat*}

The reproduction data can be computed from the name, the
uninstantiated formula, the \texttt{tpsubst} of an axiom, by
\begin{equation*}
  \texttt{(aconst-to-computed-repro-data \textsl{aconst})}.%
  \index{aconst-to-computed-repro-data@\texttt{aconst-to-computed-repro-data}}
\end{equation*}
However, to avoid recomputations we carry them along.

We also provide
\begin{alignat*}{2}
  &\texttt{(check-aconst \textsl{x})},%
  \index{check-aconst@\texttt{check-aconst}}
  \\
  &\texttt{(aconst=?\ \textsl{aconst1} \textsl{aconst2})},%
  \index{aconst=?@\texttt{aconst=?}}
  \\
  &\texttt{(aconst-without-rules?\ \textsl{aconst})},%
  \index{aconst-without-rules?@\texttt{aconst-without-rules?}}
  \\
  &\texttt{(aconst-to-string\ \textsl{aconst})}.%
  \index{aconst-to-string@\texttt{aconst-to-string}}
\end{alignat*}

\subsection{Axioms}
\label{SS:Axioms}
In $\TCF$ the only axioms are the clauses and the least- or
greatest-fixed-point axioms of inductively or coinductively defined
predicates, and equality axioms stating the (Leibniz) equality of
both sides of a computation rule.  However, as long as (i) we allow
free type parameters and (ii) make use of convenient abbreviations, we
need to allow corresponding additional axioms.

Recall that we require nullary constructors in every free algebra;
hence, it has a \inquotes{canonical inhabitant}%%
\index{canonical inhabitant}\index{inhabitant!canonical}.  Since we
allow free type parameters $\alpha$, we provide a constant
$\Inhab_{\alpha}$ intended to denote the canonical inhabitant of the
(unknown) type $\alpha$.  When finally we substitute a closed type
$\rho$ for the type parameter $\alpha$, we can give a value to
$\Inhab_{\rho}$ by adding an appropriate computation rule; an example
is
\begin{verbatim}
  (add-computation-rule (pt "(Inhab nat)") (pt "0")).
\end{verbatim}
Since for closed types the canonical inhabitant is total, we add an
axiom stating the totality of $\Inhab_{\alpha}$; it appears among the
initial theorems under the name InhabTotal%
\index{Inhabtotal@\texttt{Inhabtotal}}.

Recall that in order to make formal arguments with quantifiers
relativized to total objects more managable, we use a special sort of
variables intended to range over such objects only.  For example,
$\texttt{n}, \texttt{n0}, \texttt{n1}, \texttt{n2}, \dots$
range over total natural numbers, and $\verb#n^#, \verb#n^0#, \verb#n^1#,
\verb#n^2#, \dots$ are general variables.
Formally this is done by providing the abbreviating axioms
\begin{align*}
  \mathtt{AllTotalElim}\index{AllTotalElim@\texttt{AllTotalElim}} \colon
  &\forall_x P x \to \allnc_{\hat{x}}(\GTotal \hat{x} \to P \hat{x}),
  \\
  \mathtt{AllncTotalElim}\index{AllncTotalElim@\texttt{AllncTotalElim}} \colon
  &\allnc_x P x \to \allnc_{\hat{x}}(\GTotal \hat{x} \tonc P \hat{x})
\end{align*}
and their converses $\mathtt{AllTotalIntro}$%
\index{AllTotalIntro@\texttt{AllTotalIntro}} and $\mathtt{AllncTotalIntro}$%
\index{AllncTotalIntro@\texttt{AllncTotalIntro}}.  For the inductively
defined (decorated) existential quantifiers we have the abbreviating axioms
\begin{align*}
  \mathtt{ExDTotalElim}\index{ExDTotalElim@\texttt{ExDTotalElim}} \colon
  &\exd_x P x \to \exR_{\hat{x}}(\GTotal \hat{x} \landd P \hat{x}),
  \\
  \mathtt{ExLTotalElim}\index{ExLTotalElim@\texttt{ExLTotalElim}} \colon
  &\exL_x P x \to \exR_{\hat{x}}(P \hat{x} \landR \GTotal \hat{x}),
  \\
  \mathtt{ExRTotalElim}\index{ExRTotalElim@\texttt{ExRTotalElim}} \colon
  &\exR_x P x \to \exR_{\hat{x}}(\GTotal \hat{x} \landR P \hat{x}),
  \\
  \mathtt{ExUTotalElim}\index{ExUTotalElim@\texttt{ExUTotalElim}} \colon
  &\exnc_x P x \to \exnc_{\hat{x}}(\GTotal \hat{x} \landnc P \hat{x})
\end{align*}
and their converses $\mathtt{ExDTotalIntro}$%
\index{ExDTotalIntro@\texttt{ExDTotalIntro}}, $\mathtt{ExLTotalIntro}$%
\index{ExLTotalIntro@\texttt{ExLTotalIntro}}, $\mathtt{ExRTotalIntro}$%
\index{ExRTotalIntro@\texttt{ExRTotalIntro}} and $\mathtt{ExUTotalIntro}$%
\index{ExUTotalIntro@\texttt{ExUTotalIntro}}.  For the primitive
existential quantifier we have the abbreviating axiom
\begin{align*}
  \mathtt{ExTotalElim}\index{ExTotalElim@\texttt{ExTotalElim}} \colon
  &\ex_x P x \to \exR_{\hat{x}}(\GTotal \hat{x} \land P \hat{x})
\end{align*}
and its converse $\mathtt{ExTotalIntro}$%
\index{ExTotalIntro@\texttt{ExTotalIntro}}.

Recall the treatment in \ref{SS:IDPredConsts} of induction axioms,
viewed as elimination axioms for inductively defined totality
predicates.  We can also use the predicate constant $\GTotal$ instead,
which gives essentially the same axioms; this is what
\texttt{ind}\index{ind@\texttt{ind}} calls.  In more detail, the
command $(\mathtt{ind}\ \mathtt{r})$ expects a goal $A(r)$ with a
syntactically total term $r$.  Then the induction axiom
\begin{equation*}
  \forall_n(A(0) \to \forall_n(A(n) \to A(\suc n)) \to A(n))
\end{equation*}
is used with the term $r$ and two new goals for the base and the step
case.  Similarly, $(\mathtt{elim}\ \mathtt{Hyp})$ with a hypothesis
$\mathtt{Hyp} \colon \GTotal r$ and a goal $A(r)$ uses the elimination
axiom
\begin{equation*}
  \allnc_{\hat{n}}(\GTotal \hat{n} \to A(0) \to
  \allnc_{\hat{n}}(A(\hat{n}) \to A(\suc \hat{n})) \to A(\hat{n}))
\end{equation*}
with the term $r$, the hypothesis $\mathtt{Hyp}$ and two new goals for
the base and the step case.  The resulting proofs clearly can be
transformed into each other using the abbreviating axioms above
dealing with total variables.

We now spell out what in detail we mean by induction\index{induction}
over \emph{simultaneous} free algebras $\vec{\iota} = \mu_{\vec{\xi}} \,
\vec{\kappa}$, with goal formulas $\forall^{\iota_j}_{x_j}\, P_j x_j$.
For the constructor type
\begin{equation*}
  \kappa_i = \vec{\rho} \to (\vec{\sigma}_1 \to \xi_{j_1}) \to \dots \to
  (\vec{\sigma}_n \to \xi_{j_n}) \to \xi_j \in
  \constrtypes{\vec{\xi}}
\end{equation*}
we have the \emph{step formula}
\begin{align*}
  D_i \defeq \forall_{y_1^{\rho_1}, \dots,y_m^{\rho_m},
    y_{m+1}^{\vec{\sigma}_1 \to \iota_{j_1}},\dots,
    y_{m+n}^{\vec{\sigma}_n \to \iota_{j_n}}}(
  &\forall_{\vec{x}^{\vec{\sigma}_1}}\,
  P_{j_1}(y_{m+1}\vec{x}\,) \to \dots \to
  \\
  &\forall_{\vec{x}^{\vec{\sigma}_n}}\,
  P_{j_n}(y_{m+n}\vec{x}\,) \to
  \\
  &P_j(\constr_i^{\vec{\iota}}(\vec{y}\,))).
\end{align*}
Here $\vec{y} = y_1^{\rho_1},\dots,y_m^{\rho_m},
y_{m+1}^{\vec{\sigma}_1 \to \iota_{j_1}},\dots,
y_{m+n}^{\vec{\sigma}_n \to \iota_{j_n}}$ are the
\emph{components} of the object $\constr_i^{\vec{\iota}}(\vec{y}\,)$
of type $\iota_j$ under consideration, and
\begin{equation*}
  \forall_{\vec{x}^{\vec{\sigma}_1}}\,
  P_{j_1}(y_{m+1}\vec{x}\,), \dots,
  \forall_{\vec{x}^{\vec{\sigma}_n}}\,
  P_{j_n}(y_{m+n}\vec{x}\,)
\end{equation*}
are the hypotheses available when proving the induction step.  The
induction axiom $\ind_{\iota_j}$ then proves the formula
\begin{equation*}
  \ind_{\iota_j} \colon
  D_1 \to \dots \to D_k \to \forall^{\iota_j}_{x_j}\, P_j x_j.
\end{equation*}
We will often write $\ind_j$ for $\ind_{\iota_j}$.

An example is
\begin{alignat*}{2}
  &E_1 \to E_2 \to E_3 \to E_4 \to \forall_{x_1^{\typeTree}}
  P_1(x_1)
  \\
  \intertext{with} &E_1 \defeq P_1(\leaf),
  \\
  &E_2 \defeq \forall_{x^{\typeTlist}}(P_2 x \to
  P_1(\branch(x))),
  \\
  &E_3 \defeq P_2(\emp),
  \\
  &E_4 \defeq \forall_{x_1^{\typeTree},x_2^{\typeTlist}}(
  P_1(x_1) \to P_2(x_2) \to P_2(\tcons(x_1,x_2))).
\end{alignat*}
Here the fact that we deal with a simultaneous induction (over
\texttt{tree} and \texttt{tlist}), and that we prove a formula of the
form $\forall_{x^{\typeTree}} \dots$, can all be inferred from what is
given: the $\forall_{x^{\typeTree}} \dots$ is right there, and for
\texttt{tlist} we can look up the simultaneously defined algebras.  --
The internal representation is
\begin{alignat*}{2}
  &\texttt{(aconst Ind $E_1 \to E_2 \to E_3 \to E_4 \to
    \forall_{x_1^{\typeTree}} P_1(x_1)$}
  \\
  &\qquad \qquad \qquad
  \texttt{$(P_1 \mapsto \set{x_1^{\typeTree}}{A_1},
    P_2 \mapsto \set{x_2^{\typeTlist}}{A_2})$)}.%
  \index{Ind@\texttt{Ind}}
\end{alignat*}

A simplified version (without the recursive calls) of the induction
axiom is the cases axiom
\begin{alignat*}{2}
  &\texttt{(aconst Cases $E_1 \to E_2 \to
    \forall_{x_1^{\typeTree}} P_1(x_1)$
    $(P_1 \mapsto \set{x_1^{\typeTree}}{A_1})$)}%%
  \index{Cases@\texttt{Cases}}
  \\
  \intertext{with}
  &E_1 \defeq P_1(\leaf),
  \\
  &E_2 \defeq \forall_x^{\typeTlist} P_1(\branch(x)).
\end{alignat*}

The assumption constants corresponding to these axioms are generated by
\begin{alignat*}{2}
  &\texttt{(all-formulas-to-ind-aconst \textsl{all-formula1} \dots)}%%
  \index{all-formulas-to-ind-aconst@\texttt{all-formulas-to-ind-aconst}}
  &\quad&\text{for \texttt{Ind}\index{Ind@\texttt{Ind}}}
  \\
  &\texttt{(all-formula-to-cases-aconst \textsl{all-formula})}%%
  \index{all-formula-to-cases-aconst@\texttt{all-formula-to-cases-aconst}}
  &\quad&\text{for \texttt{Cases}\index{Cases@\texttt{Cases}}}.
\end{alignat*}
\texttt{all-formula-and-number-to-gind-aconst}%
\index{all-formula-and-number-to-gind-aconst@\texttt{all-for{\dots}-to-gind-aconst}}
takes an all-formula, a number $n$ for the arity of the measure
function and an optional argument for the name of a theorem proving
general induction from induction.  If \texttt{opt-gindthmname} is not
present, general induction is viewed as an axiom (and \texttt{GRec}
will be extracted).  Otherwise general induction is viewed as proved
from structural induction (and \texttt{Rec} is extracted).

We also provide
\begin{equation*}
  \texttt{(formula-to-efq-aconst \textsl{formula})}%
    \index{formula-to-efq-aconst@\texttt{formula-to-efq-aconst}},
\end{equation*}
which is conceived as a global assumption (of $\falsityF \to A$).  As
we have seen in section~\ref{SS:IDPredConsts}, it can be proved if the
formula $A$ has no stricly positive occurrences of predicate variables
(cf.\ section~\ref{SS:BasicProofConstr}).

For the introduction and elimination axioms
\texttt{ExIntro}\index{ExIntro@\texttt{ExIntro}} and
\texttt{ExElim}\index{ExElim@\texttt{ExElim}} of the primitive
existential quantifier%
\index{existential quantifier!primitive} we provide
\begin{align*}
  &\texttt{(ex-formula-to-ex-intro-aconst ex-formula)},%
  \index{ex-formula-to-ex-intro-aconst@\texttt{ex-formula-to-ex-intro-aconst}}
  \\
  &\texttt{(ex-formula-and-concl-to-ex-elim-aconst ex-formula concl)}.%
  \index{ex-formula-and-concl-to-ex-elim-aconst@\texttt{ex-for{\dots}-to-ex-elim-aconst}}
\end{align*}
To deal with inductively defined predicate constants, we need
additional axioms with names \inquotes{Intro}\index{Intro} and
\inquotes{Elim}\index{Elim}, which are generated by
\begin{align*}
  &\texttt{(number-and-idpredconst-to-intro-aconst i idpc)},%
  \index{number-and-idpredconst-to-intro-aconst@\texttt{number{\dots}-to-intro-aconst}}
  \\
  &\texttt{(imp-formulas-to-elim-aconst imp-formula1\ \dots)};%
  \index{imp-formulas-to-elim-aconst@\texttt{imp-formulas-to-elim-aconst}}
\end{align*}
here an \texttt{imp-formula} is expected to have the form $I \vec{x}
\to A$.  (For simultaneously inductively defined predicates we need
many such \texttt{imp-formulas}).

For coinductively defined predicate constants we need additional
axioms, with names \inquotes{Closure}\index{Closure} and
\inquotes{Gfp}\index{Gfp}.  They are generated by
\begin{align*}
  &\texttt{(coidpredconst-to-closure-aconst \textsl{coidpc})},%
  \index{coidpredconst-to-closure-aconst@\texttt{coidpredconst-to-closure-aconst}}
  \\
  &\texttt{(imp-formulas-to-gfp-aconst \textsl{imp-formula1} \dots)}.%
  \index{imp-formulas-to-gfp-aconst@\texttt{imp-formulas-to-gfp-aconst}}
\end{align*}

\subsection{Theorems}
\label{SS:Theorems}
A theorem is a special assumption constant.  We maintain an
association list \texttt{THEOREMS}\index{THEOREMS@\texttt{THEOREMS}}
assigning to every name of a theorem the assumption constant and its
proof.

Theorems are normally created after successfully completing an
interactive proof.  One may also create a theorem from an explicitly
given (closed) proof.  The command is
\begin{alignat*}{2}
  &\texttt{(add-theorem \textsl{string} .\ \textsl{opt-proof})}%%
  \index{add-theorem@\texttt{add-theorem}}
  &\quad&\text{or \texttt{save}\index{save@\texttt{save}}}.
\end{alignat*}
From a theorem name we can access its aconst, its (original) proof and
also its instantiated proof by
\begin{align*}
  &\texttt{(theorem-name-to-aconst \textsl{string})},%
  \index{theorem-name-to-aconst@\texttt{theorem-name-to-aconst}}
  \\
  &\texttt{(theorem-name-to-proof \textsl{string})},%
  \index{theorem-name-to-proof@\texttt{theorem-name-to-proof}}
  \\
  &\texttt{(theorem-name-to-inst-proof \textsl{string})}.%
  \index{theorem-name-to-inst-proof@\texttt{theorem-name-to-inst-proof}}
\end{align*}
We also provide
\begin{align*}
  &\texttt{(remove-theorem \textsl{string1} \dots)},%
  \index{remove-theorem@\texttt{remove-theorem}}
  \\
  &\texttt{(display-theorems \textsl{string1} \dots)},%
  \index{display-theorems@\texttt{display-theorems}}
  \\
  &\texttt{(pp \textsl{theorem-name})}\index{pp@\texttt{pp}}.
\end{align*}

Initially we provide the following theorems
\begin{alignat*}{2}
  &\atom(\hat{p}) \to \hat{p} = \true
  &\quad&\text{\texttt{AtomTrue}\index{AtomTrue@\texttt{AtomTrue}}}
  \\
  &(\atom(p) \to \falsityF) \to p = \false
  &&\text{\texttt{AtomFalse}\index{AtomFalse@\texttt{AtomFalse}}}
  \\
  \intertext{and for every finitary algebra, e.g., \texttt{nat}}
  &n=n
  &&\text{\texttt{=-Refl-nat}%
    \index{equalfinalg-Refl-nat@\texttt{=-Refl-nat}}}
  \\
  &\hat{n}_1 = \hat{n}_2 \to \hat{n}_2 = \hat{n}_1
  &&\text{\texttt{=-Sym-nat}%
    \index{equalfinalg-Sym-nat@\texttt{=-Sym-nat}}}%
  \\
  &\hat{n}_1 = \hat{n}_2 \to \hat{n}_2 = \hat{n}_3 \to \hat{n}_1 = \hat{n}_3
  &&\text{\texttt{=-Trans-nat}.%
    \index{equalfinalg-Trans-nat@\texttt{=-Trans-nat}}}
\end{alignat*}
Notice that the more general $(\atom(\hat{p}) \to \falsityF) \to
\hat{p} = \false$ does \emph{not} hold.  A counterexample is the empty
ideal of type $\typeB$.

Here are some other examples of theorems; we give the internal
representation as assumption constants, which show how the assumed
formula is split into an uninstantiated formula and a substitution, in
this case a type substitution $\alpha \mapsto \rho$, an object
substitution $f^{\alpha \to \typeN} \mapsto g^{\rho \to \typeN}$ and a
predicate variable substitution $P^{(\alpha)} \mapsto
\set{\hat{z}^{\rho}}{A}$.
\begin{alignat*}{2}
  &\texttt{(aconst Cvind-with-measure-11}
  \\
  &\qquad \qquad \qquad\texttt{$
    \forall_f^{\alpha \to \typeN}(
    \forall_{x^{\alpha}}(\forall_y(f y {<} f x  \to P y) \to
    P x) \to
    \forall_x P x)$}
  \\
  &\qquad \qquad \qquad
  \texttt{$(\alpha \mapsto \rho,
    f^{\alpha \to \typeN} \mapsto g^{\rho \to \typeN},
    P^{(\alpha)} \mapsto \set{\hat{z}^{\rho}}{A})$).}
  \index{Cvind-with-measure-11@\texttt{Cvind-with-measure-11}}
  \\
  &\texttt{(aconst Minpr-with-measure-l11}
  \\
  &\qquad \qquad \qquad\texttt{$
    \forall_{f^{\alpha \to \typeN}}(
    \excl_{x^{\alpha}} P x \to \excl_x(P x \landcl
    \forall_y(f y {<} f x  \to P y \to \falsum)))$}
  \\
  &\qquad \qquad \qquad
  \texttt{$(\alpha \mapsto \rho,
    f^{\alpha \to \typeN} \mapsto g^{\rho \to \typeN},
    P^{(\alpha)} \mapsto \set{\hat{z}^{\rho}}{A})$).}
  \index{Minpr-with-measure-l11@\texttt{Minpr-with-measure-l11}}
\end{alignat*}
Here $\excl$ is the classical existential quantifier defined by
$\excl_x A \defeq \forall_x(A \to \falsum) \to \falsum$ with the
logical form of falsity $\falsum$ (as opposed to the arithmetical form
$\falsityF$).  \texttt{l} indicates \inquotes{logic} (we have used the
logical form of falsity), the first \texttt{1} that we have one
predicate variable $P$, and the second that we quantify over
just one variable $x$.  Both theorems can easily be generalized to
more such parameters.

When dealing with classical logic it will be useful to have
\begin{alignat*}{2}
  &(P \to P_1) \to
  ((P \to \falsum) \to P_1) \to P_1
  &\quad&\text{\texttt{CasesLog}.%
    \index{CasesLog@\texttt{CasesLog}}}
\end{alignat*}
The proof uses the global assumption \texttt{StabLog} (see below) for
$P_1$; hence we cannot extract a term from it.

The assumption constants corresponding to these theorems are
generated by
\begin{alignat*}{2}
  &\texttt{(theorem-name-to-aconst \textsl{name})}.%
  \index{theorem-name-to-aconst@\texttt{theorem-name-to-aconst}}
\end{alignat*}

\subsection{Global assumptions}
\label{SS:GlobalAss}
A global assumption\index{global assumption} is a special assumption
constant.  It provides a proposition whose proof does not concern us
presently.  Global assumptions are added, removed and displayed by
\begin{align*}
  &\texttt{(add-global-assumption \textsl{name} \textsl{formula})}%
  \index{add-global-assumption@\texttt{add-global-assumption}}
  \quad \hbox{(abbreviated \texttt{aga}%
    \index{aga@\texttt{aga}})},
  \\
  &\texttt{(remove-global-assumption \textsl{string1} \dots)},%
  \index{remove-global-assumption@\texttt{remove-global-assumption}}
  \\
  &\texttt{(display-global-assumptions \textsl{string1} \dots)}.%
  \index{display-global-assumptions@\texttt{display-global-assumptions}}
\end{align*}

We initially supply global assumptions for ex-falso-quodlibet and
stability, both in logical and arithmetical form (for our two
forms of falsity).
\begin{alignat*}{2}
  &\falsum \to P
  &\quad&\text{\texttt{EfqLog},%
    \index{EfqLog@\texttt{EfqLog}}}
  \\
  &((P \to \falsum) \to \falsum) \to P
  &\quad&\text{\texttt{StabLog},%
    \index{StabLog@\texttt{StabLog}}}
  \\
  &\falsityF \to P
  &\quad&\text{\texttt{Efq},%
    \index{Efq@\texttt{Efq}}}
  \\
  &((P \to \falsityF) \to \falsityF) \to P
  &\quad&\text{\texttt{Stab}.%
    \index{Stab@\texttt{Stab}}}
\end{alignat*}
The assumption constants corresponding to these global assumptions are
generated by
\begin{alignat*}{2}
  &\texttt{(global-assumption-name-to-aconst \textsl{name})}.%
  \index{global-assumption-name-to-aconst@\texttt{global-ass{\dots}-name-to-aconst}}
\end{alignat*}

It is a practical problem to find existing theorems or global
assumptions relevant for the situation at hand.  To help searching
for those we provide
\begin{equation*}
  \texttt{(search-about \textsl{symbol-or-string} .\ \textsl{opt-strings})}.%
  \index{search-about@\texttt{search-about}}
\end{equation*}
It searches in \texttt{THEOREMS}\index{THEOREMS@\texttt{THEOREMS}} and
\texttt{GLOBAL-ASSUMPTIONS}%
\index{GLOBAL-ASSUMPTIONS@\texttt{GLOBAL-ASSUMPTIONS}} for all items
whose name contains each of the strings given, excluding the strings
Total Partial CompRule RewRule Sound.  It one wants to list all these as
well, take the symbol 'all as first argument.

\section{Proofs}
\label{S:Proof}
Proofs are built from assumption variables and assumption constants
(i.e., axioms, theorems and global assumptions) by the usual rules of
natural deduction, i.e., introduction and elimination rules for
implication, conjunction and universal quantification.  From a proof
we can read off its \emph{context}\index{context}, which is an ordered
list of object and assumption variables.

\subsection{Constructors and accessors}
We have constructors, accessors and tests for assumption variables
\begin{alignat*}{2}
  &\texttt{(make-proof-in-avar-form \textsl{avar})}
  \index{make-proof-in-avar-form@\texttt{make-proof-in-avar-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-avar-form-to-avar \textsl{proof})}
  \index{proof-in-avar-form-to-avar@\texttt{proof-in-avar-form-to-avar}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-avar-form?\ \textsl{proof})}
  \index{proof-in-avar-form?@\texttt{proof-in-avar-form?}}
  && \text{test},
\end{alignat*}
for assumption constants
\begin{alignat*}{2}
  &\texttt{(make-proof-in-aconst-form \textsl{aconst})}
  \index{make-proof-in-aconst-form@\texttt{make-proof-in-aconst-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-aconst-form-to-aconst \textsl{proof})}%%
  \index{proof-in-aconst-form-to-aconst@\texttt{proof-in-aconst-form-to-aconst}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-aconst-form?\ \textsl{proof})}
  \index{proof-in-aconst-form?@\texttt{proof-in-aconst-form?}}
  && \text{test},
\end{alignat*}
for implication introduction
\begin{alignat*}{2}
  &\texttt{(make-proof-in-imp-intro-form \textsl{avar} \textsl{proof})}%
  \index{make-proof-in-imp-intro-form@\texttt{make-proof-in-imp-intro-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-imp-intro-form-to-avar \textsl{proof})}
  \index{proof-in-imp-intro-form-to-avar@\texttt{proof-in-imp-intro-form-to-avar}}
  &&\text{accessor},
  \\
  &\texttt{(proof-in-imp-intro-form-to-kernel \textsl{proof})}
  \index{proof-in-imp-intro-form-to-kernel@\texttt{pr{\dots}-imp-intro-form-to-kernel}}
  &&\text{accessor},
  \\
  &\texttt{(proof-in-imp-intro-form?\ \textsl{proof})}
  \index{proof-in-imp-intro-form?@\texttt{proof-in-imp-intro-form?}}
  && \text{test},
\end{alignat*}
for implication elimination
\begin{alignat*}{2}
  &\texttt{(make-proof-in-imp-elim-form \textsl{proof1} \textsl{proof2})}%%
  \index{make-proof-in-imp-elim-form@\texttt{make-proof-in-imp-elim-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-imp-elim-form-to-op \textsl{proof})}
  \index{proof-in-imp-elim-form-to-op@\texttt{proof-in-imp-elim-form-to-op}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-imp-elim-form-to-arg \textsl{proof})}
  \index{proof-in-imp-elim-form-to-arg@\texttt{proof-in-imp-elim-form-to-arg}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-imp-elim-form?\ \textsl{proof})}
  \index{proof-in-imp-elim-form?@\texttt{proof-in-imp-elim-form?}}
  && \text{test},
\end{alignat*}
for and introduction
\begin{alignat*}{2}
  &\texttt{(make-proof-in-and-intro-form \textsl{proof1} \textsl{proof2})}
  \index{make-proof-in-and-intro-form@\texttt{make-proof-in-and-intro-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-and-intro-form-to-left \textsl{proof})}%%
  \index{proof-in-and-intro-form-to-left@\texttt{pr{\dots}and-intro-form-to-left}}
  &&\text{accessor},
  \\
  &\texttt{(proof-in-and-intro-form-to-right \textsl{proof})}
  \index{proof-in-and-intro-form-to-right@\texttt{pr{\dots}and-intro-form-to-right}}
  &&\text{accessor},
  \\
  &\texttt{(proof-in-and-intro-form?\ \textsl{proof})}
  \index{proof-in-and-intro-form?@\texttt{proof-in-and-intro-form?}}
  && \text{test},
\end{alignat*}
for and elimination
\begin{alignat*}{2}
  &\texttt{(make-proof-in-and-elim-left-form \textsl{proof})}
  \index{make-proof-in-and-elim-left-form@\texttt{make-proof-in-and-elim-l{\dots}}}
  &\quad& \text{constructor},
  \\
  &\texttt{(make-proof-in-and-elim-right-form \textsl{proof})}
  \index{make-proof-in-and-elim-right-form@\texttt{make-proof-in-and-elim-r{\dots}}}
  && \text{constructor},
  \\
  &\texttt{(proof-in-and-elim-left-form-to-kernel \textsl{proof})}
  \index{proof-in-and-elim-left-form-to-kernel@\texttt{proof-in-and-elim{\dots}}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-and-elim-right-form-to-kernel \textsl{proof})}%%
  \index{proof-in-and-elim-right-form-to-kernel@\texttt{proof-in-and-elim{\dots}}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-and-elim-left-form?\ \textsl{proof})}
  \index{proof-in-and-elim-left-form?@\texttt{proof-in-and-elim-left-form?}}
  && \text{test},
  \\
  &\texttt{(proof-in-and-elim-right-form?\ \textsl{proof})}
  \index{proof-in-and-elim-right-form?@\texttt{proof-in-and-elim-right-form?}}
  && \text{test},
\end{alignat*}
for all introduction
\begin{alignat*}{2}
  &\texttt{(make-proof-in-all-intro-form \textsl{var} \textsl{proof})}
  \index{make-proof-in-all-intro-form@\texttt{make-proof-in-all-intro-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-all-intro-form-to-var \textsl{proof})}
  \index{proof-in-all-intro-form-to-var@\texttt{pr{\dots}all-intro-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-all-intro-form-to-kernel \textsl{proof})}
  \index{proof-in-all-intro-form-to-kernel@\texttt{pr{\dots}all-intro-form-to-kernel}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-all-intro-form?\ \textsl{proof})}
  \index{proof-in-all-intro-form?@\texttt{proof-in-all-intro-form?}}
  && \text{test},
\end{alignat*}
for all elimination
\begin{alignat*}{2}
  &\texttt{(make-proof-in-all-elim-form \textsl{proof} \textsl{term})}%%
  \index{make-proof-in-all-elim-form@\texttt{make-proof-in-all-elim-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-all-elim-form-to-op \textsl{proof})}
  \index{proof-in-all-elim-form-to-op@\texttt{proof-in-all-elim-form-to-op}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-all-elim-form-to-arg \textsl{proof})}
  \index{proof-in-all-elim-form-to-arg@\texttt{proof-in-all-elim-form-to-arg}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-all-elim-form?\ \textsl{proof})}
  \index{proof-in-all-elim-form?@\texttt{proof-in-all-elim-form?}}
  && \text{test},
\end{alignat*}
and for \texttt{cases}-constructs
\begin{alignat*}{2}
  &\texttt{(make-proof-in-cases-form \textsl{test} \textsl{alt1} \dots)}
  \index{make-proof-in-cases-form@\texttt{make-proof-in-cases-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-cases-form-to-test \textsl{proof})}
  \index{proof-in-cases-form-to-test@\texttt{proof-in-cases-form-to-test}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-cases-form-to-alts \textsl{proof})}
  \index{proof-in-cases-form-to-alts@\texttt{proof-in-cases-form-to-alts}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-cases-form-to-rest \textsl{proof})}
  \index{proof-in-cases-form-to-rest@\texttt{proof-in-cases-form-to-rest}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-cases-form?\ \textsl{proof})}
  \index{proof-in-cases-form?@\texttt{proof-in-cases-form?}}
  && \text{test}.
\end{alignat*}
It is convenient to have more general introduction and elimination
operators that take arbitrary many arguments.  The former works for
implication-introduction and all-introduction, and the latter for
implication-elimination, and-elimination and all-elimination.
\begin{alignat*}{2}
  &\texttt{(mk-proof-in-intro-form \textsl{x1} \dots\ \textsl{proof})},%
  \index{mk-proof-in-intro-form@\texttt{mk-proof-in-intro-form}}
  \\
  &\texttt{(mk-proof-in-elim-form \textsl{proof} \textsl{arg1} \dots)}.%
  \index{mk-proof-in-elim-form@\texttt{mk-proof-in-elim-form}}
\end{alignat*}
The result of \texttt{(mk-proof-in-intro-form \textsl{x1} \dots\
  \textsl{proof})} is formed from proof by first abstracting
\textsl{x1}, then \textsl{x2} and so on.  Here \textsl{x1},
\textsl{x2} \dots can be assumption or object variables.  Further
related functions are
\begin{alignat*}{2}
  &\texttt{(proof-in-intro-form-to-kernel-and-vars \textsl{proof})},%
  \index{proof-in-intro-form-to-kernel-and-vars@\texttt{proof-in-intro-form-to{\dots}}}
  \\
  &\texttt{(proof-in-elim-form-to-final-op \textsl{proof})},%
  \index{proof-in-elim-form-to-final-op@\texttt{pr{\dots}elim-form-to-final-op}}
  \\
  &\texttt{(proof-in-elim-form-to-args \textsl{proof}).}%%
  \index{proof-in-elim-form-to-args@\texttt{proof-in-elim-form-to-args}}
\end{alignat*}

We also provide
\begin{alignat*}{2}
  &\texttt{(mk-proof-in-and-intro-form \textsl{proof} \textsl{proof1} \dots)}.%
  \index{mk-proof-in-and-intro-form@\texttt{mk-proof-in-and-intro-form}}
\end{alignat*}

In our setup there are axioms rather than rules for the existential
quantifier.  However, sometimes it is useful to construct proofs as if
an existence introduction rule would be present; internally then an
existence introduction axiom is used.
\begin{alignat*}{2}
  &\texttt{(make-proof-in-ex-intro-form \textsl{term} \textsl{ex-formula}
    \textsl{proof-of-inst})},%
  \index{make-proof-in-ex-intro-form@\texttt{make-proof-in-ex-intro-form}}
  \\
  &\texttt{(mk-proof-in-ex-intro-form .\ }
  \\
  &\quad\texttt{\textsl{terms-and-ex-formula-and-proof-of-inst})}.%
  \index{mk-proof-in-ex-intro-form@\texttt{mk-proof-in-ex-intro-form}}
\end{alignat*}

For the non-computational connectives $\tonc$ and $\allnc$ (cf.\
\ref{SS:FormulaConstr}) we need similar constructors, accessors and tests.
For n.c.\ implication introduction
\begin{alignat*}{2}
  &\texttt{(make-proof-in-impnc-intro-form \textsl{avar} \textsl{proof})}
  \index{make-proof-in-impnc-intro-form@\texttt{make-proof-in-impnc-intro-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-impnc-intro-form-to-avar \textsl{proof})}
  \index{proof-in-impnc-intro-form-to-avar@\texttt{proof-in-impnc-intro-form-to-avar}}
  &&\text{accessor},
  \\
  &\texttt{(proof-in-impnc-intro-form-to-kernel \textsl{proof})}
  \index{proof-in-impnc-intro-form-to-kernel@\texttt{pr{\dots}-impnc-intro-form-to-kernel}}
  &&\text{accessor},
  \\
  &\texttt{(proof-in-impnc-intro-form?\ \textsl{proof})}
  \index{proof-in-impnc-intro-form?@\texttt{proof-in-impnc-intro-form?}}
  && \text{test},
\end{alignat*}
for n.c.\ implication elimination
\begin{alignat*}{2}
  &\texttt{(make-proof-in-impnc-elim-form \textsl{proof1} \textsl{proof2})}%%
  \index{make-proof-in-impnc-elim-form@\texttt{make-proof-in-impnc-elim-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-impnc-elim-form-to-op \textsl{proof})}
  \index{proof-in-impnc-elim-form-to-op@\texttt{proof-in-impnc-elim-form-to-op}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-impnc-elim-form-to-arg \textsl{proof})}
  \index{proof-in-impnc-elim-form-to-arg@\texttt{proof-in-impnc-elim-form-to-arg}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-impnc-elim-form?\ \textsl{proof})}
  \index{proof-in-impnc-elim-form?@\texttt{proof-in-impnc-elim-form?}}
  && \text{test},
\end{alignat*}
for n.c.\ all introduction
\begin{alignat*}{2}
  &\texttt{(make-proof-in-allnc-intro-form \textsl{var} \textsl{proof})}
  \index{make-proof-in-allnc-intro-form@\texttt{make-proof-in-allnc-intro-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-allnc-intro-form-to-var \textsl{proof})}
  \index{proof-in-allnc-intro-form-to-var@\texttt{pr{\dots}allnc-intro-form-to-var}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-allnc-intro-form-to-kernel \textsl{proof})}
  \index{proof-in-allnc-intro-form-to-kernel@\texttt{pr{\dots}allnc-intro-form-to-kernel}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-allnc-intro-form?\ \textsl{proof})}
  \index{proof-in-allnc-intro-form?@\texttt{proof-in-allnc-intro-form?}}
  && \text{test},
\end{alignat*}
for n.c.\ all elimination
\begin{alignat*}{2}
  &\texttt{(make-proof-in-allnc-elim-form \textsl{proof} \textsl{term})}%%
  \index{make-proof-in-allnc-elim-form@\texttt{make-proof-in-allnc-elim-form}}
  &\quad& \text{constructor},
  \\
  &\texttt{(proof-in-allnc-elim-form-to-op \textsl{proof})}
  \index{proof-in-allnc-elim-form-to-op@\texttt{proof-in-allnc-elim-form-to-op}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-allnc-elim-form-to-arg \textsl{proof})}
  \index{proof-in-allnc-elim-form-to-arg@\texttt{proof-in-allnc-elim-form-to-arg}}
  && \text{accessor},
  \\
  &\texttt{(proof-in-allnc-elim-form?\ \textsl{proof})}
  \index{proof-in-allnc-elim-form?@\texttt{proof-in-allnc-elim-form?}}
  && \text{test}.
\end{alignat*}
Again it is convenient to have
\begin{alignat*}{2}
  &\texttt{(mk-proof-in-nc-intro-form \textsl{x1} \dots\ \textsl{proof})}.%
  \index{mk-proof-in-nc-intro-form@\texttt{mk-proof-in-nc-intro-form}}
\end{alignat*}

We also provide
\begin{alignat*}{2}
  &\texttt{(mk-proof-in-cr-nc-intro-form \textsl{x}\ .\ \textsl{rest})}.%
  \index{mk-proof-in-cr-nc-intro-form@\texttt{mk-proof-in-cr-nc-intro-form}}
\end{alignat*}
Here \texttt{x} is obtained from a list of premises and variables
where each element is followed by an indicator for \texttt{nc} or
\texttt{cr} (true means \texttt{nc}).  Moreover we need
\begin{alignat*}{2}
  &\texttt{(proof?\ \textsl{x})},%
  \index{proof?@\texttt{proof?}}
  \\
  &\texttt{(proof=?\ \textsl{proof1} \textsl{proof2})},%
  \index{proof=?@\texttt{proof=?}}
  \\
  &\texttt{(proofs=?\ \textsl{proofs1} \textsl{proofs2})},%
  \index{proofs=?@\texttt{proofs=?}}
  \\
  &\texttt{(proof-to-formula \textsl{proof})},%
  \index{proof-to-formula@\texttt{proof-to-formula}}
  \\
  &\texttt{(proof-to-context \textsl{proof})},%
  \index{proof-to-context@\texttt{proof-to-context}}
  \\
  &\texttt{(proof-to-cvars \textsl{proof})},%
  \index{proof-to-cvars@\texttt{proof-to-cvars}}
  \\
  &\texttt{(proof-to-free \textsl{proof})},%
  \index{proof-to-free@\texttt{proof-to-free}}
  \\
  &\texttt{(proof-to-tvars \textsl{proof})},%
  \index{proof-to-tvars@\texttt{proof-to-tvars}}
  \\
  &\texttt{(proof-to-pvars \textsl{proof})},%
  \index{proof-to-pvars@\texttt{proof-to-pvars}}
  \\
  &\texttt{(proof-to-free-avars \textsl{proof})},%
  \index{proof-to-free-avars@\texttt{proof-to-free-avars}}
  \\
  &\texttt{(proof-to-bound-avars \textsl{proof})},%
  \index{proof-to-bound-avars@\texttt{proof-to-bound-avars}}
  \\
  &\texttt{(proof-to-free-and-bound-avars-wrt
    \textsl{avar-eq} \textsl{proof})},%
  \index{proof-to-free-and-bound-avars-wrt@\texttt{proof-to-free-and-bound-avars-wrt}}
  \\
  &\texttt{(proof-to-free-and-bound-avars \textsl{proof})},%
  \index{proof-to-free-and-bound-avars@\texttt{proof-to-free-and-bound-avars}}
  \\
  &\texttt{(proof-respects-avar-convention?\ \textsl{proof})},%
  \index{proof-respects-avar-convention?@\texttt{proof-respects-avar-convention?}}
  \\
  &\texttt{(proof-to-aconsts-without-rules \textsl{proof})},%
  \index{proof-to-aconsts-without-rules@\texttt{proof-to-aconsts-without-rules}}
  \\
  &\texttt{(proof-to-aconsts \textsl{proof})},%
  \index{proof-to-aconsts@\texttt{proof-to-aconsts}}
  \\
  &\texttt{(proof-to-global-assumptions \textsl{proof})}.%
  \index{proof-to-global-assumptions@\texttt{proof-to-global-assumptions}}
\end{alignat*}
\texttt{proof-to-cvars} computes the computational variables of the
proof, which are the ones the extra variable condition for $\allnc$
refers to.

To work with contexts we provide
\begin{alignat*}{2}
  &\texttt{(context-to-vars\ \textsl{context})},%
  \index{context-to-vars@\texttt{context-to-vars}}
  \\
  &\texttt{(context-to-avars\ \textsl{context})},%
  \index{context-to-avars@\texttt{context-to-avars}}
  \\
  &\texttt{(context=?\ \textsl{context1} \textsl{context2})},%
  \index{context=?@\texttt{context=?}}
  \\
  &\texttt{(pp-context\ \textsl{context})}.%
  \index{pp-context@\texttt{pp-context}}
\end{alignat*}

We can also convert the name of a theorem or a global assumption into
a proof consisting of just the corresponding assumption constant:
\begin{equation*}
  \texttt{(thm-or-ga-name-to-proof \textsl{name})}.%
  \index{thm-or-ga-name-to-proof@\texttt{thm-or-ga-name-to-proof}}
\end{equation*}

\subsubsection*{Decorating proofs}
%% \label{SSS:Deco}
In this section we are interested in \inquotes{fine-tuning} the
computational content of proofs, by inserting decorations
(cf.\ \ref{SS:Deco}).  Here is an example (due to
Constable\index{Constable}) of why this is of interest.  Suppose that
in a proof $M$ of a formula $C$ we have made use of a case distinction
based on an auxiliary lemma stating a disjunction, say $L \colon A
\lor B$.  Then the extract $\extrTer{M}$ will contain the extract
$\extrTer{L}$ of the proof of the auxiliary lemma, which may be large.
Now suppose further that in the proof $M$ of $C$, the only
computationally relevant use of the lemma was which one of the two
alternatives holds true, $A$ or $B$.  We can express this fact by
using a weakened form of the lemma instead: $L' \colon A \loru B$.
Since the extract $\extrTer{L'}$ is a boolean, the extract of the
modified proof has been \inquotes{purified} in the sense that the
(possibly large) extract $\extrTer{L}$ has disappeared.

We consider the question of \inquotes{optimal} decorations of proofs:
suppose we are given an undecorated proof, and a decoration of its end
formula.  The task then is to find a decoration of the whole proof
(including a further decoration of its end formula) in such a way that
any other decoration \inquotes{extends} this one.  Here
\inquotes{extends} just means that some connectives have been changed
into their more informative versions, disregarding polarities.  We
show that such an optimal decoration exists, and give an algorithm to
construct it.

We denote the \emph{sequent} of a proof $M$ by $\Seq(M)$; it consists
of its \emph{context} and \emph{end formula}.

The \emph{proof pattern}%
\index{proof pattern} $\UP(M)$ of a proof $M$ is the result of marking
in c.r.\ formulas of $M$ (i.e., those not above a c.i.\ formula) all
occurrences of implications and universal quantifiers as
non-computational, except the \inquotes{uninstantiated} formulas of
axioms and theorems.  For instance, the induction axiom for $\typeN$
consists of the uninstantiated formula $\allc_n (P 0 \toc \allc_n(P n
\toc P(\suc n)) \toc P n^{\typeN} )$ with a unary predicate variable
$P$ and a predicate substitution $P \mapsto \set {x} {A(x)}$.  Notice
that a proof pattern in most cases is not a correct proof, because at
axioms formulas may not fit.

We say that a formula $D$ \emph{extends} $C$ if $D$ is obtained from
$C$ by changing some (possibly zero) of its occurrences of
non-computational implications and universal quantifiers into their
computational variants $\toc$ and $\allc$.

A proof $N$ \emph{extends} $M$ if (i) $N$ and $M$ are the same up to
variants of implications and universal quantifiers in their formulas,
and (ii) every c.r.\ formula of $M$ is extended by the corresponding
one in $N$.  Every proof $M$ whose proof pattern $\UP(M)$ is $U$ is
called a \emph{decoration}\index{decoration!of proofs} of $U$.

Notice that if a proof $N$ extends another one $M$, then
$\FV(\extrTer{N})$ is essentially (that is, up to extensions of
assumption formulas) a superset of $\FV(\extrTer{M})$.  This can be
proven by induction on $N$.

We assume that every axiom has the property that for every extension
of its formula we can find a further extension which is an instance of
an axiom, and which is the least one under all further extensions that
are instances of axioms.  This property clearly holds for axioms whose
uninstantiated formula only has the decorated $\toc$ and $\allc$, for
instance induction.  However, in $\allc_n (A(0) \toc \allc_n(A(n) \toc
A(\suc n)) \toc A(n^{\typeN}))$ the given extension of the four $A$'s
might be different.  One needs to pick their \inquotes{least upper
  bound} as further extension.  To make this assumption true for the
other (introduction and elimination) axioms we simply add all their
extensions as axioms, if necessary.

One can define a \emph{decoration algorithm}%
\index{decoration!algorithm} \cite{RatiuSchwichtenberg10}, assigning
to every proof pattern $U$ and every extension of its sequent an
\inquotes{optimal} decoration $M_{\infty}$ of $U$, which further
extends the given extension of its sequent.

\begin{theorem*}
  Under the assumption above, for every proof pattern $U$ and every
  extension of its sequent $\Seq(U)$ we can find a decoration
  $M_{\infty}$ of $U$ such that
  \begin{enumeratea}
  \item $\Seq(M_{\infty})$ extends the given extension of $\Seq(U)$,
    and
  \item $M_{\infty}$ is \emph{optimal} in the sense that any other
    decoration $M$ of $U$ whose sequent $\Seq(M)$ extends the given
    extension of $\Seq(U)$ has the property that $M$ also extends
    $M_{\infty}$.
  \end{enumeratea}
\end{theorem*}

The main function for decorating is
\begin{align*}
  \texttt{decorate proof .\ \textsl{opt-decfla-and-name-and-altname}}
  \index{decorate@\texttt{decorate}}
\end{align*}
The default case for \texttt{opt-decfla} is the formula of the proof.
If \texttt{opt-decfla} is present, it must be a decoration variant of
the formula of the proof.  If the optional argument
\texttt{name-and-altname} is present, then in every recursive call it
is checked whether (1) the present proof is an application of the
assumption constant \texttt{op} with \texttt{name} to some
\texttt{args}, (2) \texttt{op} applied to \texttt{args} proves an
extension of \texttt{decfla}, and (3) \texttt{altop} applied to
\texttt{args} and some of \texttt{decavars} is between \texttt{op}
applied to \texttt{args} and \texttt{decfla} w.r.t. extension.  If so,
a proof based on \texttt{altop} is returned, else one carries on.

An important auxiliary function is
\texttt{proof-to-ppat}\index{proof-to-ppat@\texttt{proof-to-ppat}},
used to transform a proof into its proof pattern.  It turns every
$\to$, $\forall$ formula in the given proof into an $\tonc$, $\allnc$
formula, including the parts of an assumption constant which come from
its uninstatiated formula.  It does not touch the c.i.\ parts of the
proof, i.e., those which are above a c.i.\ formula.  Recall that the
proof pattern \texttt{ppat} is in general not a proof.

We illustrate the effects of decoration on a simple example (from
\cite{RatiuSchwichtenberg10}) involving implications.  Consider $A \to
B \to A$ with the trivial proof $M \defeq \lambda_{u_1}^A
\lambda_{u_2}^B u_1$.  Clearly the second implication has no
computational significance.  We apply the decoration algorithm and
specify as extension of $\Seq(\UP(M))$ the formula $A \tonc B \tonc
A$.  The algorithm detects that the first implication needs to be
decorated, since the abstracted assumption variable is computational.
Since the second implication can be left undecorated, a proof of $A
\toc B \tonc A$ is constructed from $M$.

A similar phenomenon occurs for $A \landd B \to B$.  Let $M$ be its
proof and $U \defeq \UP(M)$ its proof pattern.  When given the
extension $A \landnc B \tonc B$ for $\Seq(U)$, the decoration
algorithm constructs a correct proof of $A \landR B \toc B$.

\subsection{Normalization by evaluation}
Normalization of proofs will be done by reduction to normalization of
terms.  (1) Construct a term from the proof.  To do this properly,
create for every free avar in the given proof a new variable whose
type comes from the formula of the avar; store this information.  Note
that in this construction one also has to create new variables for the
bound avars.  Similary to avars we have to treat assumption constants
which are not axioms, i.e., theorems or global assumptions.  (2)
Normalize the resulting term.  (3) Reconstruct a normal proof from
this term, the end formula and the stored information.  -- The critical
variables are carried along for efficiency reasons.

To assign recursion constants to induction constants, we need to
associate type variables with predicate variables, in such a way that
we can later refer to this assignment.  Therefore we carry along a
procedure \texttt{pvar-to-tvar} which remembers the assignment done so far
(cf.\ \texttt{make-rename}).

Due to our distinction between general variables $\verb#x^0#,
\verb#x^1#, \verb#x^2#, \dots$ and variables $\texttt{x0},
\texttt{x1}, \texttt{x2}, \dots$ intended to range over total objects
only, $\eta$-conversion of proofs cannot be done via reduction to
$\eta$-conversion of terms.  To see this, consider the proof
\begin{equation*}
  \AxiomC{$\forall_{\hat{x}} P \hat{x}$}
  \AxiomC{$x$}
  \BinaryInfC{$P x$}
  \UnaryInfC{$\forall_x P x$}
  \UnaryInfC{$\forall_{\hat{x}} P \hat{x} \to \forall_x P x$}
  \DisplayProof
\end{equation*}
The proof term is $\lambda_u \lambda_x(u x)$.  If we $\eta$-normalize
this to $\lambda_u u$, the proven formula would be all
$\forall_{\hat{x}} P \hat{x} \to \forall_{\hat{x}} P \hat{x}$.
Therefore we split \texttt{nbe-normalize-proof} into
\texttt{nbe-normalize-proof-without-eta} and \texttt{proof-to-eta-nf}.

Moreover, for a full normalization of proofs (including permutative
conversions) we need a preprocessing step that $\eta$-expands each
ex-elim axiom such that the conclusion is atomic or existential.

We need the following functions.
\begin{alignat*}{2}
  &\texttt{(proof-and-genavar-var-alist-to-pterm \textsl{pvar-to-tvar}
    \textsl{proof})}
  \\
  &\texttt{(npterm-and-var-genavar-alist-and-formula-to-proof}
  \\
  &\quad \texttt{\textsl{npterm} \textsl{var-genavar-alist} \textsl{crit}
    \textsl{formula})}
  \\
  &\texttt{(elim-npterm-and-var-genavar-alist-to-proof}
  \\
  &\quad \texttt{\textsl{npterm} \textsl{var-genavar-alist} \textsl{crit})}.
\end{alignat*}

Normalization of proofs can be made more efficient if we are
interested in extraction only.  To this end we can provide an
\inquotes{extraction-flag}, indicating whether this is the case.  If
so, we can disregard (maximal) parts of the proof without
computational content.  Accordingly we have
\begin{align*}
  &\texttt{(nbe-normalize-proof \textsl{proof})},%
  \index{nbe-normalize-proof@\texttt{nbe-normalize-proof}}
  \\
  &\texttt{(nbe-normalize-proof-for-extraction \textsl{proof})}%
  \index{nbe-normalize-proof-for-extraction@\texttt{nbe-normalize-proof-for-extraction}}
\end{align*}
abbreviated \texttt{np}\index{np@\texttt{np}} and \texttt{npe}%
\index{npe@\texttt{npe}}, respectively.

Finally we consider some proof transformations%
\index{proof transformation} (Prawitz' simplification conversions,
removal of predecided assumption variables, removal of predecided
if-theorems, generalized pruning).

Simplification conversions (Prawitz \cite{Prawitz65}) make use of
the concept of a permutative assumption constant.  It is checked
whether one side-proof-kernel has no free occurrence of any assumption
variable bound in this side-proof.  The corresponding function is
\begin{equation*}
  \texttt{(normalize-proof-simp \textsl{proof})}.%
  \index{normalize-proof-simp@\texttt{normalize-proof-simp}}
\end{equation*}
The function
\begin{equation*}
  \texttt{(proof-to-proof-without-predecided-avars \textsl{proof})}%
  \index{proof-to-proof-without-predecided-avars@\texttt{proof-to-proof-without-predec..-avars}}
\end{equation*}
removes dependencies on assumption variables, and in this way helps to
make \texttt{normalize-proof-simp} useful.

It can be also useful to remove predecided If's, including those with
True or False as boolean arguments.  This is particularly important in
the context of \inquotes{pruning} (cf.\ Chiarabini \cite{Chiarabini09}).
We have
\begin{align*}
  &\texttt{(prune \textsl{proof})},%
  \index{prune@\texttt{prune}}
  \\
  &\texttt{(remove-predecided-if-theorems \textsl{proof})}.%
  \index{remove-predecided-if-theorems@\texttt{remove-predecided-if-theorems}}
\end{align*}

For tests it is useful to have a level-wise decomposition of proofs
into subproofs: one level transforms a proof $\lambda_{\vec{u}}v
\vec{M}$ into the list $(v, M_1, \dots, M_n)$.  We provide
\begin{align*}
  &\texttt{(proof-to-parts \textsl{proof} .\ \textsl{opt-level})},%
  \index{proof-to-parts@\texttt{proof-to-parts}}
  \\
  &\texttt{(proof-to-proof-parts \textsl{proof})},%
  \index{proof-to-proof-parts@\texttt{proof-to-proof-parts}}
  \\
  &\texttt{(proof-to-depth \textsl{proof})}.%
  \index{proof-to-depth@\texttt{proof-to-depth}}
\end{align*}
It can also be useful to do normalization by hand, including
$\beta$-conversion and idpredconst-elim-intro conversion.  The latter
uses for nested idpredconstants
\begin{align*}
  &\texttt{(formula-and-psubsts-to-mon-proof \textsl{proof})}.%
  \index{formula-and-psubsts-to-mon-proof@\texttt{formula-and-psubsts-to-mon-proof}}
\end{align*}
An elim-intro redex occurs when an elim aconst is applied to terms and
the result of applying an intro-aconst to terms and an idpc-proof.
\begin{align*}
  &\texttt{(proof-to-one-step-idpredconst-elim-intro-reduct \textsl{proof})},%
  \index{proof-to-one-step-idpredconst-elim-intro-reduct@\texttt{proof-to-one-step-idp..-elim-intro-reduct}}
  \\
  &\texttt{(proof-to-one-step-reduct \textsl{proof})},%
  \index{proof-to-one-step-reduct@\texttt{proof-to-one-step-reduct}}
  \\
  &\texttt{(proof-to-normal-form \textsl{proof})},%
  \index{proof-to-normal-form@\texttt{proof-to-normal-form}}
  \\
  &\texttt{(proof-to-length \textsl{proof})}.%
  \index{proof-to-length@\texttt{proof-to-length}}
\end{align*}

\subsection{Substitution}
In a proof we can substitute\index{substitution}
\begin{enumeratei}
\item types for type variables (by a type variable substitution
  \texttt{tsubst}),
\item terms for variables (by a substitution \texttt{subst}),
\item comprehension terms for predicate variables (by a predicate
  variable substitution \texttt{psubst}), and
\item proofs for assumption variables (by a assumption variable
  substitution \texttt{asubst}).
\end{enumeratei}
All these substitutions can be packed together, as an argument
\texttt{topasubst} for \texttt{proof-substitute}.  It is assumed that
\texttt{topasubst} is admissible\index{substitution!admissible},
in the sense of section~\ref{SS:GenSubst}.
\begin{align*}
  &\texttt{(aconst-substitute \textsl{aconst} \textsl{topsubst})},%
  \index{aconst-substitute@\texttt{aconst-substitute}}
  \\
  &\texttt{(proof-substitute \textsl{proof} \textsl{topasubst})}.%
  \index{proof-substitute@\texttt{proof-substitute}}
\end{align*}
If we want to substitute for a single variable only (which can be a
type-, an object-, a predicate- or an assumption-variable), then
we can use
\begin{alignat*}{2}
  &\texttt{(proof-subst \textsl{proof} \textsl{arg} \textsl{val})}.%
  \index{proof-subst@\texttt{proof-subst}}
\end{alignat*}

The procedure \texttt{expand-theorems} expects a proof and a test
whether a string denotes a theorem to be replaced by its proof.  The
result is the (normally quite long) proof obtained by replacing the
theorems by their saved proofs.  If \texttt{opt-name-test} is
provided, it only expands (non-recursively) the theorems passing the
test.  \texttt{expand-thm}\index{expand-thm@\texttt{expand-thm}}
expands a single theorem given by its name and
\texttt{expand-theorems-with-positive-content}
does what its name says.
\begin{alignat*}{2}
  &\texttt{(expand-theorems \textsl{proof} .\ \textsl{opt-name-test})},%
  \index{expand-theorems@\texttt{expand-theorems}}
  \\
  &\texttt{(expand-thm \textsl{proof} \textsl{thm-name})},%
  \index{expand-thm@\texttt{expand-thm}}
  \\
  &\texttt{(expand-theorems-with-positive-content \textsl{proof})}.%
  \index{expand-theorems-with-positive-content@\texttt{expand-theorems-with-pos..-content}}
\end{alignat*}

\subsection{Display}
There are many ways to display a proof.  We normally use
\texttt{display-proof} for a linear representation, showing the
formulas and the rules used.  We also provide a (hopefully) readable
type-free lambda expression via \texttt{proof-to-expr}, and can add
useful information with one of \texttt{proof-to-expr-with-formulas},
\texttt{proof-to-expr-with-aconsts}.  In case the optional proof
argument is not present, the current proof is taken instead.
\begin{alignat*}{2}
  &\texttt{(display-proof .\ \textsl{opt-proof})}
  \index{display-proof@\texttt{display-proof}}
  &\quad& \text{abbreviated \texttt{dp}\index{dp@\texttt{dp}}},
  \\
  &\texttt{(display-normalized-proof .\ \textsl{opt-proof})}
  \index{display-normalized-proof@\texttt{display-normalized-proof}}
  &\quad& \text{abbreviated \texttt{dnp}\index{dnp@\texttt{dnp}}},
  \\
  &\texttt{(proof-to-expr .\ \textsl{opt-proof})},%
  \index{proof-to-expr@\texttt{proof-to-expr}}
  \\
  &\texttt{(proof-to-expr-with-formulas .\ \textsl{opt-proof})},%
  \index{proof-to-expr-with-formulas@\texttt{proof-to-expr-with-formulas}}
  \\
  &\texttt{(proof-to-expr-with-aconsts .\ \textsl{opt-proof})}.%
  \index{proof-to-expr-with-aconsts@\texttt{proof-to-expr-with-aconsts}}
\end{alignat*}
Here \texttt{display-normalized-proof} normalizes the proof first.
There are the following (less useful) display functions:
\begin{alignat*}{2}
  &\texttt{(display-pterm .\ \textsl{opt-proof})}
  \index{display-pterm@\texttt{display-pterm}}
  &\quad& \text{abbreviated \texttt{dpt}\index{dpt@\texttt{dpt}}},
  \\
  &\texttt{(display-proof-expr .\ \textsl{opt-proof})}
  \index{display-proof-expr@\texttt{display-proof-expr}}
  &\quad& \text{abbreviated \texttt{dpe}\index{dpe@\texttt{dpe}}},
  \\
  &\texttt{(display-normalized-pterm .\ \textsl{opt-proof})}
  \index{display-normalized-pterm@\texttt{display-normalized-pterm}}
  &\quad& \text{abbreviated \texttt{dnpt}\index{dnpt@\texttt{dnpt}}},
  \\
  &\texttt{(display-normalized-proof-expr .\ \textsl{opt-proof})}
  \index{display-normalized-proof-expr@\texttt{display-normalized-proof-expr}}
  &\quad& \text{abbreviated \texttt{dnpe}\index{dnpe@\texttt{dnpe}}}.
\end{alignat*}

\texttt{rename-avars}%
\index{rename-avars@\texttt{rename-avars}} renames bound assumption
variables in terms, formulas and comprehension terms.

\subsection{Check}
When in addition one wants to check the
correctness of the proof, use \texttt{check-and-display-proof}%
\index{check-and-display-proof@\texttt{check-and-display-proof}},
abbreviated \texttt{cdp}\index{cdp@\texttt{cdp}}.
\begin{align*}
  &\texttt{(check-and-display-proof .\
    \textsl{opt-proof-and-ignore-deco-flag})}.%
  \index{check-and-display-proof@\texttt{check-and-display-proof}}
\end{align*}
\texttt{ignore-deco-flag}\index{ignore-deco-flag@\texttt{ignore-deco-flag}}
is set to true as soon as the present proof argument proves a formula
of nulltype.  There is a global variable \texttt{CDP-COMMENT-FLAG}%
\index{CDP-COMMENT-FLAG@\texttt{CDP-COMMENT-FLAG}} by which one can
suppress some of the information \texttt{cdp} is providing.  Initially
\texttt{CDP-COMMENT-FLAG} is set to true.

\subsection{Classical logic}
\texttt{(proof-of-stab-at
  \textsl{formula})}\index{proof-of-stab-at@\texttt{proof-of-stab-at}}
generates a proof of $((A \to \falsityF) \to \falsityF) \to A$.  For
$\falsityF$, $T$ one takes the obvious proof, and for other atomic
formulas the proof using cases on booleans.  For all other prime or
existential formulas one takes an instance of the global assumption
\texttt{Stab}: $((P \to \falsityF) \to \falsityF) \to P$.  Here the
argument \textsl{formula} must be unfolded.  For the logical form of
falsity we take \texttt{(proof-of-stab-log-at
  \textsl{formula})}\index{proof-of-stab-log-at@\texttt{proof-of-stab-log-at}},
and similary for ex-falso-quodlibet we provide
\begin{alignat*}{2}
  &\texttt{(proof-of-efq-at \textsl{formula})},%
  \index{proof-of-efq-at@\texttt{proof-of-efq-at}}
  \\
  &\texttt{(proof-of-efq-log-at \textsl{formula})}.%
  \index{proof-of-efq-log-at@\texttt{proof-of-efq-log-at}}
\end{alignat*}
Using these functions we can then define \texttt{(reduce-efq-and-stab
  \textsl{proof})}\index{reduce-efq-and-stab@\texttt{reduce-efq-and-stab}},
which reduces all instances of stability and ex-falso-quodlibet axioms
in a proof to instances of these global assumptions with prime or
existential formulas, or (if possible) replaces them by their proofs.

With \texttt{rm-exc}\index{rm-exc@\texttt{rm-exc}} we can transform a
proof involving classical existential quantifiers in another one
without, i.e., in minimal logic.  The Exc-Intro and Exc-Elim theorems
are replaced by their proofs, using \texttt{expand-theorems}.

We now consider the Gödel-Gentzen translation%
\index{Goedel-Gentzen translation@Gödel-Gentzen translation}, also
known as negative translation%
\index{negative translation}.  It allows embed classical logic into
minimal logic; more precisely into its \inquotes{negative fragment}
involving only $\to$ and $\forall$.  First we define the Gödel-Gentzen
translation of formulas:
\begin{align*}
  &\texttt{(formula-to-goedel-gentzen-translation \textsl{formula})}.%
  \index{formula-to-goedel-gentzen-translation@\texttt{for{\dots}-goedel-gentzen-translation}}
\end{align*}
We do not consider $\excl$, because it can be unfolded (is not needed
for program extraction).

Finally we will define the Gödel-Gentzen translation of proofs.  To
this end we introduce a further observation (due to
Leivant\index{Leivant}; see Troelstra and van Dalen \cite[Ch.2,
Sec.3]{TroelstravanDalen88}) which will be particularly useful for
program extraction from classical proofs.  There it will be necessary
to transform a given classical derivation $\vdash_c A$ into a minimal
logic derivation $\vdash A^g$.  In particular, for every assumption
constant $C$ used in the given derivation we have to provide a
derivation of $C^g$.  Now for some formulas $S$ -- the so-called
spreading\index{formula!spreading}%
\index{spreading formula} formulas -- this is immediate, for we can
derive $S \to S^g$, and hence can use the original assumption
constant.

First notice that our formulas may contain \emph{predicate
  variables}\index{predicate variable} denoted by $X$, which are place
holders for comprehension terms, i.e., formulas with distinguished
variables.  We use the notation $\subst{A}{X}{\set{\vec{x}}{B}}$ or
shortly $A[\set{\vec{x}}{B}]$ or even $A[B]$ for substitution of a
comprehension term $\set{\vec{x}}{B}$ for the predicate variable $X$.
Recall that the Gödel-Gentzen translation of $X \vec{t}$ is $\neg \neg
X \vec{t}$.

Recall also that we view an assumption constant as consisting of an
uninstantiated formula (e.g., $X 0 \to \forall_{n}(X n \to X(n+1)) \to
\forall_{n} X n$ for induction) together with a substitution of
comprehension terms for predicate variables (e.g., $X \mapsto
\set{n}{n < n+1}$).  Then in order to obtain a derivation of $C^g$ for
an assumption constant $C$ it suffices to know that its
\emph{uninstantiated} formula $S$ is spreading, for then we have
$\vdash S[\vec{A}^g] \to S[\vec{A}\,]^g$ (see the theorem below) and
hence can use the same assumption constant with a different
substitution.

We define \emph{spreading} formulas\index{formula!spreading} $S$,
\emph{wiping} formulas\index{formula!wiping} $W$ and \emph{isolating}
formulas\index{formula!isolating} $I$ inductively.
\begin{alignat*}{2}
  S &\BNFdef \falsum \BNFor R \vec{t} \BNFor X \vec{t} \BNFor S \land S
  \BNFor I \to S \BNFor \forall_{x} S,
  \\
  W &\BNFdef \falsum \BNFor X \vec{t} \BNFor W \land W \BNFor
  S \to W \BNFor \forall_{x} W,
  \\
  I &\BNFdef R \vec{t} \BNFor W \BNFor I \land I.
\end{alignat*}
Let $\C{S}$ ($\C{W}, \C{I}$) be the class of spreading (wiping,
isolating) formulas.

\begin{theorem*}
  \begin{alignat*}{2}
    &\vdash S[\vec{A}^g] \to S[\vec{A}\,]^g
    &\quad& \text{for every spreading formula $S$,}
    \\
    &\vdash W[\vec{A}\,]^g \to W[\vec{A}^g]
    &&\text{for every wiping formula $W$,}
    \\
    &\vdash I[\vec{A}\,]^g \to \neg \neg I[\vec{A}^g]
    &&\text{for every isolating formula $I$.}
  \end{alignat*}
  We assume here that all occurrences of predicate variables are
  substituted.
\end{theorem*}

\begin{proof}
  By induction on the simultaneous generation of $\C{S}$, $\C{W}$ and
  $\C{I}$.  We write $S^g$ for $S[\vec{A}\,]^g$ and $S$ for
  $S[\vec{A}^g]$, and similarly for $W$ and $I$.

  \emph{Case} $\falsum \in \C{S}$.  We must show $\vdash \falsum \to
  \falsum^g$.  Take $\lambda u^{\falsum} u$.

  \emph{Case} $R \vec{t} \in \C{S}$.  We must show $\vdash R \vec{t} \to
  \neg \neg R \vec{t}$.  Take $\lambda u^{R \vec{t}} \lambda v^{\neg
    R \vec{t}}.vu$.

  \emph{Case} $X \vec{t} \in \C{S}$, with $X$ substituted by
  $\set{\vec{x}}{A}$.  We must show $\vdash A^g[\vec{t}\,] \to A^g[\vec{t}\,]$,
  which is trivial.

  \emph{Case} $S_1 \land S_2 \in \C{S}$.  We must show $\vdash S_1 \land
  S_2 \to S_1^g \land S_2^g$.  Take
  \begin{equation*}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$S_1 \to S_1^g$}
    \AxiomC{$u \colon S_1 \land S_2$}
    \UnaryInfC{$S_1$}
    \BinaryInfC{$S_1^g$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$S_2 \to S_2^g$}
    \AxiomC{$u \colon S_1 \land S_2$}
    \UnaryInfC{$S_2$}
    \BinaryInfC{$S_2^g$}
    \BinaryInfC{$S_1^g \land S_2^g$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $I \to S \in \C{S}$.  We must show $\vdash (I \to S) \to
  I^g \to S^g$.  Recall that $\vdash \neg \neg S^g \to S^g$ by the
  Stability Lemma, because $S^g$ is negative.  Take
  \begin{equation*}
    \AxiomC{Stab}
    \noLine
    \UnaryInfC{$\neg \neg S^g \to S^g$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$I^g \to \neg \neg I$}
    \AxiomC{$v \colon I^g$}
    \insertBetweenHyps{\hspace{.5em}}
    \BinaryInfC{$\neg \neg I$}
    \AxiomC{$w_1 \colon \neg S^g$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$S \to S^g$}
    \AxiomC{$u \colon I \to S$}
    \AxiomC{$w_2 \colon I$}
    \insertBetweenHyps{\hspace{.5em}}
    \BinaryInfC{$S$}
    \BinaryInfC{$S^g$}
    \insertBetweenHyps{\hspace{.5em}}
    \BinaryInfC{$\falsum$}
    \RightLabel{$\impI w_2$}
    \UnaryInfC{$\neg I$}
    \insertBetweenHyps{\hspace{.5em}}
    \BinaryInfC{$\falsum$}
    \RightLabel{$\impI w_1$}
    \UnaryInfC{$\neg \neg S^g$}
    \insertBetweenHyps{\hspace{-2em}}
    \BinaryInfC{$S^g$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $\forall_{x} S \in \C{S}$.  We must show $\vdash \forall_{x} S
  \to \forall_{x} S^g$.  Take
  \begin{equation*}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$S \to S^g$}
    \AxiomC{$u \colon \forall_{x} S$}
    \AxiomC{$x$}
    \BinaryInfC{$S$}
    \BinaryInfC{$S^g$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $\falsum \in \C{W}$.  We must show $\vdash \falsum^g \to
  \falsum$.  Take $\lambda u^{\falsum} u$.

  \emph{Case} $X \vec{t} \in \C{W}$, with $X$ substituted by
  $\set{\vec{x}}{A}$.  We must show $\vdash A^g[\vec{t}\,] \to A^g[\vec{t}\,]$,
  which is trivial.

  \emph{Case} $W_1 \land W_2 \in \C{W}$.  We must show $\vdash W_1^g \land
  W_2^g \to W_1 \land W_2$.  Take
  \begin{equation*}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$W_1^g \to W_1$}
    \AxiomC{$u \colon W_1^g \land W_2^g$}
    \UnaryInfC{$W_1^g$}
    \BinaryInfC{$W_1$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$W_2^g \to W_2$}
    \AxiomC{$u \colon W_1^g \land W_2^g$}
    \UnaryInfC{$W_2^g$}
    \BinaryInfC{$W_2$}
    \BinaryInfC{$W_1 \land W_2$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $S \to W \in \C{W}$.  We must show $\vdash (S^g \to W^g)
  \to S \to W$.  Take
  \begin{equation*}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$W^g \to W$}
    \AxiomC{$u \colon S^g \to W^g$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$S \to S^g$}
    \AxiomC{$v \colon S$}
    \BinaryInfC{$S^g$}
    \BinaryInfC{$W^g$}
    \BinaryInfC{$W$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $\forall_{x} W \in \C{W}$.  We must show $\vdash \forall_{x} W^g
  \to \forall_{x} W$.  Take
  \begin{equation*}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$W^g \to W$}
    \AxiomC{$u \colon \forall_{x} W^g$}
    \AxiomC{$x$}
    \BinaryInfC{$W^g$}
    \BinaryInfC{$W$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $R \vec{t} \in \C{I}$.  We must show $\vdash \neg \neg R
  \vec{t} \to \neg \neg R \vec{t}$, which is trivial.

  \emph{Case} $W \in \C{I}$.  We must show $\vdash W^g \to \neg \neg W$,
  which trivially follows from the IH $\vdash W^g \to W$.  Take
  \begin{equation*}
    \AxiomC{$v \colon \neg W$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$W^g \to W$}
    \AxiomC{$u \colon W^g$}
    \BinaryInfC{$W$}
    \BinaryInfC{$\falsum$}
    \DisplayProof
  \end{equation*}

  \emph{Case} $I_1 \land I_2 \in \C{I}$.  We must show $\vdash I_1^g \land
  I_2^g \to \neg \neg (I_1 \land I_2)$.  Take
  \begin{equation*}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$I_2^g \to \neg \neg I_2$}
    \AxiomC{$I_1^g {\land} I_2^g$}
    \UnaryInfC{$I_2^g$}
    \BinaryInfC{$\neg \neg I_2$}
    \AxiomC{IH}
    \noLine
    \UnaryInfC{$I_1^g \to \neg \neg I_1$}
    \AxiomC{$I_1^g {\land} I_2^g$}
    \UnaryInfC{$I_1^g$}
    \BinaryInfC{$\neg \neg I_1$}
    \AxiomC{$\neg (I_1 \land I_2)$}
    \AxiomC{$I_1$}
    \AxiomC{$I_2$}
    \insertBetweenHyps{\hspace{.5em}}
    \BinaryInfC{$I_1 \land I_2$}
    \BinaryInfC{$\falsum$}
    %% \RightLabel{$\impI w_1$}
    \UnaryInfC{$\neg I_1$}
    \BinaryInfC{$\falsum$}
    %% \RightLabel{$\impI w_2$}
    \UnaryInfC{$\neg I_2$}
    \BinaryInfC{$\falsum$}
    \DisplayProof \qedhere
  \end{equation*}
  This completes the proof.
\end{proof}

The theory above is implemented as follows.  Simultaneously with
spreading\index{formula!spreading}%
\index{spreading formula} formulas we need to define
wiping\index{formula!wiping}%
\index{wiping formula} and isolating\index{formula!isolating}%
\index{isolating formula} formulas:
\begin{align*}
  &\texttt{(spreading-formula?\ \textsl{formula})},%
  \index{spreading-formula?@\texttt{spreading-formula?}}
  \\
  &\texttt{(wiping-formula?\ \textsl{formula})},%
  \index{wiping-formula?@\texttt{wiping-formula?}}
  \\
  &\texttt{(isolating-formula?\ \textsl{formula})},%
  \index{isolating-formula?@\texttt{isolating-formula?}}
  \\
  &\texttt{(spreading-formula-to-proof \textsl{formula} .\
    \textsl{opt-psubst})},%
  \index{spreading-formula-to-proof@\texttt{spreading-formula-to-proof}}
  \\
  &\texttt{(wiping-formula-to-proof \textsl{formula} .\
    \textsl{opt-psubst})},%
  \index{wiping-formula-to-proof@\texttt{wiping-formula-to-proof}}
  \\
  &\texttt{(isolating-formula-to-proof \textsl{formula} .\
    \textsl{opt-psubst})}.%
  \index{isolating-formula-to-proof@\texttt{isolating-formula-to-proof}}
\end{align*}
Using these we can define the Gödel-Gentzen translation:
\begin{align*}
  &\texttt{(proof-to-goedel-gentzen-translation \textsl{proof})}.%
  \index{proof-to-goedel-gentzen-translation@\texttt{proof-to-goedel-gentzen-translation}}
\end{align*}
Notice that the Gödel-Gentzen translation double negates every atom,
and hence may produce triple negations.  However, we can
systematically replace triple negations by single ones.  The final result
then is
\begin{align*}
  &\texttt{(proof-to-reduced-goedel-gentzen-translation \textsl{proof})}.%
  \index{proof-to-reduced-goedel-gentzen-translation@\texttt{proof-to-reduced-goedel-gentzen-transl..}}
\end{align*}

\subsection{Existence formulas}
In case of existence formulas $\ex_{\vec{x}_1} A_1 \dots
\ex_{\vec{x}_n} A_n$ and conclusion $B$ we recursively construct a
proof of
\begin{equation*}
  \ex_{\vec{x}_1} A_1 \to \dots \ex_{\vec{x}_n} A_n \to
  \forall_{\vec{x}_1, \dots, \vec{x}_n}(A_1 \to \dots \to A_n \to B) \to B
\end{equation*}
by means of
\begin{align*}
  &\texttt{(ex-formulas-and-concl-to-ex-elim-proof
    \textsl{x} .\ \textsl{rest})}.%
  \index{ex-formulas-and-concl-to-ex-elim-proof@\texttt{ex-formulas-and-concl-to-ex-elim-proof}}
\end{align*}

\subsection{Basic proof constructions}
\label{SS:BasicProofConstr}
For every formula $A$, a proof of $\falsityF \to A$ (i.e.,
ex-falso-quodlibet%
\index{ex-falso-quodlibet}) is constructed, and
also proofs that constructors are injective and have disjoint ranges.
For ex-falso-quodlibet we use
\begin{align*}
  &\texttt{(formula-to-efq-proof \textsl{formula})}.%
  \index{formula-to-efq-proof@\texttt{formula-to-efq-proof}}
\end{align*}
To make this work easily for (simultaneous) inductive definitions, we
assume that taking the initial clause of each inductively defined
predicate constant produces clauses without recursive calls which are
terminating.  This is checked in \texttt{add-ids}.

Given proofs of Leibniz equalities $\eqd {r_1} {s_1}, \dots, \eqd
{r_n} {s_n}$ and a predicate-proof of $I r_1 \dots r_n$ we construct a
proof of $I s_1 \dots s_n$ using \texttt{EqDCompat} by means of
\begin{align*}
  \texttt{(eqd-proofs-and-predicate-proof-to-proof} \ &\textsl{eqd-proofs}
  \\
  &\textsl{predicate-proof}\texttt{)}%
  \index{eqd-proofs-and-predicate-proof-to-proof@\texttt{eqd-proofs-and-predicate-proof-to-proof}}
\end{align*}

To generate proofs of the injectivity of constructors we have
\begin{align*}
  \texttt{(constructor-eqd-proof-to-args-eqd-proof}\ &\textsl{eqd-proof}
  \\
  &.\ \textsl{opt-index}\texttt{)}.%
  \index{constructor-eqd-proof-to-args-eqd-proof@\texttt{constructor-eqd-proof-to-args-eqd-proof}}
\end{align*}
It expects an eqd-proof of $ \eqd {\constr \vec{r}} {\constr \vec{s}}$
with the same constructor $\constr$ and $\constr \vec{r}$ of ground
type, and an optional index (with default value $0$).  The result is a
proof of $\eqd {r_i} {s_i}$.
\begin{align*}
  &\texttt{(constructor-eqd-imp-args-eqd-proof \textsl{eqd-formula} .\
    \textsl{opt-index})}%
  \index{constructor-eqd-imp-args-eqd-proof@\texttt{constructor-eqd-imp-args-eqd-proof}}
\end{align*}
is similar, but expects an \texttt{eqd-formula} rather than an
\texttt{eqd-proof} and proves the implication
$\eqd {\constr \vec{r}} {\constr \vec{s}} \to \eqd {r_i} {s_i}$.

Finally we have
\begin{align*}
  &\texttt{(constructors-overlap-imp-falsity-proof \textsl{eqd-formula})}.%
  \index{constructors-overlap-imp-falsity-proof@\texttt{constructors-overlap-imp-falsity-proof}}
\end{align*}
It generates a proof of an implication from an equality between two
constructor terms with different constructors at their heads to
falsity.  In this sense we provide proofs that constructors have
disjoint ranges.

\section{Interactive theorem proving with partial proofs}
\label{Pproof}
%% \subsection{Partial proofs}
A partial proof is a proof with holes, i.e., special
assumption variables (called goal variables) \texttt{v}, \texttt{v1},
\texttt{v2} \dots whose formulas must be closed.  We assume that every
goal variable \texttt{v} has a single occurrence in the proof.  We
then select a (not necessarily maximal) subproof \texttt{vx1...xn}
with distinct object or assumption variables \texttt{x1...xn}.  Such a
subproof is called a \emph{goal}\index{goal}.  When interactively
developing a partial proof, a goal \texttt{vx1...xn} is replaced by
another partial proof, whose context is a subset of \texttt{x1...xn}
(i.e., the context of the goal with \texttt{v} removed).

To gain some flexibility when working on our goals, we do not at each
step of an interactive proof development traverse the partial proof
searching for the remaining goals, but rather keep a list of all open
goals together with their numbers as we go along.  We maintain a
global variable \texttt{PPROOF-STATE} containing a list of three
elements: (1) \texttt{num-goals}, an alist of entries \texttt{(number
  goal drop-info hypname-info)}, (2) \texttt{proof} and (3)
\texttt{maxgoal}, the maximal goal number used.

%% \subsection{Interactive theorem proving}
To construct a goal and access its components we have
\begin{alignat*}{2}
  &\texttt{(make-goal-in-avar-form \textsl{avar})},%
  \index{make-goal-in-avar-form@\texttt{make-goal-in-avar-form}}
  \\
  &\texttt{(make-goal-in-all-elim-form \textsl{goal} \textsl{uservar})},%
  \index{make-goal-in-all-elim-form@\texttt{make-goal-in-all-elim-form}}
  \\
  &\texttt{(make-goal-in-allnc-elim-form \textsl{goal} \textsl{uservar})},%
  \index{make-goal-in-allnc-elim-form@\texttt{make-goal-in-allnc-elim-form}}
  \\
  &\texttt{(make-goal-in-imp-elim-form \textsl{avar})},%
  \index{make-goal-in-imp-elim-form@\texttt{make-goal-in-imp-elim-form}}
  \\
  &\texttt{(make-goal-in-impnc-elim-form \textsl{avar})},%
  \index{make-goal-in-impnc-elim-form@\texttt{make-goal-in-impnc-elim-form}}
  \\
  &\texttt{(mk-goal-in-elim-form .\ \textsl{elim-items})},%
  \index{mk-goal-in-elim-form@\texttt{mk-goal-in-elim-form}}
  \\
  &\texttt{(goal-to-goalvar \textsl{goal})},%
  \index{goal-to-goalvar@\texttt{goal-to-goalvar}}
  \\
  &\texttt{(goal-to-context \textsl{goal})},%
  \index{goal-to-context@\texttt{goal-to-context}}
  \\
  &\texttt{(goal-to-formula \textsl{goal})}.%
  \index{goal-to-formula@\texttt{goal-to-formula}}
\end{alignat*}
For interactively building proofs we need
\begin{alignat*}{2}
  &\texttt{(goal=?\ \textsl{proof} \textsl{goal})},%
  \index{goal=?@\texttt{goal=?}}
  \\
  &\texttt{(goal-subst \textsl{proof} \textsl{goal} \textsl{proof1})}.%
  \index{goal-subst@\texttt{goal-subst}}
\end{alignat*}
Initialization of the global variable \texttt{PPROOF-STATE} and access
to its parts is possible via
\begin{alignat*}{2}
  &\texttt{(make-pproof-state
    \textsl{num-goals} \textsl{proof} \textsl{maxgoal})},%
  \index{make-pproof-state@\texttt{make-pproof-state}}
  \\
  &\texttt{(pproof-state-to-num-goals)},%
  \index{pproof-state-to-num-goals@\texttt{pproof-state-to-num-goals}}
  \\
  &\texttt{(pproof-state-to-proof)},%
  \index{pproof-state-to-proof@\texttt{pproof-state-to-proof}}
  \\
  &\texttt{(pproof-state-to-formula)}.%
  \index{pproof-state-to-formula@\texttt{pproof-state-to-formula}}
\end{alignat*}
At each stage of an interactive proof development we have access
to the current proof and the current goal by executing
\begin{alignat*}{2}
  &\texttt{(current-proof)}\index{current-proof@\texttt{current-proof}},
  \\
  &\texttt{(current-goal)}\index{current-goal@\texttt{current-goal}}.
\end{alignat*}

We initially supply our axioms (see \ref{SS:Axioms}) as theorems, and also
\begin{verbatim}
   AtomTrue: all boole^(boole^ -> boole^ =True)
   AtomFalse: all boole((boole -> F) -> boole=False)
\end{verbatim}
\index{AtomTrue}%
\index{AtomFalse}%
There is a global constant \texttt{THEOREMS}%
\index{THEOREMS@\texttt{THEOREMS}} containing all theorems known
to the system, and also their proofs.  Similarly we maintain a global
constant \texttt{GLOBAL-ASSUMPTIONS}%
\index{GLOBAL-ASSUMPTIONS@\texttt{GLOBAL-ASSUMPTIONS}}, which
initially contains the global assumptions listed in \ref{SS:GlobalAss}.

For display we have
\begin{alignat*}{2}
  &\texttt{(display-current-goal)}%
  \index{display-current-goal@\texttt{display-current-goal}}
  \quad\hbox{abbreviated \texttt{dcg}\index{dcg@\texttt{dcg}}},
  \\
  &\texttt{(display-current-goal-with-normalized-formulas)},%
  \index{display-current-goal-with-normalized-formulas@\texttt{display-current-goal-with{\dots}}}
\end{alignat*}
abbreviated \texttt{dcg}\index{dcg@\texttt{dcg}} and
\texttt{dcgnf}\index{dcgnf@\texttt{dcgnf}}, respectively.  One can
switch to a different display style for the current goal by setting
\texttt{COQ-GOAL-DISPLAY}%
\index{COQ-GOAL-DISPLAY@\texttt{COQ-GOAL-DISPLAY}} to true.

We list some commands for interactively building proofs.

\subsection{set-goal}
An interactive proof starts with setting the goal
\begin{equation*}
  \texttt{(set-goal \textsl{string-or-formula})},%
  \index{set-goal@\texttt{set-goal}}
\end{equation*}
i.e., with setting a goal.  The goal-formula might be given by its
display string.  It should be closed; if not, universal quantifiers
are inserted automatically.

\subsection{normalize-goal}
\texttt{(normalize-goal .\
  \textsl{ng-info})}\index{normalize-goal@\texttt{normalize-goal}}
(short: \texttt{ng}\index{ng@\texttt{ng}}) takes optional
arguments \texttt{ng-info}.  If there are none, the goal formula and
all hypotheses are normalized.  Otherwise exactly those among the
hypotheses and the goal formula are normalized whose numbers (or
names, or just \verb+#t+ for the goal formula) are listed as
additional arguments.

\subsection{assume}
With \texttt{(assume \textsl{x1} \dots)}\index{assume@\texttt{assume}}
we can move universally quantified variables and hypotheses into the
context.  The variables must be given names (known to the parser as
valid variable names for the given type), and the hypotheses should be
identified by numbers or strings.

Internally, assume extends the partial proof under construction by
introduction rules.  To every quantifier $\forall_x$ (resp.\
$\allnc_y$) in the present goal corresponds an application of the
$\allI$-rule (resp.\ $\allncI$-rule).  To meet the variable condition
for $\allncI$-rules, the $\allnc$-variable \texttt{y} in the assumed
context is not admitted as a computational variable in a future proof
of the present goal.  Therefore it is displayed in braces, as
\texttt{\{y\}}.

\subsection{use}
In \texttt{(use \textsl{x} .\
  \textsl{elab-path-and-terms})}\index{use@\texttt{use}}, \textsl{x}
is one of the following.
\begin{enumeratei}
\item A number or string identifying a hypothesis form the context.
\item A formula with free variables from the context, generating a new
  goal.
\item The name of a theorem or global assumption.
\item A closed proof.
\end{enumeratei}
It is checked whether some final part of this used formula has the
form of (or \inquotes{matches}) the goal, where if (i) \textsl{x}
determines a hypothesis or is the formula for a new goal, then all its
free topvars are rigid, and if (ii) \textsl{x} determines a closed
proof, then all its (implicitly generalized) tpvars are flexible,
except the predicate variable $\falsum$ (written \texttt{bot}) from
\texttt{falsity-log}.  \textsl{elab-path-and-terms} is a list
consisting of symbols \texttt{left} or \texttt{right}, giving
directions in case the used formula contains conjunctions, and of
terms/cterms to be substituted for the variables that cannot be
instantiated by matching.  Matching is done for type and object
variables first (via \texttt{match}%
\index{match@\texttt{match}}), and in case this fails with
\texttt{huet-match}\index{huet-match@\texttt{huet-match}} next.  There
is a similar \texttt{(use2 \textsl{x} .\
  \textsl{elab-path-and-terms})}\index{use2@\texttt{use2}}, which only
applies \texttt{huet-match}.

\subsection{use-with}
This is a more verbose form of \texttt{use}, where the terms are not
inferred via unification, but have to be given explicitly.  Also, for
the instantiated premises one can indicate how they are to come about.
So in \texttt{(use-with \textsl{x} .\
  \textsl{x-list})}\index{use-with@\texttt{use-with}}, \textsl{x} is
 one of the following.
\begin{enumeratei}
\item A number or string identifying a hypothesis form the context.
\item The name of a theorem or global assumption.  If it is a global
  assumption whose final conclusion is a nullary predicate variable
  distinct from bot (e.g. \texttt{EfqLog} or \texttt{StabLog}), this
  predicate variable is substituted by the goal formula.
\item A closed proof.
\item A formula with free variables from the context, generating a new
  goal.
\end{enumeratei}
Moreover \textsl{x-list} is a list consisting of
\begin{enumeratei}
\item a number or string identifying a hypothesis form the context,
\item the name of a theorem or global assumption,
\item a closed proof,
\item the string \inquotes{?} (value of \texttt{DEFAULT-GOAL-NAME}),
  generating a new goal,
\item a symbol \texttt{left} or \texttt{right},
\item a term, whose free variables are added to the context,
\item a type, which is substituted for the first type variable,
\item a comprehension term, which is substituted for the first predicate
  variable.
\end{enumeratei}
Internally
\texttt{x-and-x-list-to-proof-and-new-num-goals-and-maxgoal}%
\index{x-and-x-list-to-proof-and-new-num-goals-and-maxgoal@\texttt{x-and-x-list-to-proof-and...}}
will be used by \texttt{use-with} (and also by \texttt{inst-with}) to
construct the new data.  It appears in error messages if the
arguments of \texttt{use-with} are incorrect.

Notice that new free variables not in the ordered context can be
introduced in \texttt{use-with}.  They will be present in the newly
generated goals.  The reason is that proofs should be allowed to
contain free variables.  This is necessary to allow logic in ground
types where no constant is available (for instance to prove $\forall_x
Px \to \forall_x \neg Px \to \falsum$).

Notice also that there are situations where \textsl{use-with} can be
applied but \textsl{use} cannot.  For an example, consider the goal
$P(S(k+l))$ with the hypothesis $\forall_l P(k+l)$ in the context.
Then \textsl{use} cannot find the term $S l$ by matching; however,
applying \textsl{use-with} to the hypothesis and the term $S l$
succeeds (since $k+S l$ and $S(k+l)$ have the same normal form).

\subsection{inst-with}
\texttt{inst-with} does for forward chaining the same as use-with for
backward chaining.  It replaces the present goal by a new one, with
one additional hypothesis obtained by instantiating a previous one;
this effect could also be obtained by cut.  In \texttt{(inst-with
  \textsl{x} .\ \textsl{x-list})}\index{inst-with@\texttt{inst-with}},
\textsl{x} is
\begin{enumeratei}
\item a number or string identifying a hypothesis form the context,
\item the name of a theorem or global assumption,
\item a closed proof,
\item a formula with free variables from the context, generating a new
  goal.
\end{enumeratei}
and \textsl{x-list} is a list consisting of
\begin{enumeratei}
\item a number or string identifying a hypothesis form the context,
\item the name of a theorem or global assumption,
\item a closed proof,
\item the string \inquotes{?} (value of \texttt{DEFAULT-GOAL-NAME}),
  generating a new goal,
\item a symbol \texttt{left} or \texttt{right},
\item a term, whose free variables are added to the context,
\item a type, which is substituted for the first type variable,
\item a comprehension term, which is substituted for the first predicate
  variable.
\end{enumeratei}

\subsection{inst-with-to}
\texttt{inst-with-to}\index{inst-with-to@\texttt{inst-with-to}}
expects a string as its last argument, which is used (via
\texttt{name-hyp}) to name the newly introduced instantiated
hypothesis.

\subsection{cut}
The command \texttt{(cut \textsl{A})}\index{cut@\texttt{cut}} replaces
the goal $B$ by the two new goals $A$ and $A \to B$, with $A \to B$ to
be proved first.

\subsection{assert}
The command \texttt{(assert \textsl{A})}\index{assert@\texttt{assert}}
replaces the goal $B$ by the two new goals $A$ and $A \to B$, with $A$
to be proved first.

\subsection{strip}
To move (all or $n$) universally quantified variables and hypotheses
of the current goal into the context, we use the command
\texttt{(strip)}\index{strip@\texttt{strip}} or \texttt{(strip n)}.

\subsection{drop}
In \texttt{(drop .\ x-list)}\index{drop@\texttt{drop}}, x-list is a
list of numbers or strings identifying hypotheses from the context.  A
new goal is created, which differs from the previous one only in
display aspects: the listed hypotheses are hidden (but still present).
If x-list is empty, all hypotheses are hidden.

\subsection{name-hyp}
The command \texttt{name-hyp}\index{name-hyp@\texttt{name-hyp}}
expects an index $i$ and a string.  Then a new goal is created, which
differs from the previous one only in display aspects: the string is
used to label the $i$th hypothesis.

\subsection{split, msplit}
The command \texttt{(split)}\index{split@\texttt{split}} expects as
goal a conjunction $A \land B$ or an \texttt{AndConst}-atom, and
splits it into two new goals $A$ and $B$.  We allow multiple split
\texttt{(msplit)}\index{msplit@\texttt{msplit}} over a conjunctive
formula (all conjuncts connected through \verb#&# which are at the
same level are split at once).

\subsection{get}
To be able to work on a goal different from that on top of the goal
stack, we can move it up using \texttt{(get \textsl{n})}%
\index{get@\texttt{get}}.

\subsection{undo}
With \texttt{(undo .\ \textsl{n})}\index{undo@\texttt{undo}}, the last
$n$ steps of an interactive proof can be made undone.  \texttt{(undo)}
has the same effect as \texttt{(undo 1)}.  \texttt{(undoto
  \textsl{n})}\index{undoto@\texttt{undoto}} allows to go back to a
previous pproof state whose (top) goal had number $n$.

\subsection{ind}
\label{SS:ind}
\texttt{(ind)}\index{ind@\texttt{ind}}\index{induction} expects a goal
$\forall_{x^{\iota}} A(x)$ with $x$ total and $\iota$ an algebra.  Let
$c_1, \dots, c_n$ be the constructors of the algebra.  Then $n$ new
goals $\forall_{\vec{x}_i}( A(x_{1i}) \to \dots \to A(x_{ki}) \to
A(c_i \vec{x}_i)$ are generated.  \texttt{(ind \textsl{t})} expects a
goal $A(t)$.  It computes the algebra $\iota$ as type of the term $t$.
Then again the $n$ new goals above are generated.

\subsection{simind}
\texttt{(simind \textsl{all-formula1}
  \dots)}\index{simind@\texttt{simind}}\index{induction!simultaneous}
expects a goal $\forall_{x^{\iota}} A(x)$ with $x$ total and $\iota$
an algebra.  We have to provide as arguments the other all-formulas to
be proved simultaneously with the goal.

\subsection{gind}
\texttt{(gind \textsl{h})}\index{gind@\texttt{gind}}%
\index{induction!general} expects a goal $\forall_{\vec{x}}
A(\vec{x}\,)$ with $\vec{x}$ total.  It generates a new goal $\Prog_h
\set {\vec{x}} {A(\vec{x}\,)}$ where $h$ is a term of type $\vec{\rho}
\typeTo \typeN$, $x_i$ has type $\rho_i$ and $\Prog_h \set {\vec{x}}
{A(\vec{x}\,)} \defequiv \forall_{\vec{x}}( \forall_{\vec{y}}( h
\vec{y} < h \vec{x} \to A(\vec{y}\,)) \to A(\vec{x}\,))$.

\texttt{(gind \textsl{h} \textsl{t1} \dots \textsl{tn})} expects a
goal $A(\vec{t}\,)$ and generates the same goal as for \texttt{(gind
  \textsl{h})} with the formula $\forall_{\vec{x}} A(x)$.

\subsection{intro}
\texttt{(intro \textsl{i} .\ \textsl{terms})}%
\index{intro@\texttt{intro}} expects as goal an inductively defined
predicate.  The $i$-th introduction axiom for this predicate is
applied, via \texttt{use} (hence \textsl{terms} may have to be
provided).  \texttt{(intro-with \textsl{i} .\
  \textsl{x-list})}\index{intro-with@\texttt{intro-with}} does the
same, via \texttt{use-with}.

\subsection{elim}
Recall that $I \vec{r}$ provides (i) a type substitution, (ii) a
predicate instantiation, and (iii) the list $\vec{r}$ of argument
terms.  In \texttt{(elim \textsl{idhyp})}\index{elim@\texttt{elim}}
\textsl{idhyp} is, with an inductively defined predicate $I$,
\begin{enumeratei}
\item a number or string identifying a hypothesis $I \vec{r}$ form the
  context
\item the name of a global assumption or theorem $I \vec{r}$;
\item a closed proof of a formula $I \vec{r}$;
\item a formula $I \vec{r}$ with free variables from the context,
  generating a new goal.
\end{enumeratei}
Then the (strengthened) elimination axiom is used with $\vec{r}$ for
$\vec{x}$ and \textsl{idhyp} for $I \vec{r}$ to prove the goal
$A(\vec{r}\,)$, leaving the instantiated (with $\set {\vec{x}}
{A(\vec{x}\,)}$) clauses as new goals.

\texttt{(elim)}\index{elim@\texttt{elim}} expects a goal $I \vec{r}
\to A(\vec{r}\,)$.  Then the (strengthened) clauses are generated as
new goals, via \texttt{use-with}.

In case of simultaneously inductively defined predicate constants we
can provide other imp-formulas to be proved simultaneously with the
given one.  Then the (strengthened) simplified clauses are generated
as new goals.

\subsection{inversion, simplified-inversion}
In
\begin{equation*}
  \texttt{(inversion \textsl{x} .\ \textsl{imp-formulas})}%
  \index{inversion@\texttt{inversion}}
\end{equation*}
it is assumed that \textsl{x} is one of the following.
\begin{enumeratei}
\item A number or string identifying a hypothesis $I \vec{r}$ form the
  context.
\item The name of a theorem or global assumption stating $I \vec{r}$.
\item A closed proof of $I \vec{r}$.
\item A formula $I \vec{r}$ with free vars from the context,
  generating a new goal.
\end{enumeratei}
\textsl{imp-formulas} have the form $J \vec{s} \to B$.  Here $I,J$ are
inductively defined predicates, with clauses $K_1, \dots, K_n$.  Now
one uses the elim-aconst for $I \vec{x} \to \vec{x}=\vec{r} \to A$
with $A$ the goal formula and the additional implications $J \vec{y}
\to \vec{y}=\vec{s} \to B$, with \inquotes{?} for the clauses,
$\vec{r}$ for $\vec{x} $ and proofs for $\vec{r} =\vec{r}$, to obtain
the goal.  Then many of the generated goals for the clauses will
contain false premises, coming from substituted equations
$\vec{x}=\vec{r}$, and are proved automatically.

\textsl{imp-formulas} not provided are taken as $J \vec{x} \to J
\vec{x}$.  Generated clauses for such $J$ are proved automatically
from the intro axioms (the rec-prems are not needed).

For simultaneous inductively defined predicates
\texttt{(simplified-inversion \textsl{x} .\ \textsl{imp-formulas})}%
\index{simplified-inversion@\texttt{simplified-inversion}} does not
add imp-formulas $J \vec{x} \to J \vec{x}$ to form the elim-aconst.
Then the (new) \texttt{imp-formulas-to-uninst-elim-formulas-etc}
generates simplified clauses.  In some special cases this suffices.

\subsection{coind}
\label{SS:coind}
Recall that $J(\vec{r}\,)$ with a coinductively defined predicate $J$
provides
\begin{enumeratei}
\item a type substitution,
\item a predicate instantiation, and
\item the list $\vec{r}$ of argument terms.
\end{enumeratei}
\texttt{(coind \textsl{hyp})}\index{coind@\texttt{coind}} expects a
goal $J(\vec{r}\,)$ with a coinductively defined predicate $J$, and
\textsl{hyp} is expected to be
\begin{enumeratei}
\item a number or string identifying a hypothesis $A(\vec{r}\,)$ form
  the context;
\item the name of a global assumption or theorem $A(\vec{r}\,)$;
\item a closed proof of a formula $A(\vec{r}\,)$;
\item a formula $A(\vec{r}\,)$ with free variables from the context,
  generating a new goal.
\end{enumeratei}
\texttt{(coind)} expects an inst-imp-formula $A(\vec{r}\,) \to J
\vec{r}$ as goal.  Then the greatest-fixed-point axiom for $J$ is
used: $P \vec{x} \to \forall_{\vec{x}}(P \vec{x} \to C(P)) \to J
\vec{x}$ with $C(J)$ the defining clause for $J$.  Substitute $\set
{\vec{x}} {A(\vec{x}\,)}$ for $P$, and use $A(\vec{x}\,) \to
\forall_{\vec{x}}(A(\vec{x}\,) \to C(\set {\vec{x}} {A(\vec{x}\,)}))
\to J \vec{x}$ (i.e., its universal closure).  After an appropriate
application ($\vec{r}$ for $\vec{x}$\,) we are left with a new goal
saying that $\set {\vec{x}} {A(\vec{x}\,)}$ satisfies the defining
clause for $J$.

In case of simultaneous coinductively defined predicates we can
provide other imp-formulas to be proved simultaneously with the
given one.  Then their clauses are generated as new goals.

\subsection{ex-intro}
In \texttt{(ex-intro
  \textsl{term})}\index{ex-intro@\texttt{ex-intro}}, the user provides
a term to be used for the present (existential) goal.

\subsection{ex-elim}
In \texttt{(ex-elim \textsl{x})}\index{ex-elim@\texttt{ex-elim}},
\textsl{x} is
\begin{enumeratei}
\item a number or string identifying an existential hypothesis from
  the context,
\item the name of an existential global assumption or theorem,
\item a closed proof on an existential formula,
\item an existential formula with free variables from the context,
  genera\-ting a new goal.
\end{enumeratei}
Let $\ex_y A$ be the existential formula identified by \textsl{x}.
The user is then asked to provide a proof for the present goal,
assuming that a $y$ satisfying $A$ is available.

\subsection{by-assume}
Suppose we prove a goal from an existential formula $\ex_x A$, $\exd_x
A$, $\exR_x A$, $\exL_x A$, $\exnc_x A$ or $\excl_{x_1, \dots,x_n}(A_1
\landcl \dots \landcl A_m)$.  The natural way to use this hypothesis
is to say \inquotes{by the existential hypothesis assume we have an
  $x$ satisfying $A$} or \inquotes{by \dots assume we have $x_1,
  \dots, x_n$ satisfying $A_1, \dots, A_m$}.  Correspondingly we
provide \texttt{(by-assume \textsl{x} \textsl{y} \textsl{u})}%
\index{by-assume@\texttt{by-assume}}.  Here \textsl{x} (as
in \texttt{ex-elim}) identifies an existential hypothesis, and we
assume (i.e., add to the context) the variable $y$ and -- with label
$u$ -- the kernel $A$.

Example (introducing abbreviations\index{abbreviations}).  Suppose
that in a proof we want to abbreviate a (complex) term $t$ by a
variable $x$.  Then do
\begin{verbatim}
(assert "ex x x=t")
 (ex-intro (pt "t"))
 (use "Truth")
(assume "ExHyp")
(by-assume "ExHyp" "x" "x-Def")
\end{verbatim}
Now we have $x$ and $x$-Def: $x=t$ in the context, and can work with $x$
rather than the term $t$.

\subsection{cases}
\texttt{(cases)}\index{cases@\texttt{cases}} expects a goal formula
$\forall_x A$ with $x$ of an algebra type and total.  Let $c_1, \dots,
c_n$ be the constructors of the algebra.  Then $n$ new goals
$\forall_{\vec{x}_i} A(c_i \vec{x}_i)$ are generated.  \texttt{(cases
  \textsl{t})} expects a goal $A(t)$ with $t$ a total term.  If $t$ is
a boolean term, the goal $A(t)$ is replaced by the two new goals
$\atom(t) \to A(\true)$ and $(\atom(t) \to \falsityF) \to A(\false)$.
If $t$ is a total non-boolean term, \texttt{cases} is called with the
all-formula $\forall_x( x=t \to A(x))$.

\texttt{(cases \textsl{'auto})} expects an atomic goal and checks
whether its boolean kernel contains an if-term whose test is neither
an if-term nor contains bound variables.  With the first such test
\texttt{(cases \textsl{test})} is called.

\subsection{casedist} \texttt{(casedist \textsl{t})}%
\index{casedist@\texttt{casedist}} replaces the goal $A$ containing a
boolean term $t$ by two new goals $\atom(t) \to A(\true)$ and
$(\atom(t) \to \falsityF) \to A(\false)$.
%% and if $t$ is not total also $\mathtt{STotal} \ t$.

\subsection{simp}
\label{SS:simp}
In \texttt{(simp \textsl{opt-dir} \textsl{x} .\
  \textsl{elab-path-and-terms})}\index{simp@\texttt{simp}}, the
optional argument \textsl{opt-dir} is either the string
\inquotes{\texttt{<-}} or missing.  \textsl{x} is
\begin{enumeratei}
\item a number or string identifying a hypothesis form the context,
\item the name of a theorem or global assumption,
\item a closed proof,
\item a formula with free variables from the context, generating a new
  goal.
\end{enumeratei}
The optional \textsl{elab-path-and-terms} is a list consisting of
symbols \texttt{left} or \texttt{right}, giving directions in case the
used formula contains conjunctions, and of terms.  The universal
quantifiers of the used formula are instantiated with appropriate
terms to match a part of the goal.  The terms provided are substituted
for those variables that cannot be inferred.  For the instantiated
premises new goals are created.  This generates a used formula, which
is to be an atom, a negated atom or $t \approx s$.  If it as a
(negated) atom, it is checked whether the kernel or its normal form is
present in the goal.  If so, it is replaced by \texttt{T} (or
\texttt{F}).  If it is an equality $t=s$ or $t \approx s$ with $t$ or
its normal form present in the goal, $t$ is replaced by $s$.  In case
\inquotes{\texttt{<-}} exchange $t$ and $s$.  Example: for \texttt{f}
of type \texttt{nat=>boole} consider the situation
\begin{verbatim}
  f  n  m  EqHyp:n=m
  fHyp:f n
-----------------------------
?_2:[if (f m) False True]=f n
\end{verbatim}
Then the command
\begin{verbatim}
(simp "EqHyp")
\end{verbatim}
generates the a new goal with \texttt{n} replaced by \texttt{m}:
\begin{verbatim}
[if (f m) False True]=f m
\end{verbatim}
We can also change the direction: the command
\begin{verbatim}
(simp "<-" "EqHyp")
\end{verbatim}
generates the a new goal with \texttt{m} replaced by \texttt{n}:
\begin{verbatim}
[if (f n) False True]=f n
\end{verbatim}

\subsection{simp-with}
This is a more verbose form of \texttt{simp}, where the terms are not
inferred via matching, but have to be given explicitly.  Also, for
the instantiated premises one can indicate how they are to come about.
So in \texttt{(simp-with \textsl{opt-dir} \textsl{x} .\
  \textsl{x-list})}\index{simp-with@\texttt{simp-with}},
\textsl{opt-dir} and \textsl{x} are as in \texttt{simp}, and
\textsl{x-list} is a list consisting of
\begin{enumeratei}
\item a number or string identifying a hypothesis form the context,
\item the name of a theorem or global assumption,
\item a closed proof,
\item the string \inquotes{?} (value of \texttt{DEFAULT-GOAL-NAME}),
  generating a new goal,
\item a symbol \texttt{left} or \texttt{right},
\item a term, whose free variables are added to the context,
\item a type, which is substituted for the first type variable,
\item a comprehension term, which is substituted for the first predicate
  variable.
\end{enumeratei}

\subsection{simphyp, simphyp-to}
\texttt{simphyp}\index{simphyp@\texttt{simphyp}} does for forward
chaining the same as \texttt{simp} for backward chaining.  It replaces
the present goal by a new one, with one additional hypothesis obtained
by simplifying a previous one.  Notice that this effect could also be
obtained by \texttt{cut} or \texttt{assert}.  In \texttt{(simphyp
  \textsl{hyp} \textsl{opt-dir} \textsl{x}
  .\ \textsl{elab-path-and-terms})}, \textsl{hyp} is one of the
following.
\begin{enumeratei}
\item A number or string identifying a hypothesis form the context.
\item The name of a theorem or global assumption, but not one whose final
  conclusion is a predicate variable.
\item A closed proof.
\item A formula with free variables from the context, generating a new goal.
\end{enumeratei}
\texttt{simphyp-to}\index{simphyp-to@\texttt{simphyp-to}}
expects a string as its last argument, which is used (via
\texttt{name-hyp}) to name the newly introduced simplified hypothesis.
Example: if in the situation in \ref{SS:simp} above we type
\begin{verbatim}
(simphyp-to "fHyp" "EqHyp" "fHypSimp")
\end{verbatim}
we obtain as new goal
\begin{verbatim}
  f  n  m  EqHyp:n=m
  fHyp:f n
  fHypSimp:f m
-----------------------------
?_2:[if (f m) False True]=f n
\end{verbatim}

\subsection{simphyp-with, simphyp-with-to}
\texttt{simphyp-with}\index{simphyp-with@\texttt{simphyp-with}} is a
more verbose form of \texttt{simphyp}, where the terms are not
inferred via matching, but have to be given explicitly.
\texttt{simphyp-with-to}\index{simphyp-with-to@\texttt{simphyp-with-to}}
again expects a string as its last argument, to be used as name for
the newly introduced simplified hypothesis.

%% \subsection{simphyp-with}
%% \texttt{simphyp-with}\index{simphyp-with@\texttt{simphyp-with}} does
%% for forward chaining the same as \texttt{simp-with} for backward
%% chaining.  It replaces the present goal by a new one, with one
%% additional hypothesis obtained by simplifying a previous one.  Notice
%% that this effect could also be obtained by \texttt{cut} or
%% \texttt{assert}.  In \texttt{(simphyp-with \textsl{opt-dir}
%%   \textsl{hyp} .\ \textsl{x-list})}, \textsl{hyp} is one of the
%% following.
%% \begin{enumeratei}
%% \item A number or string identifying a hypothesis form the context.
%% \item The name of a theorem or global assumption, but not one whose final
%%   conclusion is a predicate variable.
%% \item A closed proof.
%% \item A formula with free variables from the context, generating a new goal.
%% \end{enumeratei}
%% \textsl{x-list} is a list consisting of
%% \begin{enumeratei}
%% \item a number or string identifying a hypothesis form the context,
%% \item the name of a theorem or global assumption,
%% \item a closed proof,
%% \item the string \inquotes{?} (value of \texttt{DEFAULT-GOAL-NAME}),
%%   generating a new goal,
%% \item a symbol \texttt{left} or \texttt{right},
%% \item a term, whose free variables are added to the context,
%% \item a type, which is substituted for the first type variable,
%% \item a comprehension term, which is substituted for the first predicate
%%   variable.
%% \end{enumeratei}
%% This generates a used formula, which is to be an atom, a negated atom
%% or $t \approx s$.  If it as a (negated) atom, it is checked whether
%% the kernel or its normal form is present in the goal.  If so, it is
%% replaced by \texttt{T} (or \texttt{F}).  If it is an equality $t=s$ or
%% $t \approx s$ with $t$ or its normal form present in the goal, $t$ is
%% replaced by $s$.  In case \inquotes{\texttt{<-}} exchange $t$ and $s$.

%% \subsection{simphyp-with-to}
%% \texttt{simphyp-with-to}\index{simphyp-with-to@\texttt{simphyp-with-to}}
%% expects a string as its last argument, which is used (via
%% \texttt{name-hyp}) to name the newly introduced simplified hypothesis.

\subsection{min-pr}
In \texttt{(min-pr \textsl{x} \textsl{measure})}%
\index{min-pr@\texttt{min-pr}}, \textsl{x} is
\begin{enumeratei}
\item a number or string identifying a classical existential hypothesis
  from the context,
\item the name of a classical existential global assumption or theorem,
\item a closed proof on a classical existential formula,
\item a classical existential formula with free variables from the context,
  generating a new goal.
\end{enumeratei}
The result is a new implicational goal, whose premise provides the
(classical) existence of instances with least measure.

We also provide \texttt{exc-formula-to-min-pr-proof}%
\index{exc-formula-to-min-pr-proof@\texttt{exc-formula-to-min-pr-proof}}.
It computes first a gind-aconst (an axiom or a theorem) and from this
a proof of the minimum principle.

\subsection{by-assume-minimal-wrt}
For convenience in classical arguments there is
\texttt{(by-assume-minimal-wrt \textsl{exc-hyp} .\ \textsl{rest})}%
\index{by-assume-minimal-wrt@\texttt{by-assume-minimal-wrt}} where
\textsl{rest} may be called
\textsl{varnames-and-measure-and-minhyp-and-hyps}.  It is meant for
the following situation.  Suppose we are proving a goal $G$ from a
classical existential hypothesis $\excl_{\vec{x}} \vec{A}$.  Then by
the minimum principle we can assume that we have $\vec{x}$ which are
minimal w.r.t.\ a measure $h$ such that $\vec{A}$ are satisfied.

We also provide \texttt{make-gind-aconst}%
\index{make-gind-aconst@\texttt{make-gind-aconst}}.  It takes a
positive integer $n$ and returns an assumption constant for general
induction\index{induction!general} w.r.t.\ a measure function of type
$\alpha_1 \typeTo \dots \typeTo \alpha_n \typeTo \typeN$.

Finally we provide \texttt{make-min-pr-aconst}%
\index{make-min-pr-aconst@\texttt{make-min-pr-aconst}}.  It takes
positive integers $m,n$ and returns an assumption constant for the
minimum principle%
\index{minimum principle} w.r.t.\ a measure function of type $\alpha_1
\typeTo \dots \typeTo \alpha_n \typeTo \typeN$.

\subsection{exc-intro}
In \texttt{(exc-intro \textsl{terms})}%
\index{exc-intro@\texttt{exc-intro}}, the user provides terms to be
used for the present (classical existential) goal.  Moreover we also
provide \texttt{make-exc-intro-aconst}%
\index{make-exc-intro-aconst@\texttt{make-exc-intro-aconst}} and
\texttt{exc-formula-to-exc-intro-aconst}%
\index{exc-formula-to-exc-intro-aconst@\texttt{exc-formula-to-exc-intro-aconst}}

\subsection{exc-elim}
In \texttt{(exc-elim \textsl{x})}\index{exc-elim@\texttt{exc-elim}},
\textsl{x} is
\begin{enumeratei}
\item a number or string identifying a classical existential hypothesis
  from the context,
\item the name of a classical existential global assumption or theorem,
\item a closed proof on a classical existential formula,
\item a classical existential formula with free variables from the context,
  generating a new goal.
\end{enumeratei}
Let $\excl_{\vec{y}} \vec{A}$ be the classical existential formula
identified by \textsl{x}.  The user is then asked to provide a proof
for the present goal, assuming that terms $\vec{y}$ satisfying
$\vec{A}$ are available.  Moreover we also provide
\texttt{make-exc-elim-aconst}%
\index{make-exc-elim-aconst@\texttt{make-exc-elim-aconst}} and
\texttt{exc-formula-to-exc-elim-aconst}%
\index{exc-formula-and-concl-to-exc-elim-aconst@\texttt{exc-for{\dots}-to-exc-elim-aconst}}.

\subsection{pair-elim}
In \texttt{(pair-elim)}\index{pair-elim@\texttt{pair-elim}}, a goal
$\forall_p P(p)$ is replaced by the new goal $\forall_{x_1, x_2}
P(\langle x_1, x_2 \rangle)$.

\subsection{admit}
\texttt{(admit)}\index{admit@\texttt{admit}} temporarily accepts the
present goal, by turning it into a global assumption.

\subsection{search}
We provide a proof search tool \texttt{search}%
\index{search@\texttt{search}} based on Huet's \cite{Huet75}
unification algorithm for the simply typed lambda calculus; its
underlying theory is explained in \ref{S:UnifSearch}.  \texttt{(search
  \textsl{m} \textsl{(name1 m1)} \dots)}\index{search@\texttt{search}}
expects for \textsl{m} a default value for multiplicity (i.e., how
often assumptions are to be used), for \textsl{name1} $\dots$
\begin{enumeratei}
\item numbers of hypotheses from the present context or
\item names for theorems or global assumptions,
\end{enumeratei}
and for \textsl{m1} \dots multiplicities (positive integers for global
assumptions or theorems).  A search is started for a proof of the goal
formula from the given hypotheses with the given multiplicities and in
addition from the other hypotheses (but not any other global
assumptions or theorems) with \textsl{m} or \texttt{mult-default}.  To
exclude a hypothesis from being tried, list it with multiplicity $0$.
One can trace \texttt{search} by setting \texttt{VERBOSE-SEARCH}%
\index{VERBOSE-SEARCH@\texttt{VERBOSE-SEARCH}} to true.

\subsection{auto}
It can be convenient to automate (the easy cases of an) interactive
proof development by iterating \texttt{search} as long as it is
successful in finding a proof.  Then the first goal where it failed is
presented as the new goal.  \texttt{(auto \textsl{m} \textsl{(name1
    m1)} \dots)}\index{auto@\texttt{auto}} takes the same arguments
as \texttt{search}.

\subsection{prop}
\texttt{prop}\index{prop@\texttt{prop}} searches for a proof of the
current goal in minimal propositional logic.  In particular it
provides easy access to the axiom of truth for proving $T$ and to
ex-falso-quodlibed and proof-by-contradiction.  The search meachnism
is based on work of Hudelmaier
\cite{Hudelmaier89,Hudelmaier92,Hudelmaier93} and Dyckhoff
\cite{Dyckhoff92}.  If the search does not succeed, the same is done
for intuitionistic propositional logic (by adding ex-falso-quodlibet
assumptions to the context).  If it does not succeed again, it does
the same for classical propositional logic (by adding stability
assumptions to the context).

\section{Unification and proof search}
\label{S:UnifSearch}
We describe a proof search method suitable for minimal logic with
higher order functionals.  It is based on Huet's \cite{Huet75}
unification algorithm for the simply typed lambda calculus.

Huet's unification algorithm does not terminate in general; this must
be the case, since it is well known that higher order unification is
undecidable.  However, non-termination can be avoided if we restrict
ourselves to a certain fragment of higher order (simply typed) minimal
logic. This fragment is determined by requiring that every higher
order variable $Y$ can only occur in a context $Y \vec{x}$, where
$\vec{x}$ are distinct bound variables in the scope of the operator
binding $Y$, and of opposite polarity.  Note that for first order
logic this restriction does not mean anything, since there are no
higher order variables.  However, when designing a proof search
algorithm for first order logic only, one is naturally led into this
fragment of higher order logic, where the algorithm works as well.

In this section we only present the algorithms and state their
properties.  Proofs can be found in \cite{Schwichtenberg04}.

\subsection{Huet's unification algorithm}
\label{SS:Huet}
We work in the simply typed $\lambda$-calculus, with the usual
conventions.  For instance, whenever we write a term we assume that it
is correctly typed.  \emph{Substitutions}\index{substitution} are
denoted by $\varphi, \psi, \rho$.  The result of applying a
substitution $\varphi$ to a term $r$ or a formula $A$ is written as $r
\varphi$ or $A \varphi$, with the understanding that after the
substitution all terms are brought into long normal form.

$Q$ always denotes a $\forall \exists \forall$-prefix, say
$\forall_{\vec{x}} \exists_{\vec{y}} \forall_{\vec{z}}$, with distinct
variables.  We call $\vec{x}$ the \emph{signature variables}%
\index{variable!signature}, $\vec{y}$ the \emph{flexible
  variables}\index{variable!flexible} and $\vec{z}$ the
\emph{forbidden variables}\index{variable!forbidden} of $Q$, and write
$Q_{\exists}$ for the existential part $\exists_{\vec{y}}$ of $Q$.  A
variable is called \emph{rigid}\index{variable!rigid} if it is either
a signature variable or else a forbidden variable.

A \emph{$Q$-term}\index{Q-term@$Q$-term} is a term with all its free
variables in $Q$, and similarly a \emph{$Q$-formula}%
\index{Q-formula@$Q$-formula} is a formula with all its free variables
in $Q$.  A \emph{$Q$-substitution}%
\index{Q-substitution@$Q$-substitution} is a substitution of
$Q$-terms.

A \emph{unification problem}%
\index{unification problem} $\C{U}$ consists of a $\forall \exists
\forall$-prefix $Q$ and a conjunction $C$ of equations between
$Q$-terms of the same type, i.e., $\bigland_{i=1}^n r_i = s_i$.  We
may assume that each such equation is of the form $\lambda_{\vec{x}} r =
\lambda_{\vec{x}} s$ with the same $\vec{x}$ (which may be empty) and
$r, s$ of ground type.

A \emph{solution}%
\index{solution!to a unification problem} to such a unification
problem $\C{U}$ is a $Q$-substitution $\varphi$ such that for every
$i$, $r_i \varphi = s_i \varphi$ holds (i.e., $r_i \varphi$ and $s_i
\varphi$ have the same normal form).  We sometimes write $C$ as
$\vec{r} = \vec{s}$, and (for obvious reasons) call it a list of
unification pairs.

We now define the unification algorithm.  It takes a unification
problem $\C{U} = QC$ and produces a not necessarily well-founded tree
(called \emph{matching tree}%
\index{matching tree} by Huet \cite{Huet75})
with nodes labelled by unification problems and vertices labelled by
substitutions.

\begin{definition*}[Unification algorithm]
  We distinguish cases according to the form of the unification
  problem, and either give the transition done by the algorithm, or
  else state that it fails.

  \emph{Case} identity, i.e., $Q(r=r \land C)$.  Then
  \begin{equation*}
    Q(r=r \land C) \unifalg{\eps} QC.
  \end{equation*}

  \emph{Case} $\xi$, i.e., $Q(\lambda_{\vec{x}} r = \lambda_{\vec{x}} s
  \land C)$.  We may assume here that the bound variables $\vec{x}$ are
  the same on both sides.
  \begin{equation*}
    Q(\lambda_{\vec{x}}\,r = \lambda_{\vec{x}}\,s \land C)
    \unifalg{\eps} Q\forall_{\vec{x}}(r = s \land C).
  \end{equation*}

  \emph{Case} rigid-rigid, i.e., $Q(f \vec{r} = g \vec{s} \land C)$
  with both $f$ and $g$ rigid, that is either a signature variable or
  else a forbidden variable.  If $f$ is different from $g$ then fail.
  If $f$ equals $g$,
  \begin{equation*}
    Q(f \vec{r} = f \vec{s} \land C) \unifalg{\eps}
    Q(\vec{r} = \vec{s} \land C).
  \end{equation*}

  \emph{Case} flex-rigid, i.e., $Q(u \vec{r} = f \vec{s} \land C)$
  with $f$ rigid.  Then the algorithm branches into one
  \emph{imitation}\index{imitation} branch and $m$ \emph{projection}%
  \index{projection} branches, where $r = r_1, \dots, r_m$.  Imitation
  replaces the flexible head $u$, using the substitution $\rho =
  \subst{}{u}{\lambda_{\vec{x}}(f (h_1 \vec{x}\,) \dots (h_n \vec{x}\,))}$
  with new variables $\vec{h}$ and $\vec{x}$.  This is only allowed if
  $f$ is a signature (and not a forbidden) variable.  For $r_i$ we
  have a projection if and only if the final value type of $r_i$ is
  the (ground) type of $f \vec{s}$.  Then the $i$-th projections pulls
  $r_i$ in front, by $\rho = \subst{}{u}{\lambda_{\vec{x}}(x_i (h_1
    \vec{x}\,) \dots (h_{n_i} \vec{x}\,))}$.  In each of these branches we
  have
  \begin{equation*}
    Q(u \vec{r} = f \vec{s} \land C) \unifalg{\rho}
    Q'(u \vec{r} = f \vec{s} \land C) \rho,
  \end{equation*}
  where $Q'$ is obtained from $Q$ by removing $\exists_u$ and adding
  $\exists_{\vec{h}}$.

  \emph{Case} flex-flex, i.e., $Q(u \vec{r} = v \vec{s} \land C)$.  If
  there is a first flex-rigid or rigid-flex equation in $C$, pull this
  equation (possibly swapped) to the front and apply case flex-rigid.
  Otherwise, i.e., if all equations are between terms with flexible
  heads, pick a new variable $z$ of ground type and let $\rho$ be the
  substitution mapping each of these flexible heads $u$ to
  $\lambda_{\vec{x}} z$.
  \begin{equation*}
    Q(u \vec{r} = v \vec{s} \land C) \unifalg{\rho} Q \emptyset.
  \end{equation*}

  This concludes the definition of the unification algorithm.
\end{definition*}

Clearly $\rho$ is defined on flexible variables of $Q$ only, and its
value terms have no free occurrences of forbidden variables from $Q$.
One can prove correctness and completeness of this algorithm.

\begin{theorem*}[Huet]
  Let a unification problem $\C{U}$ consisting of a $\forall \exists
  \forall$-prefix $Q$ and a list $\vec{r} = \vec{s}$ of unification pairs
  be given.  Then either
  \begin{enumeratea}
  \item the unification algorithm can make a transition, and
    \begin{enumeratei}
    \item (correctness) for every transition $\C{U} \unifalg{\rho}
      \C{U}'$ and $\C{U}'$-solution $\varphi'$ the substitution $(\rho
      \circ \varphi') {\restriction} Q_{\exists}$ is a $\C{U}$-solution,
      and
    \item (completeness) for every $\C{U}$-solution $\varphi$ there is
      a transition $\C{U} \unifalg{\rho} \C{U}'$ and $\C{U}'$-solution
      $\varphi'$ such that $\varphi = (\rho \circ \varphi')
      {\restriction} Q_{\exists}$, and moreover $\mu(\varphi') \le
      \mu(\varphi)$ with $<$ in case flex-rigid, or else
    \end{enumeratei}
  \item the unification algorithm fails, and there is no
    $\C{U}$-solution, or else
  \item the unification algorithm succeeds, and $\vec{r} = \vec{s}$ is
    empty.
  \end{enumeratea}
  Here $\mu(\varphi)$ denotes the number of applications in the value
  terms of $\varphi$.
\end{theorem*}

\begin{corollary*}
  Given a unification problem $\C{U} = QC$, and a success node in the
  matching tree, labelled with a prefix $Q'$ (i.e., a unification
  problem $\C{U}'$ with no unification pairs).  Then by composing the
  substitution labels on the branch leading to this node we obtain a
  pair $(Q', \rho)$ with a \inquotes{transition} substitution $\rho$
  and such that for any $Q'$-substitution $\varphi'$, $(\rho \circ
  \varphi') {\restriction} Q_{\exists}$ is an $\C{U}$-solution.
  Moreover, every $\C{U}$-solution can be obtained in this way, for an
  appropriate success node.  Since the empty substitution is a
  $Q'$-substitution, $\rho {\restriction} Q_{\exists}$ is a
  $\C{U}$-solution, which is most general in the sense stated.
\end{corollary*}

\subsection{The pattern unification algorithm}
\label{SS:PatternMatching}
We restrict the notion of a $Q$-term as follows.  \emph{$Q$-terms}%
\index{Q-term@$Q$-term} are inductively defined by the following
clauses.
\begin{itemize}
\item If $u$ is a universally quantified variable in $Q$ or a
  constant, and $\vec{r}$ are $Q$-terms, then $u \vec{r}$ is a $Q$-term.
\item For any flexible variable $y$ and distinct forbidden variables
  $\vec{z}$ from $Q$, $y \vec{z}$  is a $Q$-term.
  %% $
\item If $r$ is a $Q \forall_z$-term, then $\lambda_z r$ is a
  $Q$-term.
\end{itemize}
Explicitly, $r$ is a $Q$-term iff all its free variables are in $Q$,
and for every subterm $y \vec{r}$ of $r$ with $y$ free in $r$ and
flexible in $Q$, the $\vec{r}$ are distinct variables either
$\lambda$-bound in $r$ (such that $y \vec{r}$ is in the scope of this
$\lambda$) or else forbidden in $Q$.

\emph{$Q$-goals}\index{Q-goal@$Q$-goal} and
\emph{$Q$-clauses}\index{Q-clause@$Q$-clause} are simultaneously
defined by
\begin{itemize}
\item If $\vec{r}$ are $Q$-terms, then $P \vec{r}$ is a $Q$-goal as
well as a $Q$-clause.
\item If $D$ is a $Q$-clause and $G$ is a $Q$-goal, then $D \to G$
is a $Q$-goal.
\item If $G$ is a $Q$-goal and $D$ is a $Q$-clause, then $G \to D$
is a $Q$-clause.
\item If $G$ is a $Q\forall_x$-goal, then $\forall_x G$ is a $Q$-goal.
\item If $\subst{D}{y}{Y \vec{z}\,}$ is a $\forall_{\vec{x}}
  \exists_{\vec{y}, Y} \forall_{\vec{z}}\,$-clause, then $\forall_y D$
  is a $\forall_{\vec{x}} \exists_{\vec{y}} \forall_{\vec{z}}\,$-clause.
\end{itemize}
Explicitly, a formula $A$ is a \emph{$Q$-goal}\index{Q-goal@$Q$-goal}
iff all its free variables are in $Q$, and for every subterm $y
\vec{r}$ of $A$ with $y$ either existentially bound in $A$ (with $y
\vec{r}$ in the scope) or else free in $A$ and flexible in $Q$, the
$\vec{r}$ are distinct variables either $\lambda$- or universally
bound in $A$ (such that $y \vec{r}$ is in the scope) or else free in
$A$ and forbidden in $Q$.

A \emph{$Q$-substitution}\index{Q-substitution@$Q$-substitution} is a
substitution of $Q$-terms.

A \emph{pattern unification problem}%
\index{pattern unification problem} $\C{U}$ consists of a $\forall
\exists \forall$-prefix $Q$ and a conjunction $C$ of equations between
$Q$-terms of the same type, i.e., $\bigland_{i=1}^n(r_i = s_i)$.  We
may assume that each such equation is of the form $\lambda_{\vec{x}} r =
\lambda_{\vec{x}} s$ with the same $\vec{x}$ (which may be empty) and
$r, s$ of ground type.

A \emph{solution}\index{solution} to such a unification problem
$\C{U}$ is a $Q$-substitution $\varphi$ such that for every $i$, $r_i
\varphi = s_i \varphi$ holds (i.e., $r_i \varphi$ and $s_i \varphi$
have the same normal form).  We sometimes write $C$ as $\vec{r} =
\vec{s}$, and (for obvious reasons) call it a list of unification
pairs.

We now define the pattern unification algorithm.  It takes a
unification problem $\C{U} = QC$ and returns a substitution $\rho$ and
another unification problem $\C{U}' = Q'C'$.  Note that $\rho$ will be
neither a $Q$-substitution nor a $Q'$-substitution, but will have the
property that
\begin{enumeratea}
\item $\rho$ is defined on flexible variables of $Q$ only, and its
  value terms have no free occurrences of forbidden variables from $Q$,
\item if $G$ is a $Q$-goal, then $G \rho$ is a $Q'$-goal, and
\item whenever $\varphi'$ is a $\C{U}'$-solution, then $(\rho \circ
  \varphi') {\restriction} Q_{\exists}$ is a $\C{U}$-solution.
\end{enumeratea}

\begin{definition*}[Pattern unification algorithm]
We distinguish cases according to the form of the unification problem,
and either give the transition done by the algorithm, or else state
that it fails.

\emph{Case} identity, i.e., $Q(r=r \land C)$.  Then
\begin{equation*}
  Q(r=r \land C) \unifalg{\eps} QC.
\end{equation*}

\emph{Case} $\xi$, i.e., $Q(\lambda_{\vec{x}}r = \lambda_{\vec{x}}s
\land C)$.  We may assume here that the bound variables $\vec{x}$ are
the same on both sides.
\begin{equation*}
  Q(\lambda_{\vec{x}}r = \lambda_{\vec{x}}s \land C)
  \unifalg{\eps} Q(\forall_{\vec{x}}(r = s) \land C).
\end{equation*}

\emph{Case} rigid-rigid, i.e., $Q(f \vec{r} = f \vec{s} \land C)$
with $f$ either a signature variable or else a forbidden variable.
\begin{equation*}
  Q(f \vec{r} = f \vec{s} \land C) \unifalg{\eps}
  Q(\vec{r} = \vec{s} \land C).
\end{equation*}

\emph{Case} flex-flex with equal heads, i.e., $Q(u \vec{y} = u
\vec{z} \land C)$.
\begin{equation*}
  Q(u \vec{y} = u \vec{z} \land C) \unifalg{\rho} Q'(C \rho)
\end{equation*}
with $\rho = \subst{}{u}{\lambda_{\vec{y}}(u' \vec{w})}$, $Q'$ is $Q$
with $\exists_u$ replaced by $\exists_{u'}$, and $\vec{w}$ an
enumeration of those $y_i$ which are identical to $z_i$ (i.e., the
variable at the same position in $\vec{z}$\,).  Notice that
$\lambda_{\vec{y}}(u' \vec{w}) = \lambda_{\vec{z}}(u' \vec{w})$.

\emph{Case} flex-flex with different heads, i.e., $Q(u \vec{y} =
v \vec{z} \land C)$.
\begin{equation*}
  Q(u \vec{y} = v \vec{z} \land C) \unifalg{\rho} Q' C \rho,
\end{equation*}
where $\rho$ and $Q'$ are defined as follows.  Let $\vec{w}$ be an
enumeration of the variables both in $\vec{y}$ and in $\vec{z}$.  Then
$\rho = \subst{}{u,v}{\lambda_{\vec{y}}(u' \vec{w}\,),
  \lambda_{\vec{z}}(u' \vec{w}\,)}$, and $Q'$ is $Q$ with $\exists_u,
\exists_v$ removed and $\exists_{u'}$ inserted.

\emph{Case} flex-rigid, i.e., $Q(u \vec{y} = t \land C)$ with $t$
rigid, i.e., not of the form $v \vec{z}$ with flexible $v$.

\emph{Subcase} occurrence check: $t$ contains (a critical subterm with
head) $u$.  Then fail.

\emph{Subcase} pruning: $t$ contains a subterm $v \vec{w}_1 z \vec{w}_2$
with $\exists_v$ in $Q$, and $z$ free in $t$ but not in $\vec{y}$.
\begin{equation*}
  Q(u \vec{y} = t \land C) \unifalg{\rho}
  Q'(u \vec{y} = t \rho \land C \rho)
\end{equation*}
where $\rho = \subst{}{v}{\lambda_{\vec{w}_1} \lambda_z
  \lambda_{\vec{w}_2}(v' \vec{w}_1 \vec{w}_2)}$, $Q'$ is $Q$ with
$\exists_v$ replaced by $\exists_{v'}$.

\emph{Subcase} pruning impossible: $\lambda_{\vec{y}} t$ (after all
pruning steps are done still) has a free occurrence of a forbidden
variable $z$.  Then fail.

\emph{Subcase} explicit definition: otherwise.
\begin{equation*}
  Q(u \vec{y} = t \land C) \unifalg{\rho} Q' C \rho
\end{equation*}
where $\rho = \subst{}{u}{\lambda_{\vec{y}} t}$, and $Q'$ is obtained
from $Q$ by removing $\exists_u$.  This concludes the definition of
the pattern unification algorithm.
\end{definition*}

One can prove that this algorithm indeed has the three
properties stated above.  The first one ($\rho$ is defined on flexible
variables of $Q$ only, and its value terms have no free occurrences of
forbidden variables from $Q$) is obvious from the definition.  We now
state the second one; the third one will be stated next.

\begin{lemma*}[$Q'$-goals]
  If $Q \unifalg{\rho} Q'$ and $G$ is a $Q$-goal, then $G \rho$ is a
  $Q'$-goal.
\end{lemma*}

Let $Q \unifprefixalg{\rho} Q'$ mean that for some $C, C'$ we have $QC
\unifalg{\rho} Q'C'$.  Write $Q \unifprefixalg{\rho}^* Q'$ if there
are $\rho_1, \dots, \rho_n$ and $Q_1, \dots, Q_{n-1}$ such that
\begin{equation*}
  Q \unifprefixalg{\rho_1} Q_1 \unifprefixalg{\rho_2} \dots
  \unifprefixalg{\rho_{n-1}} Q_{n-1} \unifprefixalg{\rho_n} Q',
\end{equation*}
and $\rho = \rho_1 \circ \dots \circ \rho_n$.

\begin{corollary*}
  If $Q \unifprefixalg{\rho}^* Q'$ and $G$ is a $Q$-goal, then $G
  \rho$ is a $Q'$-goal.
\end{corollary*}

\begin{lemma*}
  Let a unification problem $\C{U}$ consisting of a $\forall \exists
  \forall$-prefix $Q$ and a list $\vec{r} = \vec{s}$ of unification pairs
  be given.  Then either
  \begin{enumeratea}
  \item the unification algorithm makes a transition $\C{U}
    \unifalg{\rho} \C{U}'$, and
    \begin{align*}
      \Phi' \colon &\C{U}'\hbox{-solutions} \to \C{U}\hbox{-solutions}\\
      &\varphi' \mapsto (\rho \circ \varphi') {\restriction} Q_{\exists}
    \end{align*}
    is well-defined and we have $\Phi \colon \C{U}\hbox{-solutions} \to
    \C{U}'\hbox{-solutions}$ such that $\Phi'$ is inverse to $\Phi$, i.e.\
    $\Phi'(\Phi \varphi) = \varphi$, or else
  \item the unification algorithm fails, and there is no
    $\C{U}$-solution.
  \end{enumeratea}
\end{lemma*}

It is not hard to see that the unification algorithm terminates, by
defining a measure that decreases with each transition.

\begin{corollary*}
  Given a unification problem $\C{U} = QC$, the unification algorithm
  either fails, and there is no $\C{U}$-solution, or else returns a pair
  $(Q', \rho)$ with a \inquotes{transition} substitution $\rho$ and a
  prefix $Q'$ (i.e., a unification problem $\C{U}'$ with no unification
  pairs) such that for any $Q'$-substitution $\varphi'$, $(\rho \circ
  \varphi') {\restriction} Q_{\exists}$ is an $\C{U}$-solution, and every
  $\C{U}$-solution can be obtained in this way.  Since the empty
  substitution is a $Q'$-substitution, $\rho {\restriction} Q_{\exists}$
  is a $\C{U}$-solution, which is most general in the sense
  stated.
\end{corollary*}

\subsection{Proof search}
\label{SS:Search}
A \emph{$Q$-sequent}\index{Q-sequent@$Q$-sequent} has the form $\C{P}
\Rightarrow G$, where $\C{P}$ is a list of $Q$-clauses and $G$ is a
$Q$-goal.

We write $M[\C{P}]$ to indicate that all assumption variables in the
derivation $M$ are assumptions of clauses in $\C{P}$.

Write $\vdash^n S$ for a set $S$ of sequents if there are derivations
$M_i^{G_i}[\C{P}_i]$ in long normal form for all $(\C{P}_i \Rightarrow
G_i) \in S$ such that $\sum \dep{M_i} \le n$.  Let $\vdash^{<n} S$
mean $\exists_{m<n} \vdash^m S$.

We prove correctness and completeness of the proof search procedure:
correctness is the if-part of the two lemmata to follow, and
completeness the only-if-part.

\begin{lemma*}
  Let $Q$ be a $\forall \exists \forall$-prefix, $\{\C{P} \Rightarrow
  \forall_{\vec{x}}(\vec{D} \to A)\} \cup S$ $Q$-sequents with $\vec{x},
  \vec{D}$ not both empty.  Then we have for every substitution
  $\varphi$:
  \begin{equation*}
    \hbox{$\varphi$ is a $Q$-substitution such that $\vdash^n
      \bigl(\{\C{P} \Rightarrow \forall_{\vec{x}}(\vec{D} \to A)\} \cup S\bigr)
      \varphi$}
  \end{equation*}
  if and only if
  \begin{equation*}
    \hbox{$\varphi$ is a $Q \forall_{\vec{x}}$-substitution such that
      $\vdash^{<n}
      \bigl(\{\C{P} \cup \vec{D} \Rightarrow A\} \cup S\bigr) \varphi$.}
  \end{equation*}
\end{lemma*}

\begin{proof}
  \emph{\inquotes{If}}.  Let $\varphi$ be a $Q
  \forall_{\vec{x}}$-substitution and $\vdash^{<n} \bigl(\{\C{P} \cup
    \vec{D} \Rightarrow A\} \cup S\bigr) \varphi$.  So we have
  \begin{equation*}
    N^{A \varphi}[\vec{D} \varphi \cup \C{P} \varphi].
  \end{equation*}
  Since $\varphi$ is a $Q \forall_{\vec{x}}$-substitution, no variable in
  $\vec{x}$ can be free in $\C{P} \varphi$, or free in $y \varphi$ for
  some $y \in \dom(\varphi)$.  Hence
  \begin{equation*}
    M^{(\forall_{\vec{x}}(\vec{D} \to A))\varphi}[\C{P} \varphi] :=
    \lambda_{\vec{x}} \lambda_{\vec{u}^{\vec{D} \varphi}} N
  \end{equation*}
  is a correct derivation.

  \emph{\inquotes{Only if}}.  Let $\varphi$ be a $Q$-substitution and
  $\vdash^n \bigl(\{\C{P} \Rightarrow \forall_{\vec{x}}(\vec{D} \to A)\} \cup
  S\bigr) \varphi$.  This means we have a derivation (in long normal
  form)
  \begin{equation*}
    M^{(\forall_{\vec{x}}(\vec{D} \to A))\varphi}[\C{P} \varphi] =
    \lambda_{\vec{x}} \lambda_{\vec{u}^{\vec{D} \varphi}}(N^{A \varphi}
    [\vec{D} \varphi \cup \C{P} \varphi]).
  \end{equation*}
  Now $\dep{N} < \dep{M}$, hence $\vdash^{<n} \bigl(\{\C{P} \cup \vec{D}
  \Rightarrow A\} \cup S\bigr) \varphi$, and $\varphi$ clearly is a $Q
  \forall_{\vec{x}}$-substitution.
\end{proof}

\begin{lemma*}
  Let $Q$ be a $\forall \exists \forall$-prefix, $\{\C{P} \Rightarrow P
  \vec{r}\} \cup S$ $Q$-sequents and $\varphi$ a substitution.  Then
  \begin{equation*}
    \hbox{$\varphi$ is a $Q$-substitution such that
      $\vdash^n \bigl(\{\C{P} \Rightarrow P \vec{r}\} \cup S\bigr) \varphi$}
  \end{equation*}
  if and only if there is a clause $\forall_{\vec{x}}(\vec{G} \to P
  \vec{s}\,)$ in $\C{P}$ such that the following holds.  Let $\vec{z}$
  be the final universal variables in $Q$, $\vec{X}$ be new
  (\inquotes{raised}) variables such that $X_i \vec{z}$ has the same
  type as $x_i$, let $Q^*$ be $Q$ with the existential variables
  extended by $\vec{X}$, and let $*$ indicate the substitution
  $\subst{}{x_1, \dots, x_n}{X_1 \vec{z}, \dots, X_n \vec{z}\,}$.  Then
  there is a result $(Q', \rho)$ of either Huet's or the pattern
  unification algorithm applied to $Q^*(\vec{r}=\vec{s}^*)$ and a
  $Q'$-substitution $\varphi'$ such that $\vdash^{<n} \bigl(\{\C{P}
  \Rightarrow \vec{G}^*\} \cup S\bigr) \rho \varphi'$, and $\varphi =
  (\rho \circ \varphi'){\restriction} Q_{\exists}$.
\end{lemma*}

\begin{proof}
  \emph{\inquotes{If}}.  Let $(Q', \rho)$ be such a result, and assume
  that $\varphi'$ is a $Q'$-substitution such that $N_i \vdash
  \bigl(\C{P} \Rightarrow \vec{G}^*\bigr) \rho \varphi'$.  Let $\varphi :=
  (\rho \circ \varphi') {\restriction} Q_{\exists}$.  From
  $\unif{Q^*}{\vec{r}}{\vec{s}^*} = (Q', \rho)$ we know $\vec{r} \rho =
  \vec{s}^* \rho$, hence $\vec{r} \varphi = \vec{s}^* \rho \varphi'$.
  Then
  \begin{equation*}
    u^{(\forall_{\vec{x}}. \vec{G} \to P \vec{s}) \varphi}
    ( (\vec{X} \rho \varphi') \vec{z}\,)
    \vec{N}^{\vec{G}^* \rho \varphi'}
  \end{equation*}
  derives $P \vec{s}^* \rho \varphi'$ (i.e., $P \vec{r} \varphi$) from
  $\C{P} \varphi$.

  \emph{\inquotes{Only if}}.  Assume $\varphi$ is a $Q$-substitution
  such that $\vdash (\C{P} \Rightarrow P \vec{r}\,) \varphi$, say by
  $u^{\forall_{\vec{x}}(\vec{G} \to P \vec{s})\varphi} \vec{t}
  \vec{N}^{\subst{(\vec{G} \varphi)}{\vec{x}}{\vec{t}}}$, with
  $\forall_{\vec{x}}(\vec{G} \to P \vec{s})$ a clause in $\C{P}$, and
  with additional assumptions from $\C{P} \varphi$ in $\vec{N}$.  Then
  $\vec{r} \varphi = \subst{(\vec{s} \varphi)}{\vec{x}}{\vec{t}}$.
  Since we can assume that the variables $\vec{x}$ are new and in
  particular not range variables of $\varphi$, with
  \begin{equation*}
    \vartheta := \varphi \cup \subst{}{\vec{x}}{\vec{t}}
  \end{equation*}
  we have $\vec{r} \varphi = \vec{s} \vartheta$.  Let $\vec{z}$ be the
  final universal variables in $Q$, $\vec{X}$ be new (\inquotes{raised})
  variables such that $X_i \vec{z}$ has the same type as $x_i$, let
  $Q^*$ be $Q$ with the existential variables extended by $\vec{X}$, and
  for terms and formulas let $*$ indicate the substitution
  $\subst{}{x_1, \dots, x_n}{X_1 \vec{z}, \dots, X_n \vec{z}\,}$.
  Moreover, let
  \begin{equation*}
    \vartheta^* :=
    \varphi \cup \subst{}{X_1, \dots, X_n}{\lambda_{\vec{z}} t_1,
      \dots, \lambda_{\vec{z}} t_n}.
  \end{equation*}
  Then $\vec{r} \vartheta^* = \vec{r} \varphi = \vec{s} \vartheta =
  \vec{s}^* \vartheta^*$, i.e., $\vartheta^*$ is a solution to the
  unification problem given by $Q^*$ and $\vec{r} = \vec{s}$.  Hence
  by the corollary $\unif{Q^*}{\vec{r}}{\vec{s}^*} = (Q', \rho)$ and
  there is a $Q'$-substitution $\varphi'$ such that $\vartheta^* =
  (\rho \circ \varphi'){\restriction} Q^*_{\exists}$, hence $\varphi =
  (\rho \circ \varphi'){\restriction} Q_{\exists}$.  Also,
  $\subst{(\vec{G} \varphi)}{\vec{x}}{\vec{t}\,} = \vec{G} \vartheta =
  \vec{G}^* \vartheta^* = \vec{G}^* \rho \varphi'$.
\end{proof}

A \emph{state}\index{state} is a pair $(Q,S)$ with $Q$ a prefix and
$S$ a finite set of $Q$-sequents.  By the two lemmas just proved we
have \emph{state transitions}%
\index{state transition}
\begin{align*}
  (Q,\{\C{P} \Rightarrow \forall_{\vec{x}}(\vec{D} \to A)\} \cup S)
  &\mapsto^{\eps}
  (Q \forall_{\vec{x}}, \{\C{P} \cup \vec{D} \Rightarrow A\} \cup S)
  \\
  (Q,\{\C{P} \Rightarrow P \vec{r}\} \cup S) &\mapsto^{\rho}
  (Q',(\{\C{P} \Rightarrow \vec{G}^*\} \cup S) \rho),
\end{align*}
where in the latter case there is a clause $\forall_{\vec{x}}(\vec{G}
\to P \vec{s}\,)$ in $\C{P}$ such that the following holds.  Let
$\vec{z}$ be the final universal variables in $Q$, $\vec{X}$ be new
(\inquotes{raised}) variables such that $X_i \vec{z}$ has the same
type as $x_i$, let $Q^*$ be $Q$ with the existential variables
extended by $\vec{X}$, and let $*$ indicate the substitution
$\subst{}{x_1, \dots, x_n}{X_1 \vec{z}, \dots, X_n \vec{z}\,}$, and
$\unif{Q^*}{\vec{r}}{\vec{s}^*} = (Q', \rho)$.

Notice that by the lemma on $Q'$-goals above, if $\C{P} \Rightarrow P
\vec{r}$ is a \emph{$Q$-sequent}\index{Q-sequent@$Q$-sequent} (which
means that $\bigland \C{P} \to P \vec{r}$ is a $Q$-goal), then $(\C{P}
\Rightarrow \vec{G}^*) \rho$ is a $Q'$-sequent.

\begin{theorem*}
  Let $Q$ be a prefix, and $S$ be a set of $Q$-sequents.  For every
  substitution $\varphi$ we have: $\varphi$ is a $Q$-substitution
  satisfying $\vdash S\varphi$ iff there is a prefix $Q'$, a
  substitution $\rho$ and a $Q'$-substitution $\varphi'$ such that
  \begin{align*}
    &(Q,S) \mapsto^{\rho *} (Q', \emptyset),
    \\
    &\varphi = (\rho \circ \varphi'){\restriction} Q_{\exists}.
  \end{align*}
\end{theorem*}

\begin{examples*}
  \begin{enumeratea}
  \item The sequent $\forall_y(\forall_z Ryz \to Q), \forall_{y_1,y_2}
    Ry_1y_2 \Rightarrow Q$ leads first to $\forall_{y_1,y_2} Ry_1y_2
    \Rightarrow Ryz$ under $\exists_y\forall_z$, then to $y_1 = y
    \land y_2 = z$ under $\exists_y\forall_z \exists_{y_1,y_2}$, and
    finally to $Y_1 z = y \land Y_2 z = z$ under $\exists_{y,Y_1,
      Y_2}\forall_z$, which has the solution $Y_1 = \lambda_z y$, $Y_2
    = \lambda_z z$.
  \item $\forall_y(\forall_z Ryz \to Q), \forall_{y_1} Ry_1y_1
    \Rightarrow Q$ leads first to $\forall_{y_1} Ry_1y_1 \Rightarrow
    Ryz$ under $\exists_y \forall_z$, then to $y_1 = y \land y_1 = z$
    under $\exists_y \forall_z \exists_{y_1}$, and finally to $Y_1 z =
    y \land Y_1 z = z$ under $\exists_{y, Y_1} \forall_z$, which has
    no solution.
  \item Here is a more complex example (derived from proofs of the
    Orevkov-formulas), for which we only give the derivation tree.
    \def\defaultHypSeparation{\hskip.08in}
    \def\ScoreOverhang{.5pt}
    \begin{equation*}
      \AxiomC{$\forall_y(\forall_z( Ryz {\to} \falsum)) {\to} \falsum$}
      \UnaryInfC{$\forall_z( R0z {\to} \falsum) {\to} \falsum$}
      \AxiomC{$\forall_y(\forall_{z_1}( Ryz_1 {\to} \falsum) {\to} \falsum)$}
      \UnaryInfC{$\forall_{z_1}( Rzz_1 {\to} \falsum) {\to} \falsum$}
      \AxiomC{$\forall_z(S0z {\to}\falsum)$}
      \UnaryInfC{$S0z_1 {\to}\falsum$}
      \AxiomC{$(*)$}
      \AxiomC{$R0z$}
      \AxiomC{$Rzz_1$}
      \TrinaryInfC{$S0z_1$}
      \BinaryInfC{$\falsum$}
      \UnaryInfC{$Rzz_1 {\to} \falsum$}
      \UnaryInfC{$\forall_{z_1}(Rzz_1 {\to} \falsum)$}
      \BinaryInfC{$\falsum$}
      \UnaryInfC{$R0z {\to} \falsum$}
      \UnaryInfC{$\forall_z(R0z {\to} \falsum)$}
      \BinaryInfC{$\falsum$}
      \DisplayProof
    \end{equation*}
    where $(*)$ is a derivation from Hyp$_1 \colon \forall_{z,z_1}(R0z
    \to Rz z_1 \to S0z_1)$.
  \end{enumeratea}
\end{examples*}

\subsection{Extension by $\land$ and $\exists$}
\label{SS:Ext}
The extension by conjunction is rather easy; it is even superfluous in
principle, since conjunctions can always be avoided at the expense of
having lists of formulas instead of single formulas.

However, having conjunctions available is clearly useful at times, so
let's add it.  This requires the notion of an \emph{elaboration path}%
\index{elaboration path} for a formula (cf.\ \cite{Miller91b}).  The
reason is that the property of a formula to have a unique atom as its
\emph{head}\index{head} is lost when conjunctions are present.  An
elaboration path is meant to give the directions (left or right) to go
when we encounter a conjunction as a strictly positive subformula.
For example, the elaboration paths of $\forall_x A \land (B \land C
\to D \land \forall_y E)$ are $(\texttt{left})$, $(\texttt{right},
\texttt{left})$ and $(\texttt{right}, \texttt{right})$.  Clearly, a
formula is equivalent to the conjunction (over all elaboration paths)
of all formulas obtained from it by following an elaboration path
(i.e., always throwing away the other part of the conjunction).  In
our example,
\begin{equation*}
  \forall_x A \land (B \land C \to D \land \forall_y E) \leftrightarrow
  \forall_x A \land (B \land C \to D) \land (B \land C \to \forall_y E).
\end{equation*}
In this way we regain the property of a formula to have a unique head,
and our previous search procedure continues to work.

For the existential quantifier $\exists$ the problem is of a
different nature.  We chose to introduce $\exists$ by means of axiom
schemata.  Then the problem is which of such schemes to use in
proof search, given a goal $G$ and a set $\C{P}$ of clauses.  We
might proceed as follows.

List all prime, positive and negative existential subformulas of
$\C{P} \Rightarrow G$, and remove any formula from those lists which
is of the form of another one\footnote{To do this, for patterns the
  dual of the theory of \inquotes{most general unifiers}, i.e., a
  theory of \inquotes{most special generalizations}, needs to be
  developed.}.  For every positive existential formula -- say
$\exists_x B$ -- add (the generalization of) the existence
introduction scheme
\begin{equation*}
  \exI_{x,B} \colon \forall_x(B \to \exists_x B)
\end{equation*}
to $\C{P}$.  Moreover, for every negative existential formula -- say
$\exists_x A$ -- and every (prime or existential) formula $C$ in any
of those two lists, except the formula $\exists_x A$ itself, add (the
generalization of) the existence elimination scheme
\begin{equation*}
  \exE_{x,A,B} \colon \exists_x A \to \forall_x(A \to B) \to B
\end{equation*}
to $\C{P}$.  Then start the search algorithm as described in
section~\ref{SS:Search}.  The normal form theorem for the natural
deduction system of minimal logic with $\exists$ then guarantees
completeness.

However, experience has shown that this complete search procedure
tends to be trapped in too large a search space.  Therefore in our
actual implementation we decided to only take instances of the
existence elimination scheme with \emph{existential} conclusions.

Moreover, it seems appropriate that -- before the search is started --
one eliminates in a preprocessing step as many existential quantifiers
as possible.
%% We shall discuss in the following to what extent this
%% might be done.  As a preparation, we first prove a lemma.

%% Call a formula \emph{decidable}\index{formula!decidable}, if it is
%% built from atoms $\atom(t)$ and contains boolean quantifiers only
%% (i.e., only quantifiers $\forall_b$ or $\ex_b$ with variables $b$ of
%% type $\typeB$).

%% \begin{remark*}
%%   Notice that this is a rather crude syntactical notion of
%%   decidability for formulas; a more elaborate version would also allow
%%   bounded quantification over, e.g., the natural numbers.  The lemma
%%   to follow says that it implies the derivability of $D \lor \neg D$,
%%   for the usual second order definition of disjunction $\lor$.
%% \end{remark*}

%% \begin{lemma*}[Case distinction on decidable formulas]
%%   For decidable formulas $D$ we have
%%   \begin{equation*}
%%   \vdash_i (D \to A) \to (\neg D \to A) \to A.
%%   \end{equation*}
%% \end{lemma*}

%% \begin{proof}
%%   By induction on $D$.  \emph{Case} $\atom(t)$.  To prove
%%   \begin{equation*}
%%   \vdash (\atom(p) \to A) \to (\neg \atom(p) \to A) \to A,
%%   \end{equation*}
%%   use boolean induction, and the truth axiom $\atom(\true)$.

%%   \emph{Case} $D \to E$.  We have to show
%%   \begin{equation*}
%%   \vdash_i ((D \to E) \to A) \to (\neg (D \to E) \to A) \to A.
%%   \end{equation*}
%%   By induction hypothesis it suffices to argue by cases on $D$ and
%%   $E$.  We proceed informally.  If $E$ holds, then we have $D \to E$
%%   and hence $A$.  If $\neg E$ holds, then distinguish cases on $D$.
%%   If $D$ holds, then $\neg(D \to E)$, hence $A$.  If $\neg D$ holds,
%%   then $D \to E$ by ex-falso, hence $A$.

%%   \emph{Case} $D \land E$.  Easy, again using the induction
%%   hypothesis, and cases on $D$ and $E$.

%%   \emph{Case} $\ex_p D$.  We must show
%%   \begin{equation*}
%%     \vdash_i (\ex_p D \to A) \to (\neg \ex_p D \to A) \to A.
%%   \end{equation*}
%%   By induction hypothesis it suffices to argue by cases on
%%   $\subst{D}{p}{\true}$ and $\subst{D}{p}{\false}$.  If either
%%   $\subst{D}{p}{\true}$ or $\subst{D}{p}{\false}$ hold, then we
%%   clearly have $A$, by the assumption $\ex_p D \to A$.  If both $\neg
%%   \subst{D}{p}{\true}$ and $\neg \subst{D}{p}{\false}$ hold, then
%%   $\forall_p \neg D$ (by boolean induction), hence $\neg \ex_p D$,
%%   hence $A$ by the assumption $\neg \ex_p D \to A$.

%%   \emph{Case} $\forall_p D$.  We must show
%%   \begin{equation*}
%%     \vdash_i (\forall_p D \to A) \to (\neg \forall_p D \to A) \to A.
%%   \end{equation*}
%%   Use $\forall_p D \leftrightarrow \subst{D}{p}{\true} \land
%%   \subst{D}{p}{\false}$ (provable by boolean induction), and the
%%   induction hypothesis for $\subst{D}{p}{\true}$ and
%%   $\subst{D}{p}{\false}$.
%% \end{proof}

%% \begin{lemma*}[Independence of premise for decidable formulas]
%%   For decidable formulas $D$ we have
%%   \begin{equation*}
%%     \vdash_i (D \to \ex_x A) \to \ex_x(D \to A).
%%   \end{equation*}
%% \end{lemma*}

%% \begin{proof}
%%   One can see easily that
%%   \begin{align*}
%%     &\vdash (D \to \ex_x A) \to D \to \ex_x(D \to A),
%%     \\
%%     &\vdash_i \neg D \to \ex_x(D \to A).
%%   \end{align*}
%%   Now use case distinction on decidable formulas (i.e., the lemma above).
%% \end{proof}

%% We now assign to every formula $A$ an \inquotes{existentially reduced}
%% formula $A^*$ equivalent to $A$.  It is obtained by first moving
%% existential quantifiers to the front (as much as possible), and then
%% -- if they appear to the left of an implication -- changing them into
%% universal quantifiers (again as much as possible).  Both processes are
%% done simultaneously in the following definition.

%% \begin{definition*}[$A^*$]
%%   In the recursive cases below, assume $A^* = \ex_{\vec{x}} A_0$
%%   with $A_0$ not an existential formula, and similarly for $B$.
%%   \begin{align*}
%%     \atom(t)^* &:= \atom(t),
%%     \\
%%     (A \land B)^* &:= \ex_{\vec{x}} \ex_{\vec{y}}(A_0 \land B_0),
%%     \\
%%     (A \to B)^* &:= \begin{cases}
%%       \ex_{\vec{y}}(A^* \to B_0) &\text{if $A$ is decidable,}
%%       \\
%%       \forall_{\vec{x}}(A_0 \to B^*) &\text{else, and $\vec{x}$ not empty,}
%%       \\
%%       A^* \to B^* &\text{otherwise,}
%%     \end{cases}
%%     \\
%%     (\forall_x A)^* &:= \forall_x A^*,
%%     \\
%%     (\ex_x A)^* &:= \ex_x A^*.
%%   \end{align*}
%% \end{definition*}

%% \begin{theorem*}
%%   $\vdash_i A \leftrightarrow A^*$.
%% \end{theorem*}

%% \begin{proof}
%%   Use independence of premise for decidable formulas.
%% \end{proof}

\subsection{Implementation}
\label{SS:SearchImpl}
Following Miller\index{Miller} \cite{Miller91b}, Berger\index{Berger}
and \cite{Schwichtenberg04}, we have implemented a proof search
algorithm for minimal logic.  To enforce termination, every assumption
can only be used a fixed number of times.

We work with lists of sequents instead of single sequents; they all
are $Q$-sequents for the same prefix $Q$.  One then searches for a
$Q$-substitution $\varphi$ and proofs of the $\varphi$-substituted
sequents.  \texttt{intro-search}%
\index{intro-search@\texttt{intro-search}} takes the first sequent and
extends $Q$ by all universally quantified variables $x_1 \dots$.  It
then calls \texttt{select}\index{select@\texttt{select}}, which
selects (using \texttt{or}) a fitting clause.  If one is found, a new
prefix $Q'$ (raising the new flexible variables) is formed, and the
$n$ ($\ge 0$) new goals with their clauses (and also all remaining
sequents) are substituted with $\texttt{star} \circ \rho$, where
\texttt{star} is the \inquotes{rai\-sing} substitution and $\rho$ is
the most general unificator.  For this constellation
\texttt{intro-search} is called again.  In case of success, one
obtains a $Q'$-substitution $\varphi'$ and proofs of the
$\texttt{star} \circ \rho \circ \varphi'$ -substituted new sequents.
Let $\varphi \defeq (\rho \circ \varphi') {\restriction} Q_{\ex}$, and
take the first $n$ proofs of these to build a proof of the
$\varphi$-substituted (first) sequent originally considered by
\texttt{intro-search}.

\subsection{Notes}
The present treatment benefitted from a presentation of Miller's
\cite{Miller91b} given by Ulrich Berger, in a logic seminar in
M\"unchen in 1991.  The type of restriction to higher order terms
described in the text has been introduced in \cite{Miller91b}; it has
been called \emph{patterns}\index{pattern} by Nipkow \cite{Nipkow91}.
Miller also noted its relevance for extensions of logic programming,
and showed that the unification problem for patterns is solvable and
admits most general unifiers.  The present treatment was motivated by
the desire to use Miller's approach as a basis for an implementation
of a simple proof search engine for (first and higher order) minimal
logic.

Compared with Miller \cite{Miller91b}, we make use of several
simplifications, optimizations and extensions, in particular the
following.
\begin{enumeratei}
\item Instead of arbitrarily mixed prefixes we only use those of
  the form $\forall \ex \forall$.  Nipkow in \cite{Nipkow91} already
  had presented a version of Miller's pattern unification algorithm for
  such prefixes, and Miller in \cite[section~9.2]{Miller91b} notes that
  in such a situation any two unifiers can be transformed into each
  other by a variable renaming substitution.  Here we restrict ourselves
  to $\forall \ex \forall$-prefixes throughout, i.e., in the proof
  search algorithm as well.
\item The order of events in the pattern unification algorithm is
  changed slightly, by postponing the raising step until it is really
  needed.  This avoids unnecessary creation of new higher type
  variables.  -- Already Miller noted in \cite[p.515]{Miller91b} that
  such optimizations are possible.
\item The extensions concern the (strong) existential quantifier,
  which has been left out in Miller's treatment, and also
  conjunction(cf.\ \ref{SS:Ext}).  The latter can be avoided in
  principle, but of course is a useful thing to have.
\end{enumeratei}

\section{Extracted terms}
\label{S:ExtrTerms}

\subsection{The type of a formula}
\label{SS:TypeOfFormula}
We assign to every formula $A$ an object $\tau(A)$, a type or the
nulltype\index{nulltype} symbol (written $\nulltype$ in text and
displayed $\verb#eps#$ in Minlog).  $\tau(A)$ is intended to be the
type of the program to be extracted from a proof of $A$.  This is done
by
\begin{alignat*}{2}
  &\texttt{(formula-to-et-type \textsl{formula})},%
  \index{formula-to-et-type@\texttt{formula-to-et-type}}
  \\
  &\texttt{(idpreconst-to-et-type \textsl{idpc})}.%
  \index{idpreconst-to-et-type@\texttt{idpreconst-to-et-type}}
\end{alignat*}
Both are defined simultaneously; this makes sense, since the clauses
and also the comprehension terms of an idpredconst are prior to the
idpredconst.

In \texttt{formula-to-et-type} we assign type variables to the
predicate variables.  For to be able to later refer to this
assignment, we use a global variable \texttt{PVAR-TO-TVAR-ALIST},
which memorizes the assigment done so far.  Later reference is
necessary, because such type variables will appear in extracted
programs of theorems involving predicate variables, and in a given
development there may be many auxiliary lemmata containing the same
predicate variable.  A fixed \texttt{PVAR-TO-TVAR} refers to and
updates \texttt{PVAR-TO-TVAR-ALIST}.

We also define separately
\begin{alignat*}{2}
  &\texttt{(formula-of-nulltype?\ \textsl{formula})},%
  \index{formula-of-nulltype?@\texttt{formula-of-nulltype?}}
\end{alignat*}
since this test can be done more efficiently.

\subsection{Extracted terms}
\label{SS:ExtrTerms}
We can define, for a given derivation $M$ of a formula $A$ with
$\tau(A) \ne \nulltype$, its \emph{extracted term}%
\index{extracted term} (or \emph{extracted program}%
\index{extracted program}) $\extrTer{M}$ of type $\tau(A)$.  We also
need extracted terms for the axioms.  For induction we take recursion,
for the proof-by-cases axiom we take the cases-construct for terms;
for the other axioms the extracted terms are rather clear.  Term
extraction is implemented by
\begin{alignat*}{2}
  &\texttt{(proof-to-extracted-term \textsl{proof-or-thm-name})}.%
  \index{proof-to-extracted-term@\texttt{proof-to-extracted-term}}
\end{alignat*}
Hence \texttt{proof-to-extracted-term} gets either a proof
or else a theorem name.  In the former case it works its way through
the proof, until it comes to an assumption variable, an axiom, a
theorem or a global assumption.  When it is a theorem,
\texttt{theorem-to-extracted-term} is called.  This also happens in
when a theorem name is the input.  \texttt{theorem-to-extracted-term}
applies as its default operation
\texttt{theorem-or-global-assumption-to-pconst}, where in case of a
lemma \texttt{L} the pconst has name \texttt{cL}.

When we want to execute the program, we have to replace the constant
\texttt{cL} corresponding to lemma \texttt{L} by the extracted
program of its proof, and the constant \texttt{cGA} corresponding to a
global assumption \texttt{GA} by an assumed extracted term to be
provided by the user.  This can be achieved by adding computation
rules for \texttt{cL} and \texttt{cGA}.  We can be rather flexible
here and enable/block rewriting by using
\texttt{animate}/\texttt{deanimate} as desired.  Notice that the type
of the extracted term provided for a \texttt{cGA} must be the
extracted type of the assumed formula.  When predicate variables are
present, one must use the type variables assigned to them in
\texttt{PVAR-TO-TVAR-ALIST}.
\begin{alignat*}{2}
  &\texttt{(animate \textsl{thm-or-ga-name} .\ \textsl{opt-eterm})},%
  \index{animate@\texttt{animate}}
  \\
  &\texttt{(deanimate \textsl{thm-or-ga-name})}.%
  \index{deanimate@\texttt{deanimate}}
\end{alignat*}

The constant \texttt{cL} will unfold under normalization if the lemma
is animated.  However, in some cases
\texttt{theorem-to-extracted-term} directly gives short and meaningful
terms:
\begin{alignat*}{2}
  \mathtt{InhabTotal} &\mapsto \mathtt{(Inhab\ rho)},
  \\
  \mathtt{AllAllPartial, AllPartialAll} &\mapsto \mathtt{[x]x},
  %% \\
  %% \mathtt{AllPartialAll} &\mapsto \mathtt{[x]x},
  \\
  \mathtt{ExExPartial, ExPartialEx} &\mapsto \mathtt{[x]x},
  %% \\
  %% \mathtt{ExPartialEx} &\mapsto \mathtt{[x]x},
  \\
  \mathtt{Pconst+Total} &\mapsto \mathtt{pconst},
  \\
  \mathtt{Pconst+STotal} &\mapsto \mathrm{the\ extract\ from\ the\ proof},
  \\
  \mathtt{AlgEqTotal} &\mapsto \mathtt{[n,m]n=m},
  \\
  \mathtt{BooleIfTotal} &\mapsto \mathtt{[free][if\ test\ arg1\ arg2]},
  \\
  \mathtt{EqDCompat, EqDCompatRev} &\mapsto \mathtt{[x]x},
  \\
  \mathtt{Id} &\mapsto \mathtt{[x]x},
  \quad\hbox{if \texttt{unfold-let-flag} is true}.
\end{alignat*}
Here is an example.  It is easy to prove \texttt{NatEqTotal}:
\begin{equation*}
  \allnc_{\hat{n}}(\GTotal_{\typeN} \hat{n} \to
  \allnc_{\hat{m}}(\GTotal_{\typeN} \hat{m} \to
  \GTotal_{\typeB}(\hat{n}=\hat{m})))
\end{equation*}
Since the proof is by induction (or rather elimination for
\texttt{TotalNat}), the extracted term will involve recursion:
\begin{verbatim}
[n](Rec nat=>nat=>boole)n([n0][if n0 True ([n1]False)])
   ([n0,(nat=>boole),n1][if n1 False (nat=>boole)])
\end{verbatim}
However, we can prove that $\lambda_{n,m}(n=m)$ realizes the formula
as well:
\begin{verbatim}
;; NatEqTotalSound
(set-goal (real-and-formula-to-mr-formula
	   (pt "[n,m]n=m")
	   (proof-to-formula (theorem-name-to-proof "NatEqTotal"))))
(assume "n^" "n^0" "TMRn0n")
(elim "TMRn0n")
(assume "m^" "m^0" "TMRm0m")
(elim "TMRm0m")
(use "TotalBooleTrueMR")
(assume "m^1" "m^10" "Useless1" "Useless2")
(use "TotalBooleFalseMR")
(assume "m^" "m^0" "Useless1" "IH" "m^1" "m^10" "TMRm10m1")
(elim "TMRm10m1")
(use "TotalBooleFalseMR")
(assume "m^2" "m^20" "TMRm20m2" "Useless2")
(use "IH")
(use "TMRm20m2")
;; Proof finished.
(save "NatEqTotalSound")
\end{verbatim}
Hence we are allowed to change the extracted term of
\texttt{NatEqTotal} into $\lambda_{n,m}(n=m)$.

Generally, \texttt{proof-to-soundness-proof} at \texttt{FinAlgEqTotal}
looks for a therem with name \texttt{FinAlgEqTotalSound} and uses it.
An error is raised if \texttt{FinAlgEqTotalSound} does not exist.

The following table gives the symbols of Minlog's output and the
corresponding notation in the $\lambda$-calculus.

\begin{center}
  \begin{tabular}{|l|c|c|}\hline
    \textbf{Explanation} & \textbf{Symbol} & \textbf{Minlog's output}
    \\ \hline
    $\lambda$-abstraction & $\lambda_x M$ & \texttt{[x]M}
    \\ \hline
    pair & $\langle M, N \rangle$ & \texttt{M pair N}
    \\ \hline
    left element of a pair & $(M \; 0)$ & \texttt{lft M}
    \\ \hline
    right element of a pair &$(M \; 1)$ & \texttt{rht M}
    \\ \hline
    left embedding into a sum type $\rho \typeSum \sigma$
    & $\termSumIntroLeft_{{\rho}{\sigma}} M$ & \texttt{(InL rho sigma)M}
    \\ \hline
    right embedding into a sum type $\rho \typeSum \sigma$
    & $\termSumIntroRight_{{\sigma}{\rho}} M$ & \texttt{(InR sigma rho)M}
    \\ \hline
    recursion operator &$\rec$ & \texttt{Rec}
    \\ \hline
    corecursion operator &$\corec$ & \texttt{CoRec}
    \\ \hline
    arrow for types &$\typeTo$ & \texttt{=>}
    \\ \hline
    product for types  &$\typeProd$ & \texttt{yprod}
    \\ \hline
    sum for types  &$\typeSum$ & \texttt{ysum}
    \\ \hline
    primitive pair & $\langle M, N \rangle$ & \texttt{M@N}
    \\ \hline
    left element of a primitive pair & $(M \; 0)$ & \texttt{left M}
    \\ \hline
    right element of a primitive pair &$(M \; 1)$ & \texttt{right M}
    \\ \hline
    primitive product for types  &$\times$ & \texttt{@@}
    \\ \hline
  \end{tabular}
\end{center}

\subsection{Soundness}
\label{SS:Sound}
One can prove that every theorem in $\TCF + \Ax_{\nci}$ has a
realizer: the extracted term of its proof.  Here $(\Ax_{\nci})$ is an
arbitrary set of non-computational invariant formulas viewed as
axioms.

\begin{theorem*}[Soundness%
  \index{soundness theorem!for realizability}]
  Let $M$ be a derivation of $A$ from assumptions $u_i \colon C_i$
  ($i<n$).  Then we can derive $\extrTer{M} \mr A$ from assumptions
  $x_{u_i} \mr C_i$ (with $x_{u_i} \defeq \nullterm$ in case $C_i$
  is n.c.).
\end{theorem*}

The proof is by induction on $M$, and can be traced back to early work
of Kleene\index{Kleene}, Kreisel\index{Kreisel} and
Troelstra\index{Troelstra}.  References and a detailed exposition
close to the present terminology can be found in
\cite[7.2.8]{SchwichtenbergWainer12}.

We clearly want that \texttt{proof-to-soundness-proof} does not unfold
the auxiliary propositions used in the proof.  Let a theorem
\texttt{Thm} (thought of as coming with its proof $M$) prove a formula
$A$.  Then we should have \texttt{ThmSound} proving $\extrTer{M} \mr
A$ in our proof library.  When \texttt{proof-to-soundness-proof}
arrives at \texttt{Thm}, it inserts \texttt{ThmSound}.  There is no
circularity here, since $t \mr A$ is invariant, i.e., $\nullterm \mr
(t \mr A)$ is the formula $t \mr A$ itself
(cf. \cite[7.2.4]{SchwichtenbergWainer12}).

In the special case of a theorem \texttt{PconstTotal} proving totality
of the constant \texttt{Pconst} the extracted term is \texttt{Pconst}
rather than \texttt{cPconstTotal} (which when animated would unfold
into a term with recursion operators).  Then \texttt{PconstTotalSound}
proves $\mathtt{Pconst} \mr \mathtt{PconstTotal}$, with a simple
standard proof.

An internal proof of soundness can be generated by calling
\begin{alignat*}{2}
  &\texttt{(proof-to-soundness-proof \textsl{proof-or-thm-name})}.%
  \index{proof-to-soundness-proof@\texttt{proof-to-soundness-proof}}
\end{alignat*}
This uses the auxiliary functions
\begin{alignat*}{2}
  &\texttt{(axiom-to-soundness-proof \textsl{aconst})},%
  \index{axiom-to-soundness-proof@\texttt{axiom-to-soundness-proof}}
  \\
  &\texttt{(theorem-to-soundness-proof \textsl{aconst})},%
  \index{theorem-to-soundness-proof@\texttt{theorem-to-soundness-proof}}
  \\
  &\texttt{(global-assumption-to-soundness-proof \textsl{aconst})}.%
  \index{global-assumption-to-soundness-proof@\texttt{global-ass...-to-soundness-proof}}
\end{alignat*}

\section{Computational content of classical proofs}
\label{S:Classical}

\subsection{Refined $A$-translation}
\label{S:RefinedATrans}
In this section the connectives $\to$, $\forall$ denote the
computational versions $\toc$, $\allc$, unless stated otherwise.

We will concentrate on the question of classical versus constructive
proofs.  It is known, by the so-called \inquotes{$A$-translation}%
\index{Atranslation@$A$-translation} of Friedman \cite{Friedman78}%
\index{Friedman} and Dragalin \cite{Dragalin79}\index{Dragalin}, that
any proof of a specification of the form $\forall_x \excl_y B$ with
$B$ quantifier-free and a weak (or \inquotes{classical}) existential
quantifier $\excl_y$, can be transformed into a proof of $\forall_x
\ex_y B$, now with the constructive existential quantifier $\ex_y$.
However, when it comes to extraction of a program from a proof
obtained in this way, one easily ends up with a mess.  Therefore, some
refinements of the standard transformation are necessary.  We shall
study a refined method of extracting reasonable and sometimes
unexpected programs from classical proofs.  It applies to proofs of
formulas of the form $\forall_x \excl_y B$ where $B$ need not be
quantifier-free, but only has to belong to the larger class of
\emph{goal formulas}.  Furthermore we allow unproven lemmata $D$ to
appear in the proof of $\forall_x \excl_y B$, where $D$ is a
\emph{definite} formula.

We now describe in more detail what this section is about.  It is well
known that from a derivation of a classical existential formula
$\excl_y A \defeq \forall_y( A \to \falsum) \to \falsum$ one generally
cannot read off an instance.  A simple example has been given by
Kreisel: let $R$ be a primitive recursive relation such that $\excl_z
R x z$ is undecidable.  Clearly -- even logically --
\begin{equation*}
  \vdash \forall_x \excl_y \forall_z(R x z \to R x y)
\end{equation*}
but there is no computable $f$ satisfying
\begin{equation*}
  \forall_x \forall_z(R x z \to R(x,f(x))),
\end{equation*}
for then $\excl_z R x z$ would be decidable: it would be true if and
only if $R(x,f(x))$ holds.

However, it is well known that in case $\excl_y G$ with $G$
quantifier-free one \emph{can} read off an instance.  Here is a simple
idea of how to prove this: replace $\falsum$\index{$\falsum$} anywhere
in the proof by $\ex_y G$.  Then the end formula $\forall_y( G \to
\falsum) \to \falsum$ is turned into $\forall_y( G \to \ex_y G) \to
\ex_y G$, and since the premise is trivially provable, we have the
claim.

Unfortunately, this simple argument is not quite correct.  First, $G$
may contain $\falsum$, and hence is changed under the substitution of
$\ex_y G$ for $\falsum$.  Second, we may have used axioms or lemmata
involving $\falsum$ (e.g., $\falsum \to P$), which need not be
derivable after the substitution.  But in spite of this, the simple
idea can be turned into something useful.

Assume that the lemmata $\vec{D}$ and the goal formula $G$ are such
that we can derive
\begin{align}
  \label{E:lem1}
  &\vec{D} \to \subst{D_i}{\falsum}{\ex_y G},
  \\
  \label{E:lem2}
  &\subst{G}{\falsum}{\ex_y G} \to \ex_y G.
\end{align}
Assume also that the substitution $\subst {} {\falsum} {\ex_y G}$
turns any axiom into an instance of the same axiom-schema, or else
into a derivable formula.  Then from our given derivation (in minimal
logic) of $\vec{D} \to \forall_y( G \to \falsum) \to \falsum$ we
obtain
\begin{equation*}
  \subst{\vec{D}}{\falsum}{\ex_y G} \to
  \forall_y( \subst{G}{\falsum}{\ex_y G} \to \ex_y G) \to
  \ex_y G.
\end{equation*}
Now \eqref{E:lem1} allows the substitution in $\vec{D}$ to be dropped,
and by \eqref{E:lem2} the second premise is derivable.  Hence we
obtain as desired
\begin{equation*}
  \vec{D} \to \ex_y G.
\end{equation*}
We shall identify classes of formulas -- to be called \emph{definite}
and \emph{goal} formulas -- such that slight generalizations of
\eqref{E:lem1} and \eqref{E:lem2} hold.

This section is based on \cite{BergerBuchholzSchwichtenberg02} and
particularly \cite[7.3]{SchwichtenbergWainer12}, where the theory is
developed in more detail and further references are given.  Recall
that we restrict to formulas in the language $\{\falsum, \to, \forall
\}$.

A formula is \emph{relevant}\index{formula!relevant} if it ends with
(logical) falsity.  \emph{Definite}\index{formula!definite} and
\emph{goal}\index{formula!goal} formulas are defined by a simultaneous
recursion.
\begin{alignat*}{2}
  &\texttt{(atr-relevant?\ \textsl{formula})},%
  \index{atr-relevant?@\texttt{atr-relevant?}}
  \\
  &\texttt{(atr-definite?\ \textsl{formula})},%
  \index{atr-definite?@\texttt{atr-definite?}}
  \\
  &\texttt{(atr-goal?\ \textsl{formula})}.%
  \index{atr-goal?@\texttt{atr-goal?}}
\end{alignat*}
We need to construct proofs from $\falsityF \to \falsum$ of
\begin{alignat*}{2}
  &D^{\falsityF} \to D,
  \\
  &G \to (G^{\falsityF} \to \falsum) \to \falsum,
  \\
  &((R^{\falsityF} \to \falsityF) \to \falsum) \to R
  &\quad&\text{for $R$ relevant and definite},
  \\
  &I \to I^{\falsityF}
  &\quad&\text{for $I$ irrelevant and goal}.
\end{alignat*}
This is done by
\begin{alignat*}{2}
  &\texttt{(atr-arb-definite-proof \textsl{formula})},%
  \index{atr-arb-definite-proof@\texttt{atr-arb-definite-proof}}
  \\
  &\texttt{(atr-arb-goal-proof \textsl{formula})},%
  \index{atr-arb-goal-proof@\texttt{atr-arb-goal-proof}}
  \\
  &\texttt{(atr-rel-definite-proof \textsl{formula})},%
  \index{atr-rel-definite-proof@\texttt{atr-rel-definite-proof}}
  \\
  &\texttt{(atr-irrel-goal-proof \textsl{formula})}.%
  \index{atr-irrel-goal-proof@\texttt{atr-irrel-goal-proof}}
\end{alignat*}
The next task is to generalize $G \to (G^{\falsityF} \to \falsum) \to
\falsum$ and construct a proof of $(G_1^{\falsityF} \to ... \to
G_n^{\falsityF} \to \falsum) \to G_1 \to ... \to G_n \to \falsum$, via
\begin{equation*}
  \texttt{(atr-goals-F-to-bot-proof .\ \textsl{goals})}.%
  \index{atr-goals-F-to-bot-proof@\texttt{atr-goals-F-to-bot-proof}}
\end{equation*}
Given a proof of $\vec{A} \to \vec{D} \to \forall_{\vec{y}}(\vec{G}
\to \falsum) \to \falsum$ with $\vec{A}$ arbitrary, $\vec{D}$ definite
and $\vec{G}$ goal formulas, we transform it into a proof of
$(\falsityF \to \falsum) \to \vec{A} \to \vec{D}^{\falsityF} \to
\forall_{\vec{y}}( \vec{G}^{\falsityF} \to \falsum) \to \falsum$.
This is done via
\begin{equation*}
  \texttt{(atr-min-excl-proof-to-bot-reduced-proof
    \textsl{min-excl-proof})}.%
  \index{atr-min-excl-proof-to-bot-reduced-proof@\texttt{atr-min-excl-proof-to-bot-reduced-proof}}
\end{equation*}
Substituting the formula $\ex_{\vec{y}} \vec{G}^{\falsityF}$ for
$\falsum$ in the proof given above of $(F \to \falsum) \to \vec{A} \to
\vec{D}^{\falsityF} \to \forall_{\vec{y}}(\vec{G}^{\falsityF} \to
\falsum) \to \falsum$, both the ex-falso-quodlibet premise and the
\inquotes{wrong formula} $\forall_{\vec{y}}(\vec{G}^{\falsityF} \to
\falsum)$ become provable and we obtain a proof of $\vec{A}' \to
\vec{D}^{\falsityF} \to \ex_{\vec{y}} \vec{G}^{\falsityF}$, where
$\vec{A}'$ is defined to be $\subst {\vec{A}} {\falsum} {\ex_{\vec{y}}
  \vec{G}^{\falsityF}}$.  The corresponding function is
\begin{equation*}
  \texttt{(atr-min-excl-proof-to-ex-proof \textsl{min-excl-proof})}.%
  \index{atr-min-excl-proof-to-ex-proof@\texttt{atr-min-excl-proof-to-ex-proof}}
\end{equation*}
By
\begin{align*}
  &\texttt{(atr-min-excl-proof-to-structured-extracted-term}
  \\
  &\quad\texttt{\textsl{min-excl-proof} .\
    \textsl{realizers-for-nondefinite-formulas})}%
  \index{atr-min-excl-proof-to-structured-extracted-term@\texttt{atr{\dots}-to-structured-extracted-term}}
\end{align*}
we can then extract a term $r$ such that $\vec{A} \to \vec{D} \to
\subst {\vec{G}^{\falsityF}} {y} {r}$ (if $\vec{y} = y$).

One can test with \texttt{min-excl-formula?}%
\index{min-excl-formula?@\texttt{min-excl-formula?}} whether a given
formula indeed is a classical (i.e., weak) existence formula.
Moreover, \texttt{atr-expand-theorems}%
\index{atr-expand-theorems@\texttt{atr-expand-theorems}} expands all
non-definite theorems.  This only makes sense before substituting for
$\falsum$.

See section \ref{S:ExtrTerms} for an interpretation of the symbols of
the extracted terms in Minlog's output.

\subsection{Gödel's Dialectica interpretation}
\label{SS:Dialectica}
\index{Dialectica interpretation} In his original functional
interpretation \cite{Goedel58}, Gödel\index{Goedel@Gödel}
assigned to every formula $A$ a new one $\ex_{\vec{x}}
\forall_{\vec{y}} A_D(\vec{x}, \vec{y}\,)$ with $A_D(\vec{x},
\vec{y}\,)$ quantifier-free.  Here $\vec{x}$, $\vec{y}$ are lists of
variables of finite types; the use of higher types is necessary even
when the original formula $A$ is first-order.  He did this in such a
way that whenever a proof of $A$ say in Peano arithmetic was given,
one could produce closed terms $\vec{r}$ such that the quantifier-free
formula $A_D(\vec{r}, \vec{y}\,)$ is provable in his quantifier-free
system $\T$.

In \cite{Goedel58} Gödel referred to a Hilbert-style proof calculus.
However, since the realizers will be formed in a $\lambda$-calculus
formulation of system $\T$, Gödel's interpretation becomes more
perspicuous when it is done for a natural deduction calculus.  The
present implementation is based on such a setup.  Then the need for
contractions comes up in the (only) logical rule with two premises:
modus ponens (or implication elimination $\impE$).  This makes it
possible to give a relatively simple proof of the Soundness Theorem.

We assign to every formula $A$ objects $\tau^{+}(A)$, $\tau^{-}(A)$ (a
type or the \inquotes{nulltype} symbol $\nulltype$).  $\tau^{+}(A)$ is
intended to be the type of a (Dialectica-) realizer to be extracted
from a proof of $A$, and $\tau^{-}(A)$ the type of a challenge for the
claim that this term realizes $A$.
\begin{alignat*}{4}
  &\tau^{+}(P \vec{s}\,) &&\defeq \nulltype,
  &&\tau^{-}(P \vec{s}\,) &&\defeq \nulltype,
  \\
  &\tau^{+}(\forall_{x^{\rho}} A) &&\defeq \rho \typeTo \tau^{+}(A),
  &&\tau^{-}(\forall_{x^{\rho}} A) &&\defeq \rho \typeProd \tau^{-}(A),
  \\
  &\tau^{+}(\ex_{x^{\rho}} A) &&\defeq \rho \typeProd \tau^{+}(A),
  &&\tau^{-}(\ex_{x^{\rho}} A) &&\defeq \tau^{-}(A),
  \\
  &\tau^{+}(A \land B) &&\defeq \tau^{+}(A) \typeProd \tau^{+}(B),
  &\quad&\tau^{-}(A \land B) &&\defeq \tau^{-}(A) \typeProd \tau^{-}(B),
\end{alignat*}
and for implication
\begin{align*}
  \tau^{+}(A \to B) &\defeq
  (\tau^{+}(A) \typeTo \tau^{+}(B) ) \typeProd
  (\tau^{+}(A) \typeTo \tau^{-}(B) \typeTo \tau^{-}(A) ),
  \\
  \tau^{-}(A \to B) &\defeq \tau^{+}(A) \typeProd \tau^{-}(B).
\end{align*}
Recall that $(\rho \typeTo \nulltype) \defeq \nulltype$,
$(\nulltype \typeTo \sigma) \defeq \sigma$,
$(\nulltype \typeTo \nulltype) \defeq \nulltype$, and
$(\rho \typeProd \nulltype) \defeq \rho$,
$(\nulltype \typeProd \sigma) \defeq \sigma$,
$(\nulltype \typeProd \nulltype) \defeq \nulltype$.

In case $\tau^{+}(A)$ ($\tau^{-}(A)$) is $\ne \nulltype$ we say that
$A$ has \emph{positive (negative) computational content}%
\index{formula!positive content}%
\index{formula!negative content}.  For formulas without positive or
without negative content one can give an easy characterization,
involving the well-known notion of positive or negative occurrences of
quantifiers in a formula.
\begin{align*}
    &\tau^{+}(A) = \nulltype \leftrightarrow
    \hbox{$A$ has no positive $\ex$ and no negative $\forall$},
    \\
    &\tau^{-}(A) = \nulltype \leftrightarrow
    \hbox{$A$ has no positive $\forall$ and no negative $\ex$},
    \\
    &\tau^{+}(A) = \tau^{-}(A) = \nulltype \leftrightarrow
    \hbox{$A$ is quantifier-free.}
  \end{align*}
Both the positive and the negative type of a formula can be computed
by
\begin{align*}
  &\texttt{(formula-to-etdp-type \textsl{formula})},%
  \index{formula-to-etdp-type@\texttt{formula-to-etdp-type}}
  \\
  &\texttt{(formula-to-etdn-type \textsl{formula})}.%
  \index{formula-to-etdn-type@\texttt{formula-to-etdn-type}}
\end{align*}

For every formula $A$ and terms $r$ of type $\tau^{+}(A)$ and $s$ of
type $\tau^{-}(A)$ we define a new quantifier-free formula
$\goe{A}{r}{s}$ by induction on $A$.
\begin{equation*}
  \begin{split}
    \goe{P \vec{s}\,}{r}{s} &\defequiv P \vec{s},
    \\
    \goe{\forall_x A(x)}{r}{s} &\defequiv \goe{A(s0)}{r(s0)}{s1},
    \\
    \goe{\ex_x A(x)}{r}{s} &\defequiv \goe{A(r0)}{r1}{s},
  \end{split}
  \qquad
  \begin{split}
    \goe{A \land B}{r}{s} &\defequiv \goe{A}{r0}{s0} \land \goe{B}{r1}{s1},
    \\
    \goe{A \to B}{r}{s} &\defequiv
    \goe{A}{s0}{r1(s0)(s1)} \to \goe{B}{r0(s0)}{s1}.
  \end{split}
\end{equation*}
The formula $\ex_x \forall_y \goe{A}{x}{y}$ is called the \emph{Gödel
  translation}\index{Goedel@Gödel!translation} of $A$ and is often
denoted by $A^{D}$.  Its quantifier-free kernel $\goe{A}{x}{y}$ is
called \emph{Gödel kernel} of $A$; it is denoted by $A_D$.

For readability we sometimes write terms of a pair type in pair form:
\begin{equation*}
  \begin{split}
    \goe{\forall_z A}{f}{z,y} &\defequiv \goe{A}{fz}{y},
    \\
    \goe{\ex_z A}{z,x}{y} &\defequiv \goe{A}{x}{y},
  \end{split}
  \qquad
  \begin{split}
    \goe{A \land B}{x,z}{y,u} &\defequiv \goe{A}{x}{y} \land \goe{B}{z}{u},
    \\
    \goe{A \to B}{f,g}{x,u} &\defequiv \goe{A}{x}{gxu} \to \goe{B}{fx}{u}.
  \end{split}
\end{equation*}
\texttt{formula-to-d-formula}%
\index{formula-to-d-formula@\texttt{formula-to-d-formula}} calculates
the Gödel (or Dialectica) translation of a formula.

To answer the question when the Gödel translation of a formula $A$ is
equivalent to the formula itself, we need the (constructively
doubtful) \emph{Markov principle}%
\index{Markov principle} $(\MP)$, for higher type variables and
quantifier-free formulas $A_0 ,B_0$.
\begin{equation*}
  (\forall_{x^{\rho}} A_0 \to B_0) \to \ex_{x^{\rho}}(A_0 \to B_0)
  \quad \hbox{($x^{\rho} \notin \FV(B_0)$).}
\end{equation*}
We also need the (less problematic) \emph{axiom of choice}%
\index{axiom!of choice} $(\AC)$
\begin{equation*}
  \forall_{x^{\rho}} \ex_{y^{\sigma}} A(x,y) \to
  \ex_{f^{\rho \typeTo \sigma}} \forall_{x^{\rho}} A(x,f(x)).
\end{equation*}
and the \emph{independence of premise}%
\index{axiom!independence of premise} axiom $(\IP)$
\begin{equation*}
  (A \to \ex_{x^{\rho}} B) \to \ex_{x^{\rho}}( A \to B)
  \quad \hbox{($x^{\rho} \notin \FV(A)$, $\tau^{+}(A) = \nulltype$)}.
\end{equation*}
Notice that $(\AC)$ expresses that we can only have continuous
dependencies.

\begin{theorem*}[Characterization]
  \begin{equation*}
    \AC + \IP + \MP \vdash (
    A \leftrightarrow \ex_x \forall_y\, \goe{A}{x}{y}).
  \end{equation*}
\end{theorem*}

Let \emph{Heyting arithmetic} $\HA^{\omega}$ in all finite types%
\index{Heyting arithmetic}\index{$\HA^{\omega}$} be the fragment of
$\TCF$ where (i) the only base types are $\typeN$ and $\typeB$, and
(ii) the only inductively defined predicates are totality, Leibniz
equality $\Eq$, the (proper) existential quantifier and conjunction.
We can prove soundness of the Dialectica interpretation for
$\HA^{\omega} + \AC + \IP + \MP$, for our natural deduction
formulation of the underlying logic.

\begin{theorem*}[Soundness%
  \index{soundness theorem!for Dialectica}]
  Let $M$ be a derivation
  \begin{equation*}
    \hbox{$\HA^{\omega} + \AC + \IP + \MP \vdash A$}
  \end{equation*}
  from assumptions $u_i \colon C_i$ ($i=1, \dots,n$).  Let $x_i$ of
  type $\tau^{+}(C_i)$ be variables for realizers of the assumptions,
  and $y$ be a variable of type $\tau^{-}(A)$ for a challenge of the
  goal.  Then we can find terms $\extrTerP{M} \eqdef t$ of type
  $\tau^{+}(A)$ with $y \notin \FV(t)$ and $\extrTerN{M}{i} \eqdef
  r_i$ of type $\tau^{-}(C_i)$, and a derivation in $\HA^{\omega}$ of
  $\goe{A}{t}{y}$ from assumptions $\bar{u}_i \colon
  \goe{C_i}{x_i}{r_i}$.
\end{theorem*}

\texttt{proof-to-extracted-d-terms}%
\index{proof-to-extracted-d-terms@\texttt{proof-to-extracted-d-terms}}
returns the extracted realiser and a list of extracted challenges
labelled with their associated assumption variables.

\section{Reading formulas in external form}
\label{S:Reading}
A formula can be produced from an external representation, for example
a string, using the \verb#pt# function.  It has one argument, a string
denoting a formula, that is converted to the internal representation
of the formula.  For the following syntactical entities parsing
functions are provided:
\begin{alignat*}{2}
  &\texttt{(py \textsl{string})}\index{py@\texttt{py}}
  &\quad& \hbox{for parsing types},
  \\
  &\texttt{(pv \textsl{string})}\index{pv@\texttt{pv}}
  &\quad& \hbox{for parsing variables},
  \\
  &\texttt{(pt \textsl{string})}\index{pt@\texttt{pt}}
  &\quad& \hbox{for parsing terms},
  \\
  &\texttt{(pf \textsl{string})}\index{pf@\texttt{pf}}
  &\quad& \hbox{for parsing formulas}.
\end{alignat*}

The conversion occurs in two steps: lexical analysis and parsing.

\subsection{Lexical analysis\index{lexical analysis}}
In this stage the string is brocken into short sequences, called
\emph{tokens}\index{token}.

A token can be one of the following:
\begin{enumeratei}
\item An alphabetic symbol: A sequence of letters \verb#a#--\verb#z#
and \verb#A#--\verb#Z#.  Upper and lower case letters are considered
different.
\item A number: A sequence of digits \verb#0#--\verb#9#
\item A punctuation mark: One of the characters: \verb#(# \verb#)#
\verb#[# \verb#]# \verb#.# \verb#,# \verb#;#
\item A special symbol: A sequence of characters, that are neither letters,
  digits, punctuation marks nor white space.
\end{enumeratei}

For example: \verb#abc#, \verb#ABC# and \verb#A# are alphabetic
symbols, \verb#123#, \verb#0123# and \verb#7# are numbers, \verb#(# is
a punctuation mark, and \verb#<=#, \verb#+#, and \verb+##:-^+ are
special symbols.

Tokens are always character sequences of maximal length belonging to
one of the above categories.  Therefore \verb#fx# is a single
alphabetic symbol not two and likewise \verb#<+# is a single special
symbol.  The sequence \verb#alpha<=(-x+z)#, however, consists of the 8
tokens \verb#alpha#, \verb#<=#, \verb#(#, \verb#-#, \verb#x#,
\verb#+#, \verb#z#, and \verb#)#.  Note that the special symbols
\verb#<=# and \verb#-# are separated by a punctuation mark, and the
alphabetic symbols \verb#x# and \verb#z# are separated by the special
symbol \verb#+#.

If two alphabetic symbols, two special symbols, or two numbers follow
each other they need to be separated by white space (spaces, newlines,
tabs, formfeeds, etc.).  Except for a few situations mentioned below,
whitespace has no significance other than separating tokens.  It can
be inserted and removed between any two tokens without affecting the
significance of the string.

Every token has a \emph{token type}, and a value.  The token type is
one of the following: number, var-index, var-name, const, pvar-name,
predconst, type-symbol, pscheme-symbol, postfix-op, prefix-op,
binding-op, add-op, mul-op, rel-op, and-op, or-op, imp-op, pair-op,
if-op, postfix-jct, prefix-jct, and-jct, or-jct, tensor-jct, imp-jct,
quantor, dot, hat, underscore, comma, semicolon, arrow, lpar, rpar,
lbracket, rbracket.

The possible values for a token depend on the token type and are
explained below.

New tokens can be added using the function
\begin{equation*}
  \texttt{(add-token \textsl{string} \textsl{token-type} \textsl{value})}.
\end{equation*}
The inverse is the function
\begin{equation*}
  \texttt{(remove-token \textsl{string})}.
\end{equation*}
A list of all currently defined tokens sorted by token types can be
obtained by the function
\begin{equation*}
  \texttt{(display-tokens)}.
\end{equation*}


\subsection{Parsing}
\label{SS:Parsing}
The second stage, \emph{parsing}\index{parsing}, extracts structure
form the sequence of tokens.

\emph{Types}.  Type-symbols are types; the value of a type-symbol must
be a type.  If $\rho$ and $\sigma$ are types, then
$\rho$\verb#=>#$\sigma$ is a type (function type) and
$\rho$\verb#@@#$\sigma$ is a type (primitive pair type).  Parentheses
can be used to indicate proper nesting.  For example \verb#boole# is a
predefined type-symbol and hence, \verb#(boole@@boole)=>boole# is
again a type.  The parentheses in this case are not strictly
necessary, since \verb#@@# binds stronger than \verb#=>#.  Both
operators associate to the right.

\emph{Variables}.  Var-names are variables; the value of a var-name
token must be a pair consisting of the type and the name of the
variable (the same name string again\footnote{This is not nice and may
  be later, we find a way to give the parser access to the string that
  is already implicit in the token}).  For example to add a new
boolean variable called \inquotes{flag}, you have to invoke the
function \texttt{(add-token "flag" 'var-name (cons 'boole "flag"))}.
This will enable the parser to recognize \inquotes{\texttt{flag3}},
\inquotes{\texttt{flag\^}}, or ``\verb/flag^14/'' as well.

Further, types, as defined above, can be used to construct variables.

A variable given by a name or a type can be further modified.  If it
is followed by a \verb#^#, a general (or partial)xs variable is
constructed.  Instead of the \verb#^# a \verb#_# can be used to
specify a total variable.

Total variables are the default and therefore, the \verb#_# can be
omitted.

As another modifier, a number can immediately follow, with no
whitespace in between, the \verb#^# or the \verb#_#, specifying a
specific variable index.

In the case of indexed total variables given by a variable name or a
type symbol, again the \verb#_# can be omitted.  The number must then
follow, with no whitespace in between, directly after the variable
name or the type.

Note: This is the only place where whitespace is of any significance
in the input.  If the \verb#^#, \verb#_#, type name or variable name
is separated from the following number by whitespace, this number is
no longer considered to be an index for that variable but a numeric
term in its own right.

For example, assuming that \verb#p# is declared as a variable of type
\verb#boole#, we have:
\begin{enumeratei}
\item \verb#p# a total variable of type boole with name p and no index.
\item \verb#p_# a total variable of type boole with name p and no index.
\item \verb#p^# a partial variable of type boole with name p and no index.
\item \verb#p2# a total variable of type boole with name p and index 2.
\item \verb#p_2# a total variable of type boole with name p and index 2.
\item \verb#p^2# a partial variable of type boole with name p and index 2.
\item \verb#boole# a total anonymous variable of type boole with no index.
\item \verb#boole_# a total anonymous variable of type boole with no index.
\item \verb#boole^# a partial anonymous variable of type boole with no index.
\item \verb#boole_2# a total anonymous variable of type boole with index 2.
\item \verb#boole2# a total anonymous variable of type boole with index 2.
\item \verb#boole^2# a partial anonymous variable of type boole with index 2.
\item \verb#(boole)_2# a total anonymous variable of type boole with index 2.
\item \verb#nat=>boole_2# a total anonymous variable of type
  function of nat to boole with index 2.
\item \verb#nat=>boole^2# a partial anonymous variable of type
  function of nat to boole with index 2.
\item \verb#(nat=>alpha2)# a total anonymous variable of type
  function of nat to alpha2 with no index.
\item \verb#(nat=>alpha2)_2# a total anonymous variable of type
  function of nat to alpha2 with index 2.
\item \verb#(nat=>alpha2)^2# a partial anonymous variable of type
  function of nat to alpha2 with index 2.
\end{enumeratei}
Compare these with the following applicative terms.
\begin{enumeratei}
\item \verb#nat=>boole 2# a total anonymous variable of type
  function of nat to boole with no index applied to the numeric term 2.
\item \verb#nat=>boole_ 2# a total anonymous variable of type
  function of nat to boole with no index applied to the numeric term 2.
\item \verb#nat=>boole^ 2# a partial anonymous variable of type
  function of nat to boole with no index applied to the numeric term 2.
\item \verb#nat=>boole_2 2# a total anonymous variable of type
  function of nat to boole with index 2 applied to the numeric term 2.
\item \verb#nat=>boole^2 2# a partial anonymous variable of type
  function of nat to boole with index 2 applied to the numeric term 2.
\item \verb#(nat=>alpha)2# a total anonymous variable of type
  function of nat to alpha with no index applied to the numeric term 2.
\item \verb#(nat=>alpha)_ 2# a total anonymous variable of type
  function of nat to alpha with no index applied to the numeric term 2.
\item \verb#(nat=>alpha)^ 2# a partial anonymous variable of type
  function of nat to alpha with no index applied to the numeric term 2.
\item \verb#(nat=>alpha)_2 2# a total anonymous variable of type
  function of nat to alpha with index 2 applied to the numeric term 2.
\item \verb#(nat=>alpha)^2 2# a partial anonymous variable of type
  function of nat to alpha with index 2 applied to the numeric term 2.
\item \verb#(nat=>alpha2)_2 2# a total anonymous variable of type
  function of nat to alpha2 with index 2 applied to the numeric term 2.
\item \verb#(nat=>alpha2)^2 2# a partial anonymous variable of type
  function of nat to alpha2 with index 2 applied to the numeric term 2.
\end{enumeratei}

\emph{Terms} are built from atomic terms using application and
operators.

An atomic term is one of the following: a constant, a variable, a
number, a conditional, or any other term enclosed in parentheses.

Constants have \verb#const# as token type, and the respective term in
internal form as value.  There are also composed constants, so-called
\emph{constant schemata}\index{constant scheme}.  A constant schema
has the form of the name of the constant schema (token type
\texttt{constscheme}) followed by a list of types, the whole thing
enclosed in parentheses.  There are a few built in constant schemata;
for instance, \verb#(Rec <typelist>)#\index{Rec@\texttt{Rec}} is the
recursion over the types given in the type list.

For a number, the user defined function \verb#make-numeric-term# is
called with the number as argument.  The return value of
\verb#make-numeric-term# should be the internal term representation of
the number.

To form a conditional term, the if operator \verb#if# followed by a
list of atomic terms is enclosed in square brackets.  Depending on the
constructor of the first term, the selector, a conditional term can be
reduced to one of the remaining terms.

From these atomic terms, compound terms are built not only by
application but also using a variety of operators, that differ in
binding strength and associativity.

Postfix operators (token type \verb#postfix-op#) bind strongest, next
in binding strength are prefix operators (token type
\verb#prefix-op#), next come binding operators (token type
\verb#binding-op#).

A binding operator is followed by a list of variables and finally a
term.  There are two more variations of binding operators, that bind
much weaker and are discussed later.

Next, after the binding operators, is plain application.
Juxtaposition of two terms means applying the first term to the
second.  Sequences of applications associate to the left.  According
to the \emph{vector notation} convention the meaning of application
depends on the type of the first term.  Two forms of applications are
defined by default: if the type of the first term is of
\verb#arrow-form?#  then \verb#make-term-in-app-form# is used; for the
type of a free algebra we use the corresponding form of recursion.
However, there is one exception: if the first term is of type
\verb#boole# application is read as a short-hand for the
\inquotes{if\dots then \dots else} construct (which is a special form)
rather than boolean recursion.  The user may use the function
\verb#add-new-application#
\index{add-new-application@\texttt{add-new-application}} to add new
forms of applications.  This function takes two arguments, a predicate
for the type of the first argument, and a function taking the two
terms and returning another term intended to be the result of this
form of application.  Predicates are tested in the inverse order of
their definition, so more general forms of applications should be
added first.

By default these new forms of application are \emph{not} used for
output; but the user might declare that certain terms should be output
as formal application. \emph{When doing so it is the user's
  responsibility to make sure that the syntax used for the output can
  still be parsed correctly by the parser!}  To do so the function
\texttt{(add-new-application-syntax pred toarg toop)} can be used,
where the first argument has to be a predicate (i.e., a function
mapping terms to \verb+#t+ and \verb+#f+) telling whether this special
form of application can be used. If so, the arguments \texttt{toarg}
and \texttt{toop} have to be functions mapping the term to operator
and argument of this ``application'' respectively.

After that, we have binary operators written in infix notation.  In
order of decreasing binding strength these are:
\begin{enumeratei}
\item multiplicative operators, leftassociative, token type \verb#mul-op#;
\item additive operators, leftassociative, token type \verb#add-op#;
\item relational operators, not associative, token type \verb#rel-op#;
\item boolean and operators, leftassociative, token type \verb#and-op#;
\item boolean or operators, leftassociative, token type \verb#or-op#;
\item boolean implication operators, rightassociative, token type
  \verb#imp-op#;
\item pairing operators, rightassociative, token type \verb#pair-op#.
\end{enumeratei}

On the top level, we have two more forms of binding operators, one
using the dot ``\verb#.#'', the other using square brackets
``\verb#[ ]#''.  Recall that a binding operator is followed by a list
of variables and a term.  This notation can be augmented by a period
``\verb#.#''  following after the variable list and before the term.
In this case the scope of the binding extends as far to the right as
possible.  Bindings with the lambda operator can also be specified by
including the list of variables in square brackets.  In this case,
again, the scope of the binding extends as far as possible.

Predefined operators are \texttt{=} as described above, the binding
operator \texttt{lambda}, and the primitive pairing operator \verb#@#
with two prefix operators \texttt{left} and \texttt{right} for
projection.

The value of an operator token is a function that will obtain the
internal representation of the component terms as arguments and
returns the internal representation of the whole term.

If a term is formed by application, the function
\verb#make-gen-application# is called with two subterms and returns
the compound term.  The default here (for terms with an arrow type) is
to make a term in application form.  However other rules of
composition might be introduced easily.

\emph{Formulas} are built from atomic formulas using junctors and
quantors.

The simplest atomic formulas are made from terms using the implicit
predicate \inquotes{atom}.  The semantics of this predicate is well
defined only for terms of type boole.  Further, a predicate constant
(token type \verb#predconst#) or a predicate variable (token type
\verb#pvar#) followed by a list of atomic terms is an atomic formula.
Lastly, any formula enclosed in parentheses is considered an atomic
formula.

The composition of formulas using junctors and quantors is very
similar to the composition of terms using operators and binding.  So,
first postfix junctors, token type \verb#postfix-jct#, are applied,
next prefix junctors, token type \verb#prefix-jct#, and quantors,
token type \verb#quantor#, in the usual form: quantor, list of
variables, formula.  Again, we have a notation using a period after
the list of variables, making the scope of the quantor as large as
possible.  Predefined quantors are \texttt{all}, \texttt{ex},
\texttt{allnc}, \texttt{excl}, \texttt{exca}, and \texttt{excu}.

The remaining junctors are binary junctors written in infix form.  In
order of decreasing binding strength we have:
\begin{enumeratei}
\item conjunction junctors, leftassociative, token type \verb#and-jct#;
\item disjunction junctors, leftassociative, token type \verb#or-jct#;
\item tensor junctors, rightassociative, token type \verb#tensor-jct#;
\item implication junctors, rightassociative, token type \verb#imp-jct#.
\end{enumeratei}
Predefined junctors are \verb#&# (and), \verb#!# (tensor), \verb#->#
(implication) and \verb#--># (non-computational implication).

The value of junctors and quantors is a function that will be called
with the appropriate subformulas, respectively variable lists, to
produce the compound formula in internal form.

\section{Natural numbers}
\label{S:nat}
For the algebra $\typeN$ of natural numbers there is a library file
\texttt{nat.scm} collecting definitions of some standard functions
and relations referring to the type $\typeN$.  These are
\begin{verbatim}
(add-program-constant "NatPlus" (py "nat=>nat=>nat"))
(add-program-constant "NatTimes" (py "nat=>nat=>nat"))
(add-program-constant "NatLt" (py "nat=>nat=>boole"))
(add-program-constant "NatLe" (py "nat=>nat=>boole"))
(add-program-constant "Pred" (py "nat=>nat"))
(add-program-constant "NatMinus" (py "nat=>nat=>nat"))
(add-program-constant "NatMax" (py "nat=>nat=>nat"))
(add-program-constant "NatMin" (py "nat=>nat=>nat"))
(add-program-constant "AllBNat" (py "nat=>(nat=>boole)=>boole"))
(add-program-constant "ExBNat" (py "nat=>(nat=>boole)=>boole"))
(add-program-constant "NatLeast" (py "nat=>(nat=>boole)=>nat"))
(add-program-constant "NatLeastUp" (py "nat=>nat=>(nat=>boole)=>nat"))
\end{verbatim}
with there standard (mostly infix) notations.  To see their defining
equations (i.e., computation rules) and proved equations used as
(left to right) rewrite rules one can type for instance
\begin{verbatim}
(display-pconst "NatPlus")
\end{verbatim}
and obtains
\begin{verbatim}
NatPlus
  comprules
	nat+0	nat
	nat1+Succ nat2	Succ(nat1+nat2)
  rewrules
	0+nat	nat
	Succ nat1+nat2	Succ(nat1+nat2)
	nat1+(nat2+nat3)	nat1+nat2+nat3
\end{verbatim}
The file also contains often used theorems and their proofs.  To
review them the \texttt{search-about} command is recommended.
To find out about what has been proved about a particular constant
use
\begin{verbatim}
(search-about "NatMax")
\end{verbatim}
The result contains
\begin{verbatim}
NatMaxLUB
all nat1,nat2,nat3(nat1<=nat3 -> nat2<=nat3 -> nat1 max nat2<=nat3)
NatMaxUB2
all nat1,nat2 nat2<=nat1 max nat2
NatMaxUB1
all nat1,nat2 nat1<=nat1 max nat2
NatMaxComm
all nat1,nat2 nat1 max nat2=nat2 max nat1
\end{verbatim}
To see which theorems relating to case distinctions are available type
\begin{verbatim}
(search-about "Cases")
\end{verbatim}
One obtains the following theorems with their names:
\begin{verbatim}
NatLeLtCases
all nat1,nat2((nat1<=nat2 -> Pvar) ->
              (nat2<nat1 -> Pvar) -> Pvar)
NatLeCases
all nat1,nat2(nat1<=nat2 -> 
              (nat1<nat2 -> Pvar) ->
              (nat1=nat2 -> Pvar) -> Pvar)
NatLtSuccCases
all nat1,nat2(nat1<Succ nat2 ->
              (nat1<nat2 -> Pvar) ->
              (nat1=nat2 -> Pvar) -> Pvar)
\end{verbatim}
As another example, to check the available transitivity theorems type
\begin{verbatim}
(search-about "Trans")
\end{verbatim}
The result is
\begin{verbatim}
NatLtLtSuccTrans
all nat1,nat2,nat3(nat1<nat2 -> nat2<Succ nat3 -> nat1<nat3)
NatLeLtTrans
all nat1,nat2,nat3(nat1<=nat2 -> nat2<nat3 -> nat1<nat3)
NatLtLeTrans
all nat1,nat2,nat3(nat1<nat2 -> nat2<=nat3 -> nat1<nat3)
NatLeTrans
all nat1,nat2,nat3(nat1<=nat2 -> nat2<=nat3 -> nat1<=nat3)
NatLtTrans
all nat1,nat2,nat3(nat1<nat2 -> nat2<nat3 -> nat1<nat3)
NatEqTrans
all nat1,nat2,nat3(nat1=nat2 -> nat2=nat3 -> nat1=nat3)
\end{verbatim}
Yet another example is
\begin{verbatim}
(search-about "NatLeast")
\end{verbatim}
The bounded least number operator \texttt{NatLeast}, written $\mu_n
g$, is defined recursively as follows.  Here $g$ is a variable of type
$\D{N} \typeTo \D{B}$.
\begin{align*}
  &\mu_0 g \defeq 0,
  \\
  &\mu_{\suc n}g \defeq
  \begin{cases}
    0 &\text{if $g 0$}
    \\
    \suc \mu_n(g \circ \suc) &\text{otherwise}.
  \end{cases}
\end{align*}
Then we obtain
\begin{alignat*}{2}
  &\texttt{NatLeastBound}\colon &\quad&
  \mu_n g \le n,
  \\
  &\texttt{NatLeastLeIntro}\colon &\quad&
  g m \to \mu_n g \le m,
  \\
  &\texttt{NatLeastLtElim}\colon &\quad&
  \mu_n g<n \to g(\mu_n g).
\end{alignat*}
From $\mu_n g$ we define
\begin{equation*}
  \mu_{n_0}^n g \defeq
  \begin{cases}
    (\mu_{n-n_0}\lambda_m g(m+n_0))+n_0 &\text{if $n_0 \le n$}
    \\
    0 &\text{otherwise}.
  \end{cases}
\end{equation*}
Clearly $\mu_0^n g = \mu_n g$.  Generally we have
\begin{alignat*}{2}
  &\texttt{NatLeastUpLBound}\colon &\;&
  n_0 \le n \to n_0 \le \mu_{n_0}^n g,
  \\
  &\texttt{NatLeastUpBound}\colon &\;&
  \mu_{n_0}^n g \le n,
  \\
  &\texttt{NatLeastUpLeIntro}\colon &\;&
  n_0 \le m \to g m \to \mu_{n_o}^n g \le m,
  \\
  &\texttt{NatLeastUpLtElim}\colon &\;&
  n_0 \le \mu_{n_0}^n g < n \to g(\mu_{n_0}^n g).
\end{alignat*}

\nocite{AczelSimmonsWainer92}
\frenchspacing
\bibliographystyle{amsplain}
\bibliography{minlog}
%% \bibliography{a-z}
%% \bibliographystyle{plain}

\printindex

\end{document}
